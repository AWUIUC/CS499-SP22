{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Plot-STAN-Inputs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4QOAy9EfD4iuT2A1KaKUc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","In this notebook, we aim to show/plot the inputs to STAN (the JHU data after all preprocessing)\n","\"\"\""],"metadata":{"id":"O_zYGVzPH9Sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7Nxq5jcHeH4","executionInfo":{"status":"ok","timestamp":1644420916950,"user_tz":360,"elapsed":96833,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"91bd1624-ac03-4888-908e-7e28c3a0a79f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/0. Data Analysis\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/0. Data Analysis\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"0. Data Analysis\"\n","! pwd"]},{"cell_type":"code","source":["# Install haversine for use later\n","! pip install haversine"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDz1UEQSKZ1r","executionInfo":{"status":"ok","timestamp":1644421570544,"user_tz":360,"elapsed":5079,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"d2cdb5ae-8a00-44e6-ff0f-a427ccb6c9f9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n"]}]},{"cell_type":"code","source":["# Use STAN's preprocessing code to get variables we will plot\n","from haversine import haversine\n","import numpy as np\n","import pandas as pd\n","import pickle\n","\n","\n","def gravity_law_commute_dist(lat1, lng1, pop1, lat2, lng2, pop2, r=1):\n","    \"\"\"\n","    This function calculates the edge via the gravity law\n","    :param lat1: latitude of location 1\n","    :type lat1: float\n","    :param lng1: longitude of location 1\n","    :type lat1: float\n","    :param pop1: population of location 1\n","    :type lat1: float or int\n","    :param lat2: latitude of location 2\n","    :type lat1: float\n","    :param lng2: longitude of location 2\n","    :type lat1: float\n","    :param pop2: population of location 2\n","    :type lat1: float or int\n","    :param r: diameter, by default 1\n","    :type lat1: float or int\n","    :return: edge value\n","    :rtype: float or int\n","    \"\"\"\n","    d = haversine((lat1, lng1), (lat2, lng2), 'km')\n","    c = 1\n","    w = 0\n","    alpha = 0.1\n","    beta = 0.1\n","    r = 1e4\n","    \n","    w = (np.exp(-d / r)) / (abs((pop1 ** alpha) - (pop2 ** beta))+1e-5)\n","    return w\n","\n","\n","\n","#Merge population data with downloaded data\n","raw_data = pickle.load(open('./data/jhu_processed_data.pickle','rb')) # state_covid_data.pickle --> jhu_processed_data.pickle\n","pop_data = pd.read_csv('./data/uszips.csv')\n","pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')\n","\n","# Generate location similarity\n","loc_list = list(raw_data['state'].unique())\n","loc_dist_map = {}\n","\n","for each_loc in loc_list:\n","    loc_dist_map[each_loc] = {}\n","    for each_loc2 in loc_list:\n","        lat1 = raw_data[raw_data['state']==each_loc]['latitude'].unique()[0]\n","        lng1 = raw_data[raw_data['state']==each_loc]['longitude'].unique()[0]\n","        pop1 = raw_data[raw_data['state']==each_loc]['population'].unique()[0]\n","        \n","        lat2 = raw_data[raw_data['state']==each_loc2]['latitude'].unique()[0]\n","        lng2 = raw_data[raw_data['state']==each_loc2]['longitude'].unique()[0]\n","        pop2 = raw_data[raw_data['state']==each_loc2]['population'].unique()[0]\n","        \n","        loc_dist_map[each_loc][each_loc2] = gravity_law_commute_dist(lat1, lng1, pop1, lat2, lng2, pop2, r=0.5)\n","\n","#Generate Graph\n","dist_threshold = 18\n","\n","for each_loc in loc_dist_map:\n","    loc_dist_map[each_loc] = {k: v for k, v in sorted(loc_dist_map[each_loc].items(), key=lambda item: item[1], reverse=True)}\n","    \n","adj_map = {}\n","for each_loc in loc_dist_map:\n","    adj_map[each_loc] = []\n","    for i, each_loc2 in enumerate(loc_dist_map[each_loc]):\n","        if loc_dist_map[each_loc][each_loc2] > dist_threshold:\n","            if i <= 3:\n","                adj_map[each_loc].append(each_loc2)\n","            else:\n","                break\n","        else:\n","            if i <= 1:\n","                adj_map[each_loc].append(each_loc2)\n","            else:\n","                break\n","\n","rows = []\n","cols = []\n","for each_loc in adj_map:\n","    for each_loc2 in adj_map[each_loc]:\n","        rows.append(loc_list.index(each_loc))\n","        cols.append(loc_list.index(each_loc2))\n","\n","#Preprocess features\n","\n","active_cases = []\n","confirmed_cases = []\n","new_cases = []\n","death_cases = []\n","static_feat = []\n","\n","for i, each_loc in enumerate(loc_list):\n","    active_cases.append(raw_data[raw_data['state'] == each_loc]['active'])\n","    confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","    new_cases.append(raw_data[raw_data['state'] == each_loc]['new_cases'])\n","    death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","    static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","    \n","active_cases = np.array(active_cases)\n","confirmed_cases = np.array(confirmed_cases)\n","death_cases = np.array(death_cases)\n","new_cases = np.array(new_cases)\n","static_feat = np.array(static_feat)[:, 0, :]\n","recovered_cases = confirmed_cases - active_cases - death_cases\n","susceptible_cases = np.expand_dims(static_feat[:, 0], -1) - active_cases - recovered_cases\n","\n","# Batch_feat: new_cases(dI), dR, dS\n","#dI = np.array(new_cases)\n","dI = np.concatenate((np.zeros((active_cases.shape[0],1), dtype=np.float32), np.diff(active_cases)), axis=-1)\n","dR = np.concatenate((np.zeros((recovered_cases.shape[0],1), dtype=np.float32), np.diff(recovered_cases)), axis=-1)\n","dS = np.concatenate((np.zeros((susceptible_cases.shape[0],1), dtype=np.float32), np.diff(susceptible_cases)), axis=-1)\n","\n","#Build normalizer\n","normalizer = {'S':{}, 'I':{}, 'R':{}, 'dS':{}, 'dI':{}, 'dR':{}}\n","\n","for i, each_loc in enumerate(loc_list):\n","    normalizer['S'][each_loc] = (np.mean(susceptible_cases[i]), np.std(susceptible_cases[i]))\n","    normalizer['I'][each_loc] = (np.mean(active_cases[i]), np.std(active_cases[i]))\n","    normalizer['R'][each_loc] = (np.mean(recovered_cases[i]), np.std(recovered_cases[i]))\n","    normalizer['dI'][each_loc] = (np.mean(dI[i]), np.std(dI[i]))\n","    normalizer['dR'][each_loc] = (np.mean(dR[i]), np.std(dR[i]))\n","    normalizer['dS'][each_loc] = (np.mean(dS[i]), np.std(dS[i]))\n","\n","def prepare_data(data, sum_I, sum_R, history_window=5, pred_window=15, slide_step=5):\n","    # Data shape n_loc, timestep, n_feat\n","    # Reshape to n_loc, t, history_window*n_feat\n","    n_loc = data.shape[0]\n","    timestep = data.shape[1]\n","    n_feat = data.shape[2]\n","    \n","    x = []\n","    y_I = []\n","    y_R = []\n","    last_I = []\n","    last_R = []\n","    concat_I = []\n","    concat_R = []\n","    for i in range(0, timestep, slide_step):\n","        if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","            break\n","        x.append(data[:, i:i+history_window, :].reshape((n_loc, history_window*n_feat)))\n","        \n","        concat_I.append(data[:, i+history_window-1, 0])\n","        concat_R.append(data[:, i+history_window-1, 1])\n","        last_I.append(sum_I[:, i+history_window-1])\n","        last_R.append(sum_R[:, i+history_window-1])\n","\n","        y_I.append(data[:, i+history_window:i+history_window+pred_window, 0])\n","        y_R.append(data[:, i+history_window:i+history_window+pred_window, 1])\n","\n","    x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\n","    last_I = np.array(last_I, dtype=np.float32).transpose((1, 0))\n","    last_R = np.array(last_R, dtype=np.float32).transpose((1, 0))\n","    concat_I = np.array(concat_I, dtype=np.float32).transpose((1, 0))\n","    concat_R = np.array(concat_R, dtype=np.float32).transpose((1, 0))\n","    y_I = np.array(y_I, dtype=np.float32).transpose((1, 0, 2))\n","    y_R = np.array(y_R, dtype=np.float32).transpose((1, 0, 2))\n","    return x, last_I, last_R, concat_I, concat_R, y_I, y_R\n","\n","valid_window = 25\n","test_window = 25\n","\n","history_window=6\n","pred_window=15\n","slide_step=5\n","\n","dynamic_feat = np.concatenate((np.expand_dims(dI, axis=-1), np.expand_dims(dR, axis=-1), np.expand_dims(dS, axis=-1)), axis=-1)\n","    \n","#Normalize\n","for i, each_loc in enumerate(loc_list):\n","    dynamic_feat[i, :, 0] = (dynamic_feat[i, :, 0] - normalizer['dI'][each_loc][0]) / normalizer['dI'][each_loc][1]\n","    dynamic_feat[i, :, 1] = (dynamic_feat[i, :, 1] - normalizer['dR'][each_loc][0]) / normalizer['dR'][each_loc][1]\n","    dynamic_feat[i, :, 2] = (dynamic_feat[i, :, 2] - normalizer['dS'][each_loc][0]) / normalizer['dS'][each_loc][1]\n","\n","dI_mean = []\n","dI_std = []\n","dR_mean = []\n","dR_std = []\n","\n","for i, each_loc in enumerate(loc_list):\n","    dI_mean.append(normalizer['dI'][each_loc][0])\n","    dR_mean.append(normalizer['dR'][each_loc][0])\n","    dI_std.append(normalizer['dI'][each_loc][1])\n","    dR_std.append(normalizer['dR'][each_loc][1])\n","\n","dI_mean = np.array(dI_mean)\n","dI_std = np.array(dI_std)\n","dR_mean = np.array(dR_mean)\n","dR_std = np.array(dR_std)\n","\n","#Split train-test\n","train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n","val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n","test_feat = dynamic_feat[:, -test_window:, :]\n","\n","train_x, train_I, train_R, train_cI, train_cR, train_yI, train_yR = prepare_data(train_feat, active_cases[:, :-valid_window-test_window], recovered_cases[:, :-valid_window-test_window], history_window, pred_window, slide_step)\n","val_x, val_I, val_R, val_cI, val_cR, val_yI, val_yR = prepare_data(val_feat, active_cases[:, -valid_window-test_window:-test_window], recovered_cases[:, -valid_window-test_window:-test_window], history_window, pred_window, slide_step)\n","test_x, test_I, test_R, test_cI, test_cR, test_yI, test_yR = prepare_data(test_feat, active_cases[:, -test_window:], recovered_cases[:, -test_window:], history_window, pred_window, slide_step)\n","\n","# train_x = torch.tensor(train_x).to(device)\n","# train_I = torch.tensor(train_I).to(device)\n","# train_R = torch.tensor(train_R).to(device)\n","# train_cI = torch.tensor(train_cI).to(device)\n","# train_cR = torch.tensor(train_cR).to(device)\n","# train_yI = torch.tensor(train_yI).to(device)\n","# train_yR = torch.tensor(train_yR).to(device)\n","\n","# val_x = torch.tensor(val_x).to(device)\n","# val_I = torch.tensor(val_I).to(device)\n","# val_R = torch.tensor(val_R).to(device)\n","# val_cI = torch.tensor(val_cI).to(device)\n","# val_cR = torch.tensor(val_cR).to(device)\n","# val_yI = torch.tensor(val_yI).to(device)\n","# val_yR = torch.tensor(val_yR).to(device)\n","\n","# test_x = torch.tensor(test_x).to(device)\n","# test_I = torch.tensor(test_I).to(device)\n","# test_R = torch.tensor(test_R).to(device)\n","# test_cI = torch.tensor(test_cI).to(device)\n","# test_cR = torch.tensor(test_cR).to(device)\n","# test_yI = torch.tensor(test_yI).to(device)\n","# test_yR = torch.tensor(test_yR).to(device)\n","\n","# dI_mean = torch.tensor(dI_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n","# dI_std = torch.tensor(dI_std, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n","# dR_mean = torch.tensor(dR_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n","# dR_std = torch.tensor(dR_std, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n","\n","# N = torch.tensor(static_feat[:, 0], dtype=torch.float32).to(device).unsqueeze(-1)\n"],"metadata":{"id":"tOEBpuSTIv0E","executionInfo":{"status":"ok","timestamp":1644421781891,"user_tz":360,"elapsed":51465,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Plot these variables: dynamic_feat, active_cases, recovered_cases\n","# dynamic_feat.shape # (52, 650, 3)\n","# active_cases.shape # (52, 650)\n","# recovered_cases.shape # (52, 650)\n","\n","import matplotlib.pyplot as plt\n","\n","# Plot normalized dI values in dynamic_feat\n","fig, axs = plt.subplots(26, 2)\n","fig.set_size_inches(8, 232)\n","x = np.arange(650)\n","for i, state_name in enumerate(loc_list):\n","  idx0 = i // 2\n","  idx1 = i % 2\n","  normalized_dI_state_data = dynamic_feat[i, :, 0] # ndarray with shape (650,)\n","  data_of_interest = normalized_dI_state_data\n","  axs[idx0, idx1].plot(x, data_of_interest)\n","  axs[idx0, idx1].set_title(\"Normalized dI: \" + state_name)\n","plt.savefig(\"./plots/JHU_STAN_inputs/normalized_dI.png\")\n","\n","# Plot normalized dR values in dynamic_feat\n","fig, axs = plt.subplots(26, 2)\n","fig.set_size_inches(8, 232)\n","x = np.arange(650)\n","for i, state_name in enumerate(loc_list):\n","  idx0 = i // 2\n","  idx1 = i % 2\n","  normalized_dR_state_data = dynamic_feat[i, :, 1] # ndarray with shape (650,)\n","  data_of_interest = normalized_dR_state_data\n","  axs[idx0, idx1].plot(x, data_of_interest)\n","  axs[idx0, idx1].set_title(\"Normalized dR: \" + state_name)\n","plt.savefig(\"./plots/JHU_STAN_inputs/normalized_dR.png\")\n","\n","# Plot normalized dS values in dynamic_feat\n","fig, axs = plt.subplots(26, 2)\n","fig.set_size_inches(8, 232)\n","x = np.arange(650)\n","for i, state_name in enumerate(loc_list):\n","  idx0 = i // 2\n","  idx1 = i % 2\n","  normalized_dS_state_data = dynamic_feat[i, :, 2] # ndarray with shape (650,)\n","  data_of_interest = normalized_dS_state_data\n","  axs[idx0, idx1].plot(x, data_of_interest)\n","  axs[idx0, idx1].set_title(\"Normalized dS: \" + state_name)\n","plt.savefig(\"./plots/JHU_STAN_inputs/normalized_dS.png\")\n","\n","# Plot un-normalized number of infected people\n","fig, axs = plt.subplots(26, 2)\n","fig.set_size_inches(8, 232)\n","x = np.arange(650)\n","for i, state_name in enumerate(loc_list):\n","  idx0 = i // 2\n","  idx1 = i % 2\n","  unnormalized_I_state_data = active_cases[i, :] # ndarray with shape (650,)\n","  data_of_interest = unnormalized_I_state_data\n","  axs[idx0, idx1].plot(x, data_of_interest)\n","  axs[idx0, idx1].set_title(\"I: \" + state_name)\n","plt.savefig(\"./plots/JHU_STAN_inputs/unnormalized_I.png\")\n","\n","# Plot un-normalized number of recovered people\n","fig, axs = plt.subplots(26, 2)\n","fig.set_size_inches(8, 232)\n","x = np.arange(650)\n","for i, state_name in enumerate(loc_list):\n","  idx0 = i // 2\n","  idx1 = i % 2\n","  unnormalized_R_state_data = recovered_cases[i, :] # ndarray with shape (650,)\n","  data_of_interest = unnormalized_R_state_data\n","  axs[idx0, idx1].plot(x, data_of_interest)\n","  axs[idx0, idx1].set_title(\"R: \" + state_name)\n","plt.savefig(\"./plots/JHU_STAN_inputs/unnormalized_R.png\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wRWOU75U0U0Q2tcGNVyv6iU9nboHeBfA"},"id":"3qA9ah_jJm4B","executionInfo":{"status":"ok","timestamp":1644425434843,"user_tz":360,"elapsed":59698,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"4cea2494-faea-41b3-ee33-06ad019d9c42"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}