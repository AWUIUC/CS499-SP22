{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPyk6Dvop+YRzsDKj/OhOUC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\"\n","! pip install epiweeks\n","! pip install haversine"],"metadata":{"id":"7yDiZvV2jd2R","executionInfo":{"status":"ok","timestamp":1646424127897,"user_tz":360,"elapsed":9183,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"3bbc3a9c-81af-40db-aa92-c79ac9f67a2d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n","Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Import libraries needed\n","\"\"\"\n","from data_downloader import GenerateTrainingData\n","from utils import gravity_law_commute_dist\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import torch"],"metadata":{"id":"xyi3R865jOGA","executionInfo":{"status":"ok","timestamp":1646424127898,"user_tz":360,"elapsed":12,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables used to preprocess data\n","\"\"\"\n","START_DATE = '2020-04-12'\n","END_DATE = '2022-01-24'\n","valid_window = 25\n","test_window = 25\n","history_window=6\n","pred_window=15\n","slide_step=5"],"metadata":{"id":"3aRrgY3MfbR2","executionInfo":{"status":"ok","timestamp":1646424127899,"user_tz":360,"elapsed":11,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uYf0Z0LfSNa","executionInfo":{"status":"ok","timestamp":1646424378728,"user_tz":360,"elapsed":250839,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"585b7037-456d-4b18-ed14-258b3463c3aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Finish download\n"]}],"source":["\"\"\"\n","Download JHU data\n","\"\"\"\n","# Download data\n","GenerateTrainingData().download_jhu_data(START_DATE, END_DATE)\n","\n","#Merge population data with downloaded data\n","raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n","pop_data = pd.read_csv('./uszips.csv')\n","pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')"]},{"cell_type":"code","source":["\"\"\"\n","Create edge index to be passed to GNN architecture later in Pytorch Geometric\n","\"\"\"\n","# State name to state abbreviation mapping (so we can index the state adjacency map later)\n","# Reference: https://gist.github.com/rogerallen/1583593 \n","us_state_to_abbrev = {\n","    \"Alabama\": \"AL\",\n","    \"Alaska\": \"AK\",\n","    \"Arizona\": \"AZ\",\n","    \"Arkansas\": \"AR\",\n","    \"California\": \"CA\",\n","    \"Colorado\": \"CO\",\n","    \"Connecticut\": \"CT\",\n","    \"Delaware\": \"DE\",\n","    \"Florida\": \"FL\",\n","    \"Georgia\": \"GA\",\n","    \"Hawaii\": \"HI\",\n","    \"Idaho\": \"ID\",\n","    \"Illinois\": \"IL\",\n","    \"Indiana\": \"IN\",\n","    \"Iowa\": \"IA\",\n","    \"Kansas\": \"KS\",\n","    \"Kentucky\": \"KY\",\n","    \"Louisiana\": \"LA\",\n","    \"Maine\": \"ME\",\n","    \"Maryland\": \"MD\",\n","    \"Massachusetts\": \"MA\",\n","    \"Michigan\": \"MI\",\n","    \"Minnesota\": \"MN\",\n","    \"Mississippi\": \"MS\",\n","    \"Missouri\": \"MO\",\n","    \"Montana\": \"MT\",\n","    \"Nebraska\": \"NE\",\n","    \"Nevada\": \"NV\",\n","    \"New Hampshire\": \"NH\",\n","    \"New Jersey\": \"NJ\",\n","    \"New Mexico\": \"NM\",\n","    \"New York\": \"NY\",\n","    \"North Carolina\": \"NC\",\n","    \"North Dakota\": \"ND\",\n","    \"Ohio\": \"OH\",\n","    \"Oklahoma\": \"OK\",\n","    \"Oregon\": \"OR\",\n","    \"Pennsylvania\": \"PA\",\n","    \"Rhode Island\": \"RI\",\n","    \"South Carolina\": \"SC\",\n","    \"South Dakota\": \"SD\",\n","    \"Tennessee\": \"TN\",\n","    \"Texas\": \"TX\",\n","    \"Utah\": \"UT\",\n","    \"Vermont\": \"VT\",\n","    \"Virginia\": \"VA\",\n","    \"Washington\": \"WA\",\n","    \"West Virginia\": \"WV\",\n","    \"Wisconsin\": \"WI\",\n","    \"Wyoming\": \"WY\",\n","    \"District of Columbia\": \"DC\",\n","    \"American Samoa\": \"AS\",\n","    \"Guam\": \"GU\",\n","    \"Northern Mariana Islands\": \"MP\",\n","    \"Puerto Rico\": \"PR\",\n","    \"United States Minor Outlying Islands\": \"UM\",\n","    \"U.S. Virgin Islands\": \"VI\",\n","}\n","\n","# invert the dictionary\n","abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n","\n","# State abbreviation to state adjacency list mapping (for creation of map)\n","# Modified from: https://gist.github.com/rietta/4112447 \n","states_adjacency_list = {\n","    \"AK\": \"AK\",\n","    \"AL\": \"AL,MS,TN,GA,FL\",\n","    \"AR\": \"AR,MO,TN,MS,LA,TX,OK\",\n","    \"AZ\": \"AZ,CA,NV,UT,CO,NM\",\n","    \"CA\": \"CA,OR,NV,AZ\",\n","    \"CO\": \"CO,WY,NE,KS,OK,NM,AZ,UT\",\n","    \"CT\": \"CT,NY,MA,RI\",\n","    \"DC\": \"DC,MD,VA\",\n","    \"DE\": \"DE,MD,PA,NJ\",\n","    \"FL\": \"FL,AL,GA\",\n","    \"GA\": \"GA,FL,AL,TN,NC,SC\",\n","    \"HI\": \"HI\",\n","    \"IA\": \"IA,MN,WI,IL,MO,NE,SD\",\n","    \"ID\": \"ID,MT,WY,UT,NV,OR,WA\",\n","    \"IL\": \"IL,IN,KY,MO,IA,WI\",\n","    \"IN\": \"IN,MI,OH,KY,IL\",\n","    \"KS\": \"KS,NE,MO,OK,CO\",\n","    \"KY\": \"KY,IN,OH,WV,VA,TN,MO,IL\",\n","    \"LA\": \"LA,TX,AR,MS\",\n","    \"MA\": \"MA,RI,CT,NY,NH,VT\",\n","    \"MD\": \"MD,VA,WV,PA,DC,DE\",\n","    \"ME\": \"ME,NH\",\n","    \"MI\": \"MI,WI,IN,OH\",\n","    \"MN\": \"MN,WI,IA,SD,ND\",\n","    \"MO\": \"MO,IA,IL,KY,TN,AR,OK,KS,NE\",\n","    \"MS\": \"MS,LA,AR,TN,AL\",\n","    \"MT\": \"MT,ND,SD,WY,ID\",\n","    \"NC\": \"NC,VA,TN,GA,SC\",\n","    \"ND\": \"ND,MN,SD,MT\",\n","    \"NE\": \"NE,SD,IA,MO,KS,CO,WY\",\n","    \"NH\": \"NH,VT,ME,MA\",\n","    \"NJ\": \"NJ,DE,PA,NY\",\n","    \"NM\": \"NM,AZ,UT,CO,OK,TX\",\n","    \"NV\": \"NV,ID,UT,AZ,CA,OR\",\n","    \"NY\": \"NY,NJ,PA,VT,MA,CT\",\n","    \"OH\": \"OH,PA,WV,KY,IN,MI\",\n","    \"OK\": \"OK,KS,MO,AR,TX,NM,CO\",\n","    \"OR\": \"OR,CA,NV,ID,WA\",\n","    \"PA\": \"PA,NY,NJ,DE,MD,WV,OH\",\n","    \"PR\": \"PR\",\n","    \"RI\": \"RI,CT,MA\",\n","    \"SC\": \"SC,GA,NC\",\n","    \"SD\": \"SD,ND,MN,IA,NE,WY,MT\",\n","    \"TN\": \"TN,KY,VA,NC,GA,AL,MS,AR,MO\",\n","    \"TX\": \"TX,NM,OK,AR,LA\",\n","    \"UT\": \"UT,ID,WY,CO,NM,AZ,NV\",\n","    \"VA\": \"VA,NC,TN,KY,WV,MD,DC\",\n","    \"VT\": \"VT,NY,NH,MA\",\n","    \"WA\": \"WA,ID,OR\",\n","    \"WI\": \"WI,MI,MN,IA,IL\",\n","    \"WV\": \"WV,OH,PA,MD,VA,KY\",\n","    \"WY\": \"WY,MT,SD,NE,CO,UT,ID\"\n","}\n","\n","\n","# we will use undirected graph, where nodes are represented by ints\n","edge_list_source_node = []\n","edge_list_destination_node = []\n","\n","\n","state_list = list(raw_data['state'].unique())\n","for state_name in state_list:\n","  state_abbrev = us_state_to_abbrev[state_name]\n","  curr_state_and_neighbors = states_adjacency_list[state_abbrev]\n","  comma_delimited_list = curr_state_and_neighbors.split(\",\")\n","  \n","  source_state_abbrev = None\n","  dest_state_abbreviations = None\n","  if len(comma_delimited_list) == 1:\n","    source_state_abbrev = comma_delimited_list[0]\n","    dest_state_abbreviations = [comma_delimited_list[0]]\n","  else:\n","    source_state_abbrev = comma_delimited_list[0]\n","    dest_state_abbreviations = comma_delimited_list[1:]\n","  \n","  for dest_state_abbrev in dest_state_abbreviations:\n","    source_state_full_name = abbrev_to_us_state[source_state_abbrev]\n","    dest_state_full_name = abbrev_to_us_state[dest_state_abbrev]\n","\n","    source_state_int = state_list.index(source_state_full_name)\n","    dest_state_int = state_list.index(dest_state_full_name)\n","    \n","    edge_list_source_node.append(source_state_int)\n","    edge_list_destination_node.append(dest_state_int)\n","\n","edge_index = torch.tensor([edge_list_source_node,\n","                           edge_list_destination_node], dtype=torch.long)"],"metadata":{"id":"lVx9Ll_e8nre","executionInfo":{"status":"ok","timestamp":1646424378730,"user_tz":360,"elapsed":18,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Preprocess data by separating it into different groups\n","\"\"\"\n","# Preprocess features\n","confirmed_cases = []\n","death_cases = []\n","static_feat = []\n","\n","for i, each_loc in enumerate(state_list):\n","    confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","    death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","    static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","    \n","confirmed_cases_unsmoothed = np.array(confirmed_cases)\n","death_cases_unsmoothed = np.array(death_cases)\n","static_feat_unsmoothed = np.array(static_feat)[:, 0, :]\n","\n","\n","# Calculate change in # cases and # deaths from previous day\n","daily_change_in_confirmed_unsmoothed = np.concatenate((np.zeros((confirmed_cases_unsmoothed.shape[0], 1), dtype=np.float32), np.diff(confirmed_cases_unsmoothed)), axis=-1)\n","daily_change_in_deaths_unsmoothed = np.concatenate((np.zeros((death_cases_unsmoothed.shape[0], 1), dtype=np.float32), np.diff(death_cases_unsmoothed)), axis=-1)"],"metadata":{"id":"UKHyvb_fg4HE","executionInfo":{"status":"ok","timestamp":1646424379312,"user_tz":360,"elapsed":592,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Smooth the data\n","\"\"\"\n","\n","confirmed_cases_smoothed = []\n","death_cases_smoothed = []\n","daily_change_in_confirmed_smoothed = []\n","daily_change_in_deaths_smoothed = []\n","\n","# Define smoothing function from: https://www.delftstack.com/howto/python/smooth-data-in-python/\n","def smooth(y, box_pts):\n","    box = np.ones(box_pts)/box_pts\n","    y_smooth = np.convolve(y, box, mode='same')\n","    return y_smooth\n","\n","for i in range(confirmed_cases_unsmoothed.shape[0]):\n","  confirmed_cases_smoothed.append(smooth(confirmed_cases_unsmoothed[i], 8))\n","  death_cases_smoothed.append(smooth(death_cases_unsmoothed[i], 8))\n","  daily_change_in_confirmed_smoothed.append(smooth(daily_change_in_confirmed_unsmoothed[i], 8))\n","  daily_change_in_deaths_smoothed.append(smooth(daily_change_in_deaths_unsmoothed[i], 8))\n","\n","confirmed_cases_smoothed = np.array(confirmed_cases_smoothed)\n","death_cases_smoothed = np.array(death_cases_smoothed)\n","daily_change_in_confirmed_smoothed = np.array(daily_change_in_confirmed_smoothed)\n","daily_change_in_deaths_smoothed = np.array(daily_change_in_deaths_smoothed)"],"metadata":{"id":"G5r0qZaHeWUd","executionInfo":{"status":"ok","timestamp":1646424425085,"user_tz":360,"elapsed":10,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put data together into 1 big numpy array\n","\"\"\"\n","dynamic_feat_unsmoothed = np.concatenate((np.expand_dims(confirmed_cases_unsmoothed, axis=-1),\n","                               np.expand_dims(death_cases_unsmoothed, axis=-1),\n","                               np.expand_dims(daily_change_in_confirmed_unsmoothed, axis=-1), \n","                               np.expand_dims(daily_change_in_deaths_unsmoothed, axis=-1)\n","                               ), axis=-1)\n","\n","dynamic_feat_smoothed = np.concatenate((np.expand_dims(confirmed_cases_smoothed, axis=-1),\n","                               np.expand_dims(death_cases_smoothed, axis=-1),\n","                               np.expand_dims(daily_change_in_confirmed_smoothed, axis=-1), \n","                               np.expand_dims(daily_change_in_deaths_smoothed, axis=-1)\n","                               ), axis=-1)"],"metadata":{"id":"dOMvjHswg8Jg","executionInfo":{"status":"ok","timestamp":1646424429970,"user_tz":360,"elapsed":264,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Separate data into training, testing, and validation sets\n","\"\"\"\n","\n","#Split train-test\n","train_feat_unsmoothed = dynamic_feat_unsmoothed[:, :-valid_window-test_window, :]\n","val_feat_unsmoothed = dynamic_feat_unsmoothed[:, -valid_window-test_window:-test_window, :]\n","test_feat_unsmoothed = dynamic_feat_unsmoothed[:, -test_window:, :]\n","\n","train_feat_smoothed = dynamic_feat_smoothed[:, :-valid_window-test_window, :]\n","val_feat_smoothed = dynamic_feat_smoothed[:, -valid_window-test_window:-test_window, :]\n","test_feat_smoothed = dynamic_feat_smoothed[:, -test_window:, :]\n","\n","# Helper function for creating each set of data used\n","def prepare_data(data):\n","  # Data shape num_locations, timestep, n_feat\n","  num_locations = data.shape[0]\n","  timestep = data.shape[1]\n","  n_feat = data.shape[2]\n","\n","  input_entries = []\n","  output_entries_confirmed = []\n","  output_entries_deaths = []\n","  output_entries_change_in_confirmed = []\n","  output_entries_change_in_deaths = []\n","\n","  for i in range(0, timestep, slide_step):\n","    if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","        break\n","\n","    # Shape = number nodes x num_input_features\n","    input_entry = data[:, i:i+history_window, :].reshape((num_locations, history_window*n_feat)).tolist()\n","\n","    # Shape = number nodes x num_output_features\n","    output_entry_confirmed = data[:, i+history_window:i+history_window+pred_window, 0].reshape((num_locations, pred_window)).tolist()\n","    output_entry_deaths = data[:, i+history_window:i+history_window+pred_window, 1].reshape((num_locations, pred_window)).tolist()\n","    output_entry_change_in_confirmed = data[:, i+history_window:i+history_window+pred_window, 2].reshape((num_locations, pred_window)).tolist()\n","    output_entry_change_in_deaths = data[:, i+history_window:i+history_window+pred_window, 3].reshape((num_locations, pred_window)).tolist()\n","\n","    input_entries.append(torch.tensor(input_entry))\n","    output_entries_confirmed.append(torch.tensor(output_entry_confirmed))\n","    output_entries_deaths.append(torch.tensor(output_entry_deaths))\n","    output_entries_change_in_confirmed.append(torch.tensor(output_entry_change_in_confirmed))\n","    output_entries_change_in_deaths.append(torch.tensor(output_entry_change_in_deaths))\n","\n","  return input_entries, output_entries_confirmed, output_entries_deaths, output_entries_change_in_confirmed, output_entries_change_in_deaths\n","\n","train_x_unsmoothed, train_y_confirmed_unsmoothed, train_y_deaths_unsmoothed, train_y_change_in_confirmed_unsmoothed, train_y_change_in_deaths_unsmoothed = prepare_data(train_feat_unsmoothed)\n","val_x_unsmoothed, val_y_confirmed_unsmoothed, val_y_deaths_unsmoothed, val_y_change_in_confirmed_unsmoothed, val_y_change_in_deaths_unsmoothed = prepare_data(val_feat_unsmoothed)\n","test_x_unsmoothed, test_y_confirmed_unsmoothed, test_y_deaths_unsmoothed, test_y_change_in_confirmed_unsmoothed, test_y_change_in_deaths_unsmoothed = prepare_data(test_feat_unsmoothed)\n","\n","train_x_smoothed, train_y_confirmed_smoothed, train_y_deaths_smoothed, train_y_change_in_confirmed_smoothed, train_y_change_in_deaths_smoothed = prepare_data(train_feat_smoothed)\n","val_x_smoothed, val_y_confirmed_smoothed, val_y_deaths_smoothed, val_y_change_in_confirmed_smoothed, val_y_change_in_deaths_smoothed = prepare_data(val_feat_smoothed)\n","test_x_smoothed, test_y_confirmed_smoothed, test_y_deaths_smoothed, test_y_change_in_confirmed_smoothed, test_y_change_in_deaths_smoothed = prepare_data(test_feat_smoothed)"],"metadata":{"id":"QE_pnGeYhg8v","executionInfo":{"status":"ok","timestamp":1646424432168,"user_tz":360,"elapsed":529,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Package/organize preprocessed data together into a dictionary called \"preprocessed_data\"\n","\"\"\"\n","training_variables = {'train_x_unsmoothed':train_x_unsmoothed,\n","                      'train_x_smoothed':train_x_smoothed, \n","                      'train_y_confirmed_unsmoothed':train_y_confirmed_unsmoothed,\n","                      'train_y_confirmed_smoothed':train_y_confirmed_smoothed,\n","                      'train_y_deaths_unsmoothed':train_y_deaths_unsmoothed,\n","                      'train_y_deaths_smoothed':train_y_deaths_smoothed,\n","                      'train_y_change_in_confirmed_unsmoothed':train_y_change_in_confirmed_unsmoothed,\n","                      'train_y_change_in_confirmed_smoothed':train_y_change_in_confirmed_smoothed,\n","                      'train_y_change_in_deaths_unsmoothed':train_y_change_in_deaths_unsmoothed,\n","                      'train_y_change_in_deaths_smoothed':train_y_change_in_deaths_smoothed\n","                      }\n","\n","validation_variables = {'val_x_unsmoothed':val_x_unsmoothed,\n","                        'val_x_smoothed':val_x_smoothed,\n","                        'val_y_confirmed_unsmoothed':val_y_confirmed_unsmoothed,\n","                        'val_y_confirmed_smoothed':val_y_confirmed_smoothed,\n","                        'val_y_deaths_unsmoothed':val_y_deaths_unsmoothed,\n","                        'val_y_deaths_smoothed':val_y_deaths_smoothed,\n","                        'val_y_change_in_confirmed_unsmoothed':val_y_change_in_confirmed_unsmoothed,\n","                        'val_y_change_in_confirmed_smoothed':val_y_change_in_confirmed_smoothed,\n","                        'val_y_change_in_deaths_unsmoothed':val_y_change_in_deaths_unsmoothed,\n","                        'val_y_change_in_deaths_smoothed':val_y_change_in_deaths_smoothed\n","                        }\n","\n","testing_variables = {'test_x_unsmoothed':test_x_unsmoothed,\n","                     'test_x_smoothed':test_x_smoothed, \n","                     'test_y_confirmed_unsmoothed':test_y_confirmed_unsmoothed,\n","                     'test_y_confirmed_smoothed':test_y_confirmed_smoothed,\n","                     'test_y_deaths_unsmoothed':test_y_deaths_unsmoothed,\n","                     'test_y_deaths_smoothed':test_y_deaths_smoothed,\n","                     'test_y_change_in_confirmed_unsmoothed':test_y_change_in_confirmed_unsmoothed,\n","                     'test_y_change_in_confirmed_smoothed':test_y_change_in_confirmed_smoothed,\n","                     'test_y_change_in_deaths_unsmoothed':test_y_change_in_deaths_unsmoothed,\n","                     'test_y_change_in_deaths_smoothed':test_y_change_in_deaths_smoothed\n","                     }\n","\n","preprocessed_data = {\n","    'training_variables':training_variables,\n","    'validation_variables':validation_variables,\n","    'testing_variables':testing_variables,\n","    'edge_index':edge_index\n","}"],"metadata":{"id":"XoBnRNlznfut","executionInfo":{"status":"ok","timestamp":1646424436443,"user_tz":360,"elapsed":255,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","with open('./data/preprocessed_data.pickle', 'wb') as handle:\n","    pickle.dump(preprocessed_data, handle)\n"],"metadata":{"id":"gfyf7vOJLUDK","executionInfo":{"status":"ok","timestamp":1646424482738,"user_tz":360,"elapsed":252,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put above code into 1 file and 1 function\n","\"\"\"\n","%%writefile preprocess_data.py\n","\n","\"\"\"\n","Import libraries needed\n","\"\"\"\n","from data_downloader import GenerateTrainingData\n","from utils import gravity_law_commute_dist\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import torch\n","\n","\"\"\"\n","Declare global variables used to preprocess data\n","\"\"\"\n","START_DATE = '2020-04-12'\n","END_DATE = '2022-01-24'\n","valid_window = 25\n","test_window = 25\n","history_window=6\n","pred_window=15\n","slide_step=5\n","\n","def get_preprocessed_data():\n","    \"\"\"\n","    Download JHU data\n","    \"\"\"\n","    # Download data\n","    GenerateTrainingData().download_jhu_data(START_DATE, END_DATE)\n","\n","    #Merge population data with downloaded data\n","    raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n","    pop_data = pd.read_csv('./uszips.csv')\n","    pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","    raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')\n","\n","    #############################################################################################################################################################################\n","\n","    \"\"\"\n","    Create edge index to be passed to GNN architecture later in Pytorch Geometric\n","    \"\"\"\n","    # State name to state abbreviation mapping (so we can index the state adjacency map later)\n","    # Reference: https://gist.github.com/rogerallen/1583593 \n","    us_state_to_abbrev = {\n","        \"Alabama\": \"AL\",\n","        \"Alaska\": \"AK\",\n","        \"Arizona\": \"AZ\",\n","        \"Arkansas\": \"AR\",\n","        \"California\": \"CA\",\n","        \"Colorado\": \"CO\",\n","        \"Connecticut\": \"CT\",\n","        \"Delaware\": \"DE\",\n","        \"Florida\": \"FL\",\n","        \"Georgia\": \"GA\",\n","        \"Hawaii\": \"HI\",\n","        \"Idaho\": \"ID\",\n","        \"Illinois\": \"IL\",\n","        \"Indiana\": \"IN\",\n","        \"Iowa\": \"IA\",\n","        \"Kansas\": \"KS\",\n","        \"Kentucky\": \"KY\",\n","        \"Louisiana\": \"LA\",\n","        \"Maine\": \"ME\",\n","        \"Maryland\": \"MD\",\n","        \"Massachusetts\": \"MA\",\n","        \"Michigan\": \"MI\",\n","        \"Minnesota\": \"MN\",\n","        \"Mississippi\": \"MS\",\n","        \"Missouri\": \"MO\",\n","        \"Montana\": \"MT\",\n","        \"Nebraska\": \"NE\",\n","        \"Nevada\": \"NV\",\n","        \"New Hampshire\": \"NH\",\n","        \"New Jersey\": \"NJ\",\n","        \"New Mexico\": \"NM\",\n","        \"New York\": \"NY\",\n","        \"North Carolina\": \"NC\",\n","        \"North Dakota\": \"ND\",\n","        \"Ohio\": \"OH\",\n","        \"Oklahoma\": \"OK\",\n","        \"Oregon\": \"OR\",\n","        \"Pennsylvania\": \"PA\",\n","        \"Rhode Island\": \"RI\",\n","        \"South Carolina\": \"SC\",\n","        \"South Dakota\": \"SD\",\n","        \"Tennessee\": \"TN\",\n","        \"Texas\": \"TX\",\n","        \"Utah\": \"UT\",\n","        \"Vermont\": \"VT\",\n","        \"Virginia\": \"VA\",\n","        \"Washington\": \"WA\",\n","        \"West Virginia\": \"WV\",\n","        \"Wisconsin\": \"WI\",\n","        \"Wyoming\": \"WY\",\n","        \"District of Columbia\": \"DC\",\n","        \"American Samoa\": \"AS\",\n","        \"Guam\": \"GU\",\n","        \"Northern Mariana Islands\": \"MP\",\n","        \"Puerto Rico\": \"PR\",\n","        \"United States Minor Outlying Islands\": \"UM\",\n","        \"U.S. Virgin Islands\": \"VI\",\n","    }\n","\n","    # invert the dictionary\n","    abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n","\n","    # State abbreviation to state adjacency list mapping (for creation of map)\n","    # Modified from: https://gist.github.com/rietta/4112447 \n","    states_adjacency_list = {\n","        \"AK\": \"AK\",\n","        \"AL\": \"AL,MS,TN,GA,FL\",\n","        \"AR\": \"AR,MO,TN,MS,LA,TX,OK\",\n","        \"AZ\": \"AZ,CA,NV,UT,CO,NM\",\n","        \"CA\": \"CA,OR,NV,AZ\",\n","        \"CO\": \"CO,WY,NE,KS,OK,NM,AZ,UT\",\n","        \"CT\": \"CT,NY,MA,RI\",\n","        \"DC\": \"DC,MD,VA\",\n","        \"DE\": \"DE,MD,PA,NJ\",\n","        \"FL\": \"FL,AL,GA\",\n","        \"GA\": \"GA,FL,AL,TN,NC,SC\",\n","        \"HI\": \"HI\",\n","        \"IA\": \"IA,MN,WI,IL,MO,NE,SD\",\n","        \"ID\": \"ID,MT,WY,UT,NV,OR,WA\",\n","        \"IL\": \"IL,IN,KY,MO,IA,WI\",\n","        \"IN\": \"IN,MI,OH,KY,IL\",\n","        \"KS\": \"KS,NE,MO,OK,CO\",\n","        \"KY\": \"KY,IN,OH,WV,VA,TN,MO,IL\",\n","        \"LA\": \"LA,TX,AR,MS\",\n","        \"MA\": \"MA,RI,CT,NY,NH,VT\",\n","        \"MD\": \"MD,VA,WV,PA,DC,DE\",\n","        \"ME\": \"ME,NH\",\n","        \"MI\": \"MI,WI,IN,OH\",\n","        \"MN\": \"MN,WI,IA,SD,ND\",\n","        \"MO\": \"MO,IA,IL,KY,TN,AR,OK,KS,NE\",\n","        \"MS\": \"MS,LA,AR,TN,AL\",\n","        \"MT\": \"MT,ND,SD,WY,ID\",\n","        \"NC\": \"NC,VA,TN,GA,SC\",\n","        \"ND\": \"ND,MN,SD,MT\",\n","        \"NE\": \"NE,SD,IA,MO,KS,CO,WY\",\n","        \"NH\": \"NH,VT,ME,MA\",\n","        \"NJ\": \"NJ,DE,PA,NY\",\n","        \"NM\": \"NM,AZ,UT,CO,OK,TX\",\n","        \"NV\": \"NV,ID,UT,AZ,CA,OR\",\n","        \"NY\": \"NY,NJ,PA,VT,MA,CT\",\n","        \"OH\": \"OH,PA,WV,KY,IN,MI\",\n","        \"OK\": \"OK,KS,MO,AR,TX,NM,CO\",\n","        \"OR\": \"OR,CA,NV,ID,WA\",\n","        \"PA\": \"PA,NY,NJ,DE,MD,WV,OH\",\n","        \"PR\": \"PR\",\n","        \"RI\": \"RI,CT,MA\",\n","        \"SC\": \"SC,GA,NC\",\n","        \"SD\": \"SD,ND,MN,IA,NE,WY,MT\",\n","        \"TN\": \"TN,KY,VA,NC,GA,AL,MS,AR,MO\",\n","        \"TX\": \"TX,NM,OK,AR,LA\",\n","        \"UT\": \"UT,ID,WY,CO,NM,AZ,NV\",\n","        \"VA\": \"VA,NC,TN,KY,WV,MD,DC\",\n","        \"VT\": \"VT,NY,NH,MA\",\n","        \"WA\": \"WA,ID,OR\",\n","        \"WI\": \"WI,MI,MN,IA,IL\",\n","        \"WV\": \"WV,OH,PA,MD,VA,KY\",\n","        \"WY\": \"WY,MT,SD,NE,CO,UT,ID\"\n","    }\n","\n","\n","    # we will use undirected graph, where nodes are represented by ints\n","    edge_list_source_node = []\n","    edge_list_destination_node = []\n","\n","\n","    state_list = list(raw_data['state'].unique())\n","    for state_name in state_list:\n","      state_abbrev = us_state_to_abbrev[state_name]\n","      curr_state_and_neighbors = states_adjacency_list[state_abbrev]\n","      comma_delimited_list = curr_state_and_neighbors.split(\",\")\n","      \n","      source_state_abbrev = None\n","      dest_state_abbreviations = None\n","      if len(comma_delimited_list) == 1:\n","        source_state_abbrev = comma_delimited_list[0]\n","        dest_state_abbreviations = [comma_delimited_list[0]]\n","      else:\n","        source_state_abbrev = comma_delimited_list[0]\n","        dest_state_abbreviations = comma_delimited_list[1:]\n","      \n","      for dest_state_abbrev in dest_state_abbreviations:\n","        source_state_full_name = abbrev_to_us_state[source_state_abbrev]\n","        dest_state_full_name = abbrev_to_us_state[dest_state_abbrev]\n","\n","        source_state_int = state_list.index(source_state_full_name)\n","        dest_state_int = state_list.index(dest_state_full_name)\n","        \n","        edge_list_source_node.append(source_state_int)\n","        edge_list_destination_node.append(dest_state_int)\n","\n","    edge_index = torch.tensor([edge_list_source_node,\n","                              edge_list_destination_node], dtype=torch.long)\n","\n","    #############################################################################################################################################################################\n","\n","    \"\"\"\n","    Preprocess data by separating it into different groups\n","    \"\"\"\n","    # Preprocess features\n","    confirmed_cases = []\n","    death_cases = []\n","    static_feat = []\n","\n","    for i, each_loc in enumerate(state_list):\n","        confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","        death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","        static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","        \n","    confirmed_cases_unsmoothed = np.array(confirmed_cases)\n","    death_cases_unsmoothed = np.array(death_cases)\n","    static_feat_unsmoothed = np.array(static_feat)[:, 0, :]\n","\n","\n","    # Calculate change in # cases and # deaths from previous day\n","    daily_change_in_confirmed_unsmoothed = np.concatenate((np.zeros((confirmed_cases_unsmoothed.shape[0], 1), dtype=np.float32), np.diff(confirmed_cases_unsmoothed)), axis=-1)\n","    daily_change_in_deaths_unsmoothed = np.concatenate((np.zeros((death_cases_unsmoothed.shape[0], 1), dtype=np.float32), np.diff(death_cases_unsmoothed)), axis=-1)\n","\n","    #############################################################################################################################################################################\n","\n","    \"\"\"\n","    Smooth the data\n","    \"\"\"\n","\n","    confirmed_cases_smoothed = []\n","    death_cases_smoothed = []\n","    daily_change_in_confirmed_smoothed = []\n","    daily_change_in_deaths_smoothed = []\n","\n","    # Define smoothing function from: https://www.delftstack.com/howto/python/smooth-data-in-python/\n","    def smooth(y, box_pts):\n","        box = np.ones(box_pts)/box_pts\n","        y_smooth = np.convolve(y, box, mode='same')\n","        return y_smooth\n","\n","    for i in range(confirmed_cases_unsmoothed.shape[0]):\n","      confirmed_cases_smoothed.append(smooth(confirmed_cases_unsmoothed[i], 8))\n","      death_cases_smoothed.append(smooth(death_cases_unsmoothed[i], 8))\n","      daily_change_in_confirmed_smoothed.append(smooth(daily_change_in_confirmed_unsmoothed[i], 8))\n","      daily_change_in_deaths_smoothed.append(smooth(daily_change_in_deaths_unsmoothed[i], 8))\n","\n","    confirmed_cases_smoothed = np.array(confirmed_cases_smoothed)\n","    death_cases_smoothed = np.array(death_cases_smoothed)\n","    daily_change_in_confirmed_smoothed = np.array(daily_change_in_confirmed_smoothed)\n","    daily_change_in_deaths_smoothed = np.array(daily_change_in_deaths_smoothed)\n","\n","    #############################################################################################################################################################################\n","\n","    \"\"\"\n","    Put data together into 1 big numpy array\n","    \"\"\"\n","    dynamic_feat_unsmoothed = np.concatenate((np.expand_dims(confirmed_cases_unsmoothed, axis=-1),\n","                                  np.expand_dims(death_cases_unsmoothed, axis=-1),\n","                                  np.expand_dims(daily_change_in_confirmed_unsmoothed, axis=-1), \n","                                  np.expand_dims(daily_change_in_deaths_unsmoothed, axis=-1)\n","                                  ), axis=-1)\n","\n","    dynamic_feat_smoothed = np.concatenate((np.expand_dims(confirmed_cases_smoothed, axis=-1),\n","                                  np.expand_dims(death_cases_smoothed, axis=-1),\n","                                  np.expand_dims(daily_change_in_confirmed_smoothed, axis=-1), \n","                                  np.expand_dims(daily_change_in_deaths_smoothed, axis=-1)\n","                                  ), axis=-1)\n","\n","    #############################################################################################################################################################################\n","\n","    \"\"\"\n","    Separate data into training, testing, and validation sets\n","    \"\"\"\n","\n","    #Split train-test\n","    train_feat_unsmoothed = dynamic_feat_unsmoothed[:, :-valid_window-test_window, :]\n","    val_feat_unsmoothed = dynamic_feat_unsmoothed[:, -valid_window-test_window:-test_window, :]\n","    test_feat_unsmoothed = dynamic_feat_unsmoothed[:, -test_window:, :]\n","\n","    train_feat_smoothed = dynamic_feat_smoothed[:, :-valid_window-test_window, :]\n","    val_feat_smoothed = dynamic_feat_smoothed[:, -valid_window-test_window:-test_window, :]\n","    test_feat_smoothed = dynamic_feat_smoothed[:, -test_window:, :]\n","\n","    # Helper function for creating each set of data used\n","    def prepare_data(data):\n","      # Data shape num_locations, timestep, n_feat\n","      num_locations = data.shape[0]\n","      timestep = data.shape[1]\n","      n_feat = data.shape[2]\n","\n","      input_entries = []\n","      output_entries_confirmed = []\n","      output_entries_deaths = []\n","      output_entries_change_in_confirmed = []\n","      output_entries_change_in_deaths = []\n","\n","      for i in range(0, timestep, slide_step):\n","        if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","            break\n","\n","        # Shape = number nodes x num_input_features\n","        input_entry = data[:, i:i+history_window, :].reshape((num_locations, history_window*n_feat)).tolist()\n","\n","        # Shape = number nodes x num_output_features\n","        output_entry_confirmed = data[:, i+history_window:i+history_window+pred_window, 0].reshape((num_locations, pred_window)).tolist()\n","        output_entry_deaths = data[:, i+history_window:i+history_window+pred_window, 1].reshape((num_locations, pred_window)).tolist()\n","        output_entry_change_in_confirmed = data[:, i+history_window:i+history_window+pred_window, 2].reshape((num_locations, pred_window)).tolist()\n","        output_entry_change_in_deaths = data[:, i+history_window:i+history_window+pred_window, 3].reshape((num_locations, pred_window)).tolist()\n","\n","        input_entries.append(torch.tensor(input_entry))\n","        output_entries_confirmed.append(torch.tensor(output_entry_confirmed))\n","        output_entries_deaths.append(torch.tensor(output_entry_deaths))\n","        output_entries_change_in_confirmed.append(torch.tensor(output_entry_change_in_confirmed))\n","        output_entries_change_in_deaths.append(torch.tensor(output_entry_change_in_deaths))\n","\n","      return input_entries, output_entries_confirmed, output_entries_deaths, output_entries_change_in_confirmed, output_entries_change_in_deaths\n","\n","    train_x_unsmoothed, train_y_confirmed_unsmoothed, train_y_deaths_unsmoothed, train_y_change_in_confirmed_unsmoothed, train_y_change_in_deaths_unsmoothed = prepare_data(train_feat_unsmoothed)\n","    val_x_unsmoothed, val_y_confirmed_unsmoothed, val_y_deaths_unsmoothed, val_y_change_in_confirmed_unsmoothed, val_y_change_in_deaths_unsmoothed = prepare_data(val_feat_unsmoothed)\n","    test_x_unsmoothed, test_y_confirmed_unsmoothed, test_y_deaths_unsmoothed, test_y_change_in_confirmed_unsmoothed, test_y_change_in_deaths_unsmoothed = prepare_data(test_feat_unsmoothed)\n","\n","    train_x_smoothed, train_y_confirmed_smoothed, train_y_deaths_smoothed, train_y_change_in_confirmed_smoothed, train_y_change_in_deaths_smoothed = prepare_data(train_feat_smoothed)\n","    val_x_smoothed, val_y_confirmed_smoothed, val_y_deaths_smoothed, val_y_change_in_confirmed_smoothed, val_y_change_in_deaths_smoothed = prepare_data(val_feat_smoothed)\n","    test_x_smoothed, test_y_confirmed_smoothed, test_y_deaths_smoothed, test_y_change_in_confirmed_smoothed, test_y_change_in_deaths_smoothed = prepare_data(test_feat_smoothed)\n","\n","    #############################################################################################################################################################################\n","\n","    \"\"\"\n","    Package/organize preprocessed data together into a dictionary called \"preprocessed_data\"\n","    \"\"\"\n","    training_variables = {'train_x_unsmoothed':train_x_unsmoothed,\n","                          'train_x_smoothed':train_x_smoothed, \n","                          'train_y_confirmed_unsmoothed':train_y_confirmed_unsmoothed,\n","                          'train_y_confirmed_smoothed':train_y_confirmed_smoothed,\n","                          'train_y_deaths_unsmoothed':train_y_deaths_unsmoothed,\n","                          'train_y_deaths_smoothed':train_y_deaths_smoothed,\n","                          'train_y_change_in_confirmed_unsmoothed':train_y_change_in_confirmed_unsmoothed,\n","                          'train_y_change_in_confirmed_smoothed':train_y_change_in_confirmed_smoothed,\n","                          'train_y_change_in_deaths_unsmoothed':train_y_change_in_deaths_unsmoothed,\n","                          'train_y_change_in_deaths_smoothed':train_y_change_in_deaths_smoothed\n","                          }\n","\n","    validation_variables = {'val_x_unsmoothed':val_x_unsmoothed,\n","                            'val_x_smoothed':val_x_smoothed,\n","                            'val_y_confirmed_unsmoothed':val_y_confirmed_unsmoothed,\n","                            'val_y_confirmed_smoothed':val_y_confirmed_smoothed,\n","                            'val_y_deaths_unsmoothed':val_y_deaths_unsmoothed,\n","                            'val_y_deaths_smoothed':val_y_deaths_smoothed,\n","                            'val_y_change_in_confirmed_unsmoothed':val_y_change_in_confirmed_unsmoothed,\n","                            'val_y_change_in_confirmed_smoothed':val_y_change_in_confirmed_smoothed,\n","                            'val_y_change_in_deaths_unsmoothed':val_y_change_in_deaths_unsmoothed,\n","                            'val_y_change_in_deaths_smoothed':val_y_change_in_deaths_smoothed\n","                            }\n","\n","    testing_variables = {'test_x_unsmoothed':test_x_unsmoothed,\n","                        'test_x_smoothed':test_x_smoothed, \n","                        'test_y_confirmed_unsmoothed':test_y_confirmed_unsmoothed,\n","                        'test_y_confirmed_smoothed':test_y_confirmed_smoothed,\n","                        'test_y_deaths_unsmoothed':test_y_deaths_unsmoothed,\n","                        'test_y_deaths_smoothed':test_y_deaths_smoothed,\n","                        'test_y_change_in_confirmed_unsmoothed':test_y_change_in_confirmed_unsmoothed,\n","                        'test_y_change_in_confirmed_smoothed':test_y_change_in_confirmed_smoothed,\n","                        'test_y_change_in_deaths_unsmoothed':test_y_change_in_deaths_unsmoothed,\n","                        'test_y_change_in_deaths_smoothed':test_y_change_in_deaths_smoothed\n","                        }\n","\n","    preprocessed_data = {\n","        'training_variables':training_variables,\n","        'validation_variables':validation_variables,\n","        'testing_variables':testing_variables,\n","        'edge_index':edge_index\n","    }\n","\n","    return preprocessed_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuApCB1YrAc-","executionInfo":{"status":"ok","timestamp":1646424834635,"user_tz":360,"elapsed":258,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"ef868cbd-8962-4b18-de1a-d82dfd7a99f8"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing preprocess_data.py\n"]}]}]}