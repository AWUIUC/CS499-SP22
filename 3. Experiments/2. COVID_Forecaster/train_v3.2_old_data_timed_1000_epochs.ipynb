{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.2_old_data_timed_1000_epochs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOMT0A6HcwbPWzHKwvzsk4t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1648751908345,"user_tz":300,"elapsed":727,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"dfb1e829-2859-4778-e0ae-46ca8fb09a97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da638ad7-e829-481f-edf9-9f42810198fe","executionInfo":{"status":"ok","timestamp":1648751934744,"user_tz":300,"elapsed":26409,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.13)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.6.0)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: torch-geometric-temporal in /usr/local/lib/python3.7/dist-packages (0.51.0)\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.13)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.4)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.3)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","PyTorch has version 1.10.0+cu111\n","Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["# Try to ensure reproducibility\n","torch.manual_seed(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykTEq9k-AekW","executionInfo":{"status":"ok","timestamp":1648751934746,"user_tz":300,"elapsed":47,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"bcfa4872-8407-4b20-fca1-566c90437bc2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fe399c429f0>"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["import time\n","time_start = time.time()"],"metadata":{"id":"t0Y3iC2eSF-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 36 \n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3_reorder_input_active_cases.pickle'\n","save_model_relative_path = './saved_models/v3_2_reorder_skip_and_active_1000_epochs_timed'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3_2_archived_output_1000_epochs_timed.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1648751934749,"user_tz":300,"elapsed":33,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"73fdf578-9b81-430a-8af3-a821f11f17f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_active_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_active_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_active_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class GCN(torch.nn.Module):\n","    # def __init__(self):\n","    #     super().__init__()\n","    #     self.conv1 = GCNConv(inputLayer_num_features, hiddenLayer1_num_features)\n","    #     self.conv2 = GCNConv(hiddenLayer1_num_features, outputLayer_num_features)\n","\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","      self.linear2 = Linear(history_window, outputLayer_num_features)\n","      self.linear3 = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x) + self.linear2(data.x[:, 0:history_window])\n","\n","      x = F.elu(x)\n","      x = self.linear3(x)\n","\n","      return x\n","\n","model = GCN().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648751934878,"user_tz":300,"elapsed":18,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"c36a6a24-b857-49d3-9664-19b720cfae94"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GCN(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n","  (linear2): Linear(in_features=6, out_features=15, bias=True)\n","  (linear3): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648752324101,"user_tz":300,"elapsed":389229,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"5978d716-f6f3-4061-ee16-8d2af4839ea1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 17645602828.00, Val loss 163434208.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 1170675140.00, Val loss 47347992.00\n","Epoch 2, Loss 737479161.00, Val loss 50047376.00\n","==================================================================\n","Saved best model\n","Epoch 3, Loss 760273854.00, Val loss 46209740.00\n","Epoch 4, Loss 681232655.00, Val loss 51461872.00\n","Epoch 5, Loss 630755151.00, Val loss 53649732.00\n","Epoch 6, Loss 635920014.00, Val loss 50383804.00\n","Epoch 7, Loss 549654955.50, Val loss 47406380.00\n","Epoch 8, Loss 534148913.50, Val loss 50152952.00\n","Epoch 9, Loss 517085194.00, Val loss 49495680.00\n","Epoch 10, Loss 504663217.50, Val loss 47342276.00\n","==================================================================\n","Saved best model\n","Epoch 11, Loss 495691898.25, Val loss 43829368.00\n","==================================================================\n","Saved best model\n","Epoch 12, Loss 488093127.00, Val loss 40115480.00\n","==================================================================\n","Saved best model\n","Epoch 13, Loss 480457474.25, Val loss 36967212.00\n","==================================================================\n","Saved best model\n","Epoch 14, Loss 472249889.25, Val loss 34456016.00\n","==================================================================\n","Saved best model\n","Epoch 15, Loss 463746403.75, Val loss 32426840.00\n","==================================================================\n","Saved best model\n","Epoch 16, Loss 455367577.00, Val loss 30762896.00\n","==================================================================\n","Saved best model\n","Epoch 17, Loss 447345504.50, Val loss 29451686.00\n","==================================================================\n","Saved best model\n","Epoch 18, Loss 439699489.25, Val loss 28448274.00\n","==================================================================\n","Saved best model\n","Epoch 19, Loss 432591611.75, Val loss 27735788.00\n","==================================================================\n","Saved best model\n","Epoch 20, Loss 426375927.50, Val loss 27241978.00\n","==================================================================\n","Saved best model\n","Epoch 21, Loss 421659365.50, Val loss 26927652.00\n","==================================================================\n","Saved best model\n","Epoch 22, Loss 418584366.00, Val loss 26716032.00\n","==================================================================\n","Saved best model\n","Epoch 23, Loss 417046685.25, Val loss 26538914.00\n","==================================================================\n","Saved best model\n","Epoch 24, Loss 416577205.50, Val loss 26412534.00\n","==================================================================\n","Saved best model\n","Epoch 25, Loss 416982457.75, Val loss 26225184.00\n","==================================================================\n","Saved best model\n","Epoch 26, Loss 417869216.50, Val loss 26050792.00\n","==================================================================\n","Saved best model\n","Epoch 27, Loss 419000621.25, Val loss 25888062.00\n","==================================================================\n","Saved best model\n","Epoch 28, Loss 420195185.25, Val loss 25738838.00\n","==================================================================\n","Saved best model\n","Epoch 29, Loss 421353044.75, Val loss 25605490.00\n","==================================================================\n","Saved best model\n","Epoch 30, Loss 422468028.75, Val loss 25483364.00\n","==================================================================\n","Saved best model\n","Epoch 31, Loss 423469048.00, Val loss 25373478.00\n","==================================================================\n","Saved best model\n","Epoch 32, Loss 424369795.75, Val loss 25275168.00\n","==================================================================\n","Saved best model\n","Epoch 33, Loss 425152643.75, Val loss 25185926.00\n","==================================================================\n","Saved best model\n","Epoch 34, Loss 425867378.75, Val loss 25104802.00\n","==================================================================\n","Saved best model\n","Epoch 35, Loss 426520303.00, Val loss 25028424.00\n","==================================================================\n","Saved best model\n","Epoch 36, Loss 427090928.50, Val loss 24958924.00\n","==================================================================\n","Saved best model\n","Epoch 37, Loss 427574723.75, Val loss 24896120.00\n","==================================================================\n","Saved best model\n","Epoch 38, Loss 428042765.00, Val loss 24834554.00\n","==================================================================\n","Saved best model\n","Epoch 39, Loss 428442547.50, Val loss 24781154.00\n","Epoch 40, Loss 428742122.25, Val loss 24818732.00\n","==================================================================\n","Saved best model\n","Epoch 41, Loss 429664212.00, Val loss 24714910.00\n","==================================================================\n","Saved best model\n","Epoch 42, Loss 429450040.25, Val loss 24627736.00\n","==================================================================\n","Saved best model\n","Epoch 43, Loss 429646265.50, Val loss 24578394.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 429785403.75, Val loss 24538222.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 429987870.00, Val loss 24495482.00\n","==================================================================\n","Saved best model\n","Epoch 46, Loss 430078731.50, Val loss 24455032.00\n","==================================================================\n","Saved best model\n","Epoch 47, Loss 430212191.50, Val loss 24417818.00\n","==================================================================\n","Saved best model\n","Epoch 48, Loss 430282566.25, Val loss 24376612.00\n","==================================================================\n","Saved best model\n","Epoch 49, Loss 430269960.50, Val loss 24346850.00\n","Epoch 50, Loss 430294009.75, Val loss 24359128.00\n","==================================================================\n","Saved best model\n","Epoch 51, Loss 430475297.25, Val loss 24269784.00\n","==================================================================\n","Saved best model\n","Epoch 52, Loss 430171651.25, Val loss 24236694.00\n","==================================================================\n","Saved best model\n","Epoch 53, Loss 429995807.00, Val loss 24211046.00\n","Epoch 54, Loss 429867158.25, Val loss 24212184.00\n","==================================================================\n","Saved best model\n","Epoch 55, Loss 429754711.50, Val loss 24148990.00\n","==================================================================\n","Saved best model\n","Epoch 56, Loss 429431863.00, Val loss 24133286.00\n","==================================================================\n","Saved best model\n","Epoch 57, Loss 429246821.00, Val loss 24096380.00\n","Epoch 58, Loss 428965525.00, Val loss 24121622.00\n","==================================================================\n","Saved best model\n","Epoch 59, Loss 428874709.25, Val loss 24045552.00\n","==================================================================\n","Saved best model\n","Epoch 60, Loss 428293827.75, Val loss 24034386.00\n","Epoch 61, Loss 428014722.00, Val loss 24037256.00\n","==================================================================\n","Saved best model\n","Epoch 62, Loss 427798171.00, Val loss 23988486.00\n","==================================================================\n","Saved best model\n","Epoch 63, Loss 427304436.75, Val loss 23970438.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 426897494.25, Val loss 23956648.00\n","==================================================================\n","Saved best model\n","Epoch 65, Loss 426533387.75, Val loss 23930304.00\n","==================================================================\n","Saved best model\n","Epoch 66, Loss 426067538.25, Val loss 23924526.00\n","==================================================================\n","Saved best model\n","Epoch 67, Loss 425692834.00, Val loss 23900454.00\n","==================================================================\n","Saved best model\n","Epoch 68, Loss 425238419.00, Val loss 23893602.00\n","==================================================================\n","Saved best model\n","Epoch 69, Loss 424833959.00, Val loss 23868150.00\n","==================================================================\n","Saved best model\n","Epoch 70, Loss 424374808.50, Val loss 23861964.00\n","==================================================================\n","Saved best model\n","Epoch 71, Loss 423971106.00, Val loss 23838620.00\n","==================================================================\n","Saved best model\n","Epoch 72, Loss 423490475.25, Val loss 23835622.00\n","==================================================================\n","Saved best model\n","Epoch 73, Loss 423163723.75, Val loss 23805060.00\n","==================================================================\n","Saved best model\n","Epoch 74, Loss 422737663.00, Val loss 23787236.00\n","==================================================================\n","Saved best model\n","Epoch 75, Loss 422400009.00, Val loss 23754962.00\n","==================================================================\n","Saved best model\n","Epoch 76, Loss 421980241.75, Val loss 23741110.00\n","==================================================================\n","Saved best model\n","Epoch 77, Loss 421705915.50, Val loss 23695164.00\n","Epoch 78, Loss 2484756284.75, Val loss 28714318.00\n","Epoch 79, Loss 827472426.00, Val loss 52011860.00\n","Epoch 80, Loss 413333278.00, Val loss 36915164.00\n","Epoch 81, Loss 396920239.00, Val loss 33148744.00\n","Epoch 82, Loss 394730868.50, Val loss 33027892.00\n","Epoch 83, Loss 392376664.00, Val loss 32502926.00\n","Epoch 84, Loss 390434429.25, Val loss 31805684.00\n","Epoch 85, Loss 388956806.00, Val loss 31079670.00\n","Epoch 86, Loss 387777903.00, Val loss 30360214.00\n","Epoch 87, Loss 386853683.25, Val loss 29647858.00\n","Epoch 88, Loss 386170212.50, Val loss 28940474.00\n","Epoch 89, Loss 385569505.00, Val loss 28289260.00\n","Epoch 90, Loss 385232787.75, Val loss 27637610.00\n","Epoch 91, Loss 384827149.00, Val loss 27065786.00\n","Epoch 92, Loss 384516490.75, Val loss 26533714.00\n","Epoch 93, Loss 384303970.25, Val loss 26061394.00\n","Epoch 94, Loss 383954878.25, Val loss 25656720.00\n","Epoch 95, Loss 383739352.75, Val loss 25285632.00\n","Epoch 96, Loss 383565708.50, Val loss 294856160.00\n","==================================================================\n","Saved best model\n","Epoch 97, Loss 2401062845.50, Val loss 22830550.00\n","Epoch 98, Loss 462604089.25, Val loss 30286006.00\n","Epoch 99, Loss 380792423.25, Val loss 28895448.00\n","Epoch 100, Loss 381615190.50, Val loss 27634890.00\n","Epoch 101, Loss 382257261.00, Val loss 27472728.00\n","Epoch 102, Loss 381479439.50, Val loss 27471900.00\n","Epoch 103, Loss 380731379.25, Val loss 27440192.00\n","Epoch 104, Loss 380173781.75, Val loss 27383530.00\n","Epoch 105, Loss 379721286.75, Val loss 27278472.00\n","Epoch 106, Loss 379319931.50, Val loss 27151874.00\n","Epoch 107, Loss 379020595.75, Val loss 26980858.00\n","Epoch 108, Loss 378798576.00, Val loss 26785068.00\n","Epoch 109, Loss 378502236.50, Val loss 26574648.00\n","Epoch 110, Loss 378295070.75, Val loss 26340830.00\n","Epoch 111, Loss 378134085.00, Val loss 26068370.00\n","Epoch 112, Loss 377988556.25, Val loss 25802702.00\n","Epoch 113, Loss 377827468.50, Val loss 25531960.00\n","Epoch 114, Loss 377665497.75, Val loss 25263450.00\n","Epoch 115, Loss 377599729.75, Val loss 24980246.00\n","Epoch 116, Loss 377493633.00, Val loss 24717440.00\n","Epoch 117, Loss 377295654.75, Val loss 24486074.00\n","Epoch 118, Loss 377190543.50, Val loss 24255062.00\n","Epoch 119, Loss 376988017.25, Val loss 24066652.00\n","Epoch 120, Loss 376970281.00, Val loss 23850510.00\n","Epoch 121, Loss 376884255.75, Val loss 23680588.00\n","Epoch 122, Loss 376659844.75, Val loss 23531560.00\n","Epoch 123, Loss 376589401.25, Val loss 23389396.00\n","Epoch 124, Loss 376417533.75, Val loss 23267650.00\n","Epoch 125, Loss 376361748.75, Val loss 23160160.00\n","Epoch 126, Loss 376325014.00, Val loss 23038540.00\n","Epoch 127, Loss 376158983.50, Val loss 22964238.00\n","Epoch 128, Loss 376259420.50, Val loss 22857020.00\n","==================================================================\n","Saved best model\n","Epoch 129, Loss 376072711.00, Val loss 22811276.00\n","==================================================================\n","Saved best model\n","Epoch 130, Loss 376382497.25, Val loss 22692208.00\n","==================================================================\n","Saved best model\n","Epoch 131, Loss 376288643.25, Val loss 22635480.00\n","==================================================================\n","Saved best model\n","Epoch 132, Loss 376423287.75, Val loss 22567902.00\n","==================================================================\n","Saved best model\n","Epoch 133, Loss 376503373.75, Val loss 22514812.00\n","==================================================================\n","Saved best model\n","Epoch 134, Loss 376603288.25, Val loss 22466886.00\n","==================================================================\n","Saved best model\n","Epoch 135, Loss 376630232.25, Val loss 22433104.00\n","==================================================================\n","Saved best model\n","Epoch 136, Loss 376866095.00, Val loss 22382106.00\n","==================================================================\n","Saved best model\n","Epoch 137, Loss 377224571.75, Val loss 22341420.00\n","Epoch 138, Loss 377148960.25, Val loss 22445206.00\n","Epoch 139, Loss 378106239.00, Val loss 22386758.00\n","Epoch 140, Loss 378084991.25, Val loss 22379178.00\n","==================================================================\n","Saved best model\n","Epoch 141, Loss 378268062.50, Val loss 22328380.00\n","==================================================================\n","Saved best model\n","Epoch 142, Loss 378480276.75, Val loss 22306432.00\n","==================================================================\n","Saved best model\n","Epoch 143, Loss 378314595.75, Val loss 22294970.00\n","==================================================================\n","Saved best model\n","Epoch 144, Loss 378674466.25, Val loss 22270302.00\n","Epoch 145, Loss 378367009.50, Val loss 22290270.00\n","==================================================================\n","Saved best model\n","Epoch 146, Loss 378598387.50, Val loss 22218552.00\n","==================================================================\n","Saved best model\n","Epoch 147, Loss 378471864.50, Val loss 22207028.00\n","==================================================================\n","Saved best model\n","Epoch 148, Loss 378362246.50, Val loss 22165628.00\n","Epoch 149, Loss 378294943.00, Val loss 22177278.00\n","==================================================================\n","Saved best model\n","Epoch 150, Loss 378129550.50, Val loss 22155840.00\n","Epoch 151, Loss 377752309.25, Val loss 22162678.00\n","==================================================================\n","Saved best model\n","Epoch 152, Loss 377492591.00, Val loss 22142674.00\n","==================================================================\n","Saved best model\n","Epoch 153, Loss 377106304.75, Val loss 22130688.00\n","==================================================================\n","Saved best model\n","Epoch 154, Loss 376965835.25, Val loss 22097106.00\n","Epoch 155, Loss 376664827.00, Val loss 22099028.00\n","==================================================================\n","Saved best model\n","Epoch 156, Loss 376175246.25, Val loss 22096474.00\n","==================================================================\n","Saved best model\n","Epoch 157, Loss 375785071.75, Val loss 22085738.00\n","==================================================================\n","Saved best model\n","Epoch 158, Loss 375460155.75, Val loss 22060034.00\n","==================================================================\n","Saved best model\n","Epoch 159, Loss 374974192.00, Val loss 22040590.00\n","==================================================================\n","Saved best model\n","Epoch 160, Loss 374621854.00, Val loss 21995000.00\n","Epoch 161, Loss 374109989.25, Val loss 21995326.00\n","==================================================================\n","Saved best model\n","Epoch 162, Loss 373421684.75, Val loss 21968868.00\n","Epoch 163, Loss 372888758.75, Val loss 21981720.00\n","==================================================================\n","Saved best model\n","Epoch 164, Loss 372379690.00, Val loss 21930676.00\n","Epoch 165, Loss 371719041.00, Val loss 21948406.00\n","==================================================================\n","Saved best model\n","Epoch 166, Loss 371159784.25, Val loss 21880420.00\n","Epoch 167, Loss 370317452.00, Val loss 21886474.00\n","==================================================================\n","Saved best model\n","Epoch 168, Loss 369912570.50, Val loss 21824820.00\n","Epoch 169, Loss 369276616.50, Val loss 21876344.00\n","==================================================================\n","Saved best model\n","Epoch 170, Loss 368713479.75, Val loss 21815752.00\n","Epoch 171, Loss 367994435.75, Val loss 21873160.00\n","Epoch 172, Loss 367582717.50, Val loss 21826696.00\n","Epoch 173, Loss 366809772.00, Val loss 21842144.00\n","==================================================================\n","Saved best model\n","Epoch 174, Loss 366174400.25, Val loss 21790488.00\n","Epoch 175, Loss 365296833.00, Val loss 21820124.00\n","==================================================================\n","Saved best model\n","Epoch 176, Loss 364863653.25, Val loss 21766038.00\n","Epoch 177, Loss 364041604.75, Val loss 21789446.00\n","==================================================================\n","Saved best model\n","Epoch 178, Loss 363295785.00, Val loss 21763922.00\n","Epoch 179, Loss 362783198.00, Val loss 21802918.00\n","Epoch 180, Loss 362055988.00, Val loss 21803100.00\n","Epoch 181, Loss 361302723.25, Val loss 21846194.00\n","Epoch 182, Loss 360596097.75, Val loss 21855866.00\n","Epoch 183, Loss 359993067.75, Val loss 21885698.00\n","Epoch 184, Loss 359345710.75, Val loss 21872886.00\n","Epoch 185, Loss 358479701.00, Val loss 21939852.00\n","Epoch 186, Loss 358160897.75, Val loss 21900950.00\n","Epoch 187, Loss 357716033.75, Val loss 22052190.00\n","Epoch 188, Loss 359561511.50, Val loss 21925854.00\n","Epoch 189, Loss 360008657.00, Val loss 21937872.00\n","==================================================================\n","Saved best model\n","Epoch 190, Loss 360860187.75, Val loss 21522112.00\n","==================================================================\n","Saved best model\n","Epoch 191, Loss 361400774.00, Val loss 21393038.00\n","==================================================================\n","Saved best model\n","Epoch 192, Loss 361398933.00, Val loss 21260212.00\n","==================================================================\n","Saved best model\n","Epoch 193, Loss 360325706.25, Val loss 21213370.00\n","==================================================================\n","Saved best model\n","Epoch 194, Loss 358576292.25, Val loss 21208302.00\n","Epoch 195, Loss 357083524.50, Val loss 21243368.00\n","Epoch 196, Loss 355947047.50, Val loss 21241888.00\n","Epoch 197, Loss 355151932.25, Val loss 21256610.00\n","Epoch 198, Loss 354829906.25, Val loss 21255690.00\n","Epoch 199, Loss 353299178.25, Val loss 21251228.00\n","==================================================================\n","Saved best model\n","Epoch 200, Loss 354298779.25, Val loss 21190984.00\n","==================================================================\n","Saved best model\n","Epoch 201, Loss 353258016.00, Val loss 21178100.00\n","==================================================================\n","Saved best model\n","Epoch 202, Loss 352445336.25, Val loss 21022458.00\n","==================================================================\n","Saved best model\n","Epoch 203, Loss 351790623.25, Val loss 20997626.00\n","Epoch 204, Loss 350938280.25, Val loss 20999670.00\n","==================================================================\n","Saved best model\n","Epoch 205, Loss 350233890.50, Val loss 20994592.00\n","==================================================================\n","Saved best model\n","Epoch 206, Loss 349950705.00, Val loss 20938352.00\n","Epoch 207, Loss 349023921.75, Val loss 20960846.00\n","==================================================================\n","Saved best model\n","Epoch 208, Loss 348825218.00, Val loss 20917212.00\n","==================================================================\n","Saved best model\n","Epoch 209, Loss 348244715.50, Val loss 20905526.00\n","==================================================================\n","Saved best model\n","Epoch 210, Loss 347845035.50, Val loss 20893242.00\n","==================================================================\n","Saved best model\n","Epoch 211, Loss 347355308.00, Val loss 20847970.00\n","Epoch 212, Loss 346647632.50, Val loss 20860204.00\n","==================================================================\n","Saved best model\n","Epoch 213, Loss 346292404.75, Val loss 20800328.00\n","Epoch 214, Loss 345725862.75, Val loss 20815348.00\n","==================================================================\n","Saved best model\n","Epoch 215, Loss 345507935.50, Val loss 20774708.00\n","==================================================================\n","Saved best model\n","Epoch 216, Loss 344985378.75, Val loss 20773754.00\n","==================================================================\n","Saved best model\n","Epoch 217, Loss 344572601.00, Val loss 20767728.00\n","==================================================================\n","Saved best model\n","Epoch 218, Loss 344130652.75, Val loss 20706296.00\n","Epoch 219, Loss 343216405.25, Val loss 20740398.00\n","Epoch 220, Loss 343094335.25, Val loss 20727982.00\n","==================================================================\n","Saved best model\n","Epoch 221, Loss 342566960.00, Val loss 20684512.00\n","==================================================================\n","Saved best model\n","Epoch 222, Loss 342210422.00, Val loss 20680276.00\n","==================================================================\n","Saved best model\n","Epoch 223, Loss 341897869.50, Val loss 20618016.00\n","Epoch 224, Loss 341319964.00, Val loss 20636774.00\n","==================================================================\n","Saved best model\n","Epoch 225, Loss 340796044.25, Val loss 20617778.00\n","Epoch 226, Loss 340357149.75, Val loss 20617900.00\n","Epoch 227, Loss 339760763.25, Val loss 20625178.00\n","==================================================================\n","Saved best model\n","Epoch 228, Loss 339554649.50, Val loss 20569524.00\n","==================================================================\n","Saved best model\n","Epoch 229, Loss 338882937.25, Val loss 20560360.00\n","Epoch 230, Loss 337986845.25, Val loss 20611446.00\n","==================================================================\n","Saved best model\n","Epoch 231, Loss 338780331.50, Val loss 20483278.00\n","Epoch 232, Loss 337776795.25, Val loss 20501568.00\n","Epoch 233, Loss 337198565.00, Val loss 20514936.00\n","==================================================================\n","Saved best model\n","Epoch 234, Loss 336822100.00, Val loss 20477514.00\n","==================================================================\n","Saved best model\n","Epoch 235, Loss 336384758.25, Val loss 20442714.00\n","Epoch 236, Loss 335864737.75, Val loss 20473972.00\n","==================================================================\n","Saved best model\n","Epoch 237, Loss 335408404.50, Val loss 20441136.00\n","==================================================================\n","Saved best model\n","Epoch 238, Loss 335033081.75, Val loss 20430460.00\n","Epoch 239, Loss 334464601.25, Val loss 20444650.00\n","==================================================================\n","Saved best model\n","Epoch 240, Loss 334520512.50, Val loss 20420414.00\n","==================================================================\n","Saved best model\n","Epoch 241, Loss 333865241.75, Val loss 20369820.00\n","Epoch 242, Loss 333165014.75, Val loss 20409884.00\n","==================================================================\n","Saved best model\n","Epoch 243, Loss 332783530.75, Val loss 20369812.00\n","==================================================================\n","Saved best model\n","Epoch 244, Loss 332375524.75, Val loss 20361598.00\n","==================================================================\n","Saved best model\n","Epoch 245, Loss 331991440.25, Val loss 20344496.00\n","Epoch 246, Loss 331481840.25, Val loss 20346294.00\n","==================================================================\n","Saved best model\n","Epoch 247, Loss 331014646.25, Val loss 20322650.00\n","Epoch 248, Loss 330590312.00, Val loss 20346120.00\n","Epoch 249, Loss 329525348.25, Val loss 20394804.00\n","Epoch 250, Loss 330168878.50, Val loss 20381856.00\n","==================================================================\n","Saved best model\n","Epoch 251, Loss 330625414.50, Val loss 20243420.00\n","==================================================================\n","Saved best model\n","Epoch 252, Loss 329294985.75, Val loss 20225968.00\n","Epoch 253, Loss 328449851.50, Val loss 20251536.00\n","Epoch 254, Loss 328034969.50, Val loss 20226276.00\n","Epoch 255, Loss 327512535.00, Val loss 20246052.00\n","Epoch 256, Loss 327079889.00, Val loss 20248340.00\n","Epoch 257, Loss 326791628.50, Val loss 20256788.00\n","==================================================================\n","Saved best model\n","Epoch 258, Loss 326393073.25, Val loss 20210080.00\n","Epoch 259, Loss 325667012.25, Val loss 20217704.00\n","Epoch 260, Loss 325232622.00, Val loss 20210218.00\n","Epoch 261, Loss 324702005.75, Val loss 20216344.00\n","Epoch 262, Loss 324012273.25, Val loss 20375096.00\n","==================================================================\n","Saved best model\n","Epoch 263, Loss 325351898.25, Val loss 20183376.00\n","==================================================================\n","Saved best model\n","Epoch 264, Loss 323884686.00, Val loss 20145494.00\n","Epoch 265, Loss 322888614.25, Val loss 20164920.00\n","Epoch 266, Loss 322305116.75, Val loss 20169062.00\n","==================================================================\n","Saved best model\n","Epoch 267, Loss 322130555.00, Val loss 20140786.00\n","Epoch 268, Loss 321527658.25, Val loss 20142442.00\n","Epoch 269, Loss 321095839.75, Val loss 20141778.00\n","Epoch 270, Loss 320453599.75, Val loss 20157962.00\n","Epoch 271, Loss 319744208.25, Val loss 20308820.00\n","Epoch 272, Loss 320589281.75, Val loss 20161654.00\n","==================================================================\n","Saved best model\n","Epoch 273, Loss 319882995.25, Val loss 20076652.00\n","Epoch 274, Loss 318406189.00, Val loss 20125204.00\n","Epoch 275, Loss 318009349.25, Val loss 20133976.00\n","Epoch 276, Loss 322210703.00, Val loss 21028602.00\n","==================================================================\n","Saved best model\n","Epoch 277, Loss 321739959.00, Val loss 19723744.00\n","Epoch 278, Loss 315121382.25, Val loss 20023628.00\n","Epoch 279, Loss 315262160.75, Val loss 20124964.00\n","Epoch 280, Loss 315772330.75, Val loss 20123756.00\n","Epoch 281, Loss 315419118.75, Val loss 20249748.00\n","Epoch 282, Loss 316022523.50, Val loss 20102930.00\n","Epoch 283, Loss 315143909.75, Val loss 20040220.00\n","Epoch 284, Loss 313918347.75, Val loss 20067586.00\n","Epoch 285, Loss 313577720.00, Val loss 20075532.00\n","Epoch 286, Loss 313268528.75, Val loss 20052310.00\n","Epoch 287, Loss 312781703.50, Val loss 20085730.00\n","Epoch 288, Loss 312496791.25, Val loss 20071560.00\n","Epoch 289, Loss 312172087.75, Val loss 20036448.00\n","Epoch 290, Loss 311476167.75, Val loss 20053682.00\n","Epoch 291, Loss 310999142.50, Val loss 20071272.00\n","Epoch 292, Loss 322344286.75, Val loss 20190888.00\n","Epoch 293, Loss 314530218.75, Val loss 19940596.00\n","Epoch 294, Loss 309525911.25, Val loss 20028432.00\n","Epoch 295, Loss 309164236.25, Val loss 20065164.00\n","Epoch 296, Loss 309307694.00, Val loss 20062328.00\n","Epoch 297, Loss 308936350.75, Val loss 20027932.00\n","Epoch 298, Loss 308338315.25, Val loss 20019644.00\n","Epoch 299, Loss 307784017.50, Val loss 20003810.00\n","Epoch 300, Loss 307279369.50, Val loss 20039216.00\n","Epoch 301, Loss 306968987.25, Val loss 20021008.00\n","Epoch 302, Loss 306463249.25, Val loss 20177106.00\n","Epoch 303, Loss 301549126.50, Val loss 20831008.00\n","Epoch 304, Loss 311858978.00, Val loss 20242350.00\n","==================================================================\n","Saved best model\n","Epoch 305, Loss 309065902.75, Val loss 19679484.00\n","==================================================================\n","Saved best model\n","Epoch 306, Loss 304764180.75, Val loss 19664030.00\n","Epoch 307, Loss 303213323.75, Val loss 19912134.00\n","Epoch 308, Loss 304541559.50, Val loss 19964662.00\n","Epoch 309, Loss 305003827.75, Val loss 19952234.00\n","Epoch 310, Loss 304717594.75, Val loss 19948314.00\n","Epoch 311, Loss 304084070.25, Val loss 19975312.00\n","Epoch 312, Loss 303480956.50, Val loss 20000494.00\n","Epoch 313, Loss 303162449.75, Val loss 19980334.00\n","Epoch 314, Loss 302506052.25, Val loss 19993232.00\n","Epoch 315, Loss 302174658.75, Val loss 19973646.00\n","Epoch 316, Loss 301636850.50, Val loss 19954326.00\n","Epoch 317, Loss 300807571.50, Val loss 19967304.00\n","Epoch 318, Loss 300598537.00, Val loss 20046238.00\n","Epoch 319, Loss 300568305.00, Val loss 19994206.00\n","Epoch 320, Loss 299694835.00, Val loss 20009038.00\n","Epoch 321, Loss 299295893.50, Val loss 20000594.00\n","Epoch 322, Loss 299051351.75, Val loss 20056196.00\n","Epoch 323, Loss 298667280.25, Val loss 20035030.00\n","Epoch 324, Loss 298136579.75, Val loss 20016226.00\n","Epoch 325, Loss 297813336.50, Val loss 20080184.00\n","Epoch 326, Loss 297471422.25, Val loss 20050572.00\n","Epoch 327, Loss 296886539.50, Val loss 20030924.00\n","Epoch 328, Loss 296566799.75, Val loss 20098196.00\n","Epoch 329, Loss 296004380.75, Val loss 20090498.00\n","Epoch 330, Loss 295725314.25, Val loss 20063614.00\n","Epoch 331, Loss 295156427.25, Val loss 20141376.00\n","Epoch 332, Loss 294866904.50, Val loss 20128342.00\n","Epoch 333, Loss 294510398.00, Val loss 20097002.00\n","Epoch 334, Loss 293873054.75, Val loss 20179596.00\n","Epoch 335, Loss 293746549.50, Val loss 20142316.00\n","Epoch 336, Loss 293090233.25, Val loss 20202112.00\n","Epoch 337, Loss 291837209.00, Val loss 21121816.00\n","Epoch 338, Loss 287520892.50, Val loss 21299964.00\n","Epoch 339, Loss 300316414.00, Val loss 20323332.00\n","Epoch 340, Loss 293316149.25, Val loss 19998358.00\n","Epoch 341, Loss 289953470.50, Val loss 20354280.00\n","Epoch 342, Loss 291016078.75, Val loss 20376756.00\n","Epoch 343, Loss 291616299.75, Val loss 20286248.00\n","Epoch 344, Loss 291071800.50, Val loss 20165470.00\n","Epoch 345, Loss 290145921.75, Val loss 20111928.00\n","Epoch 346, Loss 288928884.75, Val loss 20451696.00\n","Epoch 347, Loss 290543824.00, Val loss 20424548.00\n","Epoch 348, Loss 290600026.25, Val loss 20371678.00\n","Epoch 349, Loss 289425863.75, Val loss 20328494.00\n","Epoch 350, Loss 288227902.50, Val loss 20499792.00\n","Epoch 351, Loss 288126874.00, Val loss 20566598.00\n","Epoch 352, Loss 287542416.75, Val loss 20578402.00\n","Epoch 353, Loss 287314106.25, Val loss 20604150.00\n","Epoch 354, Loss 286863605.75, Val loss 20551056.00\n","Epoch 355, Loss 285924368.50, Val loss 20525326.00\n","Epoch 356, Loss 285101152.75, Val loss 20638676.00\n","Epoch 357, Loss 285102283.00, Val loss 20638416.00\n","Epoch 358, Loss 284602686.25, Val loss 20624596.00\n","Epoch 359, Loss 283676938.75, Val loss 20642442.00\n","Epoch 360, Loss 283175576.50, Val loss 20709478.00\n","Epoch 361, Loss 282840735.00, Val loss 20744574.00\n","Epoch 362, Loss 282333522.50, Val loss 20694380.00\n","Epoch 363, Loss 281792550.00, Val loss 20755294.00\n","Epoch 364, Loss 281125983.25, Val loss 20751696.00\n","Epoch 365, Loss 280623473.50, Val loss 20825838.00\n","Epoch 366, Loss 280318351.50, Val loss 20808206.00\n","Epoch 367, Loss 279834284.00, Val loss 20862164.00\n","Epoch 368, Loss 279461957.50, Val loss 20835610.00\n","Epoch 369, Loss 278946827.00, Val loss 20878976.00\n","Epoch 370, Loss 278614525.75, Val loss 20902848.00\n","Epoch 371, Loss 278168680.75, Val loss 20932006.00\n","Epoch 372, Loss 277671552.00, Val loss 20889566.00\n","Epoch 373, Loss 277064964.50, Val loss 20968406.00\n","Epoch 374, Loss 276749452.00, Val loss 20963326.00\n","Epoch 375, Loss 276168058.25, Val loss 21017190.00\n","Epoch 376, Loss 275738790.50, Val loss 21010306.00\n","Epoch 377, Loss 275280549.75, Val loss 21097788.00\n","Epoch 378, Loss 274992226.25, Val loss 21080764.00\n","Epoch 379, Loss 274360264.00, Val loss 21123880.00\n","Epoch 380, Loss 273888874.50, Val loss 21142752.00\n","Epoch 381, Loss 273335264.75, Val loss 21184652.00\n","Epoch 382, Loss 272811363.50, Val loss 21220034.00\n","Epoch 383, Loss 272182839.25, Val loss 21242542.00\n","Epoch 384, Loss 271503484.00, Val loss 21299588.00\n","Epoch 385, Loss 271013721.75, Val loss 21319996.00\n","Epoch 386, Loss 271118402.75, Val loss 21330226.00\n","Epoch 387, Loss 269546087.25, Val loss 21209892.00\n","Epoch 388, Loss 269055396.25, Val loss 21198334.00\n","Epoch 389, Loss 268990897.25, Val loss 21451134.00\n","Epoch 390, Loss 269197785.75, Val loss 21593022.00\n","Epoch 391, Loss 269578334.00, Val loss 21531436.00\n","Epoch 392, Loss 269153749.75, Val loss 21953298.00\n","Epoch 393, Loss 268905566.25, Val loss 21689998.00\n","Epoch 394, Loss 267318121.00, Val loss 21329304.00\n","Epoch 395, Loss 265685293.00, Val loss 21121940.00\n","Epoch 396, Loss 265589161.25, Val loss 21614944.00\n","Epoch 397, Loss 265138717.75, Val loss 22131642.00\n","Epoch 398, Loss 266028814.25, Val loss 21699280.00\n","Epoch 399, Loss 265068664.75, Val loss 21563712.00\n","Epoch 400, Loss 264793529.50, Val loss 21781504.00\n","Epoch 401, Loss 262600235.50, Val loss 21466906.00\n","Epoch 402, Loss 262386876.25, Val loss 21496550.00\n","Epoch 403, Loss 263834941.25, Val loss 22450066.00\n","Epoch 404, Loss 255808602.50, Val loss 23006550.00\n","Epoch 405, Loss 266744210.25, Val loss 22780358.00\n","Epoch 406, Loss 266844724.50, Val loss 21967630.00\n","Epoch 407, Loss 263558193.00, Val loss 21590636.00\n","Epoch 408, Loss 261204561.00, Val loss 21529790.00\n","Epoch 409, Loss 260052189.50, Val loss 21673148.00\n","Epoch 410, Loss 259990733.00, Val loss 21717694.00\n","Epoch 411, Loss 259308010.25, Val loss 21941254.00\n","Epoch 412, Loss 259611768.25, Val loss 21690998.00\n","Epoch 413, Loss 259224989.50, Val loss 21862722.00\n","Epoch 414, Loss 258858174.25, Val loss 21941892.00\n","Epoch 415, Loss 258737227.50, Val loss 21851030.00\n","Epoch 416, Loss 256969803.00, Val loss 21841512.00\n","Epoch 417, Loss 256949172.75, Val loss 21972618.00\n","Epoch 418, Loss 256490601.75, Val loss 22062936.00\n","Epoch 419, Loss 256083743.00, Val loss 22049892.00\n","Epoch 420, Loss 255323961.00, Val loss 22076158.00\n","Epoch 421, Loss 255138149.25, Val loss 22165838.00\n","Epoch 422, Loss 254935497.00, Val loss 22153384.00\n","Epoch 423, Loss 254606341.50, Val loss 22163742.00\n","Epoch 424, Loss 253712048.25, Val loss 22158990.00\n","Epoch 425, Loss 253344596.50, Val loss 22210756.00\n","Epoch 426, Loss 252810836.00, Val loss 22269076.00\n","Epoch 427, Loss 252834109.50, Val loss 22303986.00\n","Epoch 428, Loss 252314719.75, Val loss 22333664.00\n","Epoch 429, Loss 252303966.25, Val loss 22367180.00\n","Epoch 430, Loss 252062806.00, Val loss 22387118.00\n","Epoch 431, Loss 251463185.00, Val loss 22413622.00\n","Epoch 432, Loss 251129458.25, Val loss 22430822.00\n","Epoch 433, Loss 250980197.25, Val loss 22494710.00\n","Epoch 434, Loss 250600424.75, Val loss 22474276.00\n","Epoch 435, Loss 249778176.25, Val loss 22484310.00\n","Epoch 436, Loss 249448243.75, Val loss 22527058.00\n","Epoch 437, Loss 248981432.75, Val loss 22556210.00\n","Epoch 438, Loss 248316686.50, Val loss 22635384.00\n","Epoch 439, Loss 248967859.75, Val loss 22722516.00\n","Epoch 440, Loss 248910453.75, Val loss 22679524.00\n","Epoch 441, Loss 248210860.50, Val loss 22690952.00\n","Epoch 442, Loss 247890131.75, Val loss 22696984.00\n","Epoch 443, Loss 247106602.50, Val loss 22709434.00\n","Epoch 444, Loss 246930567.50, Val loss 22715466.00\n","Epoch 445, Loss 246434750.00, Val loss 22636100.00\n","Epoch 446, Loss 243015643.75, Val loss 22679604.00\n","Epoch 447, Loss 246496892.50, Val loss 22806880.00\n","Epoch 448, Loss 246573905.00, Val loss 22872024.00\n","Epoch 449, Loss 246497891.75, Val loss 22959850.00\n","Epoch 450, Loss 246253199.25, Val loss 23018482.00\n","Epoch 451, Loss 245996433.00, Val loss 23038758.00\n","Epoch 452, Loss 245537721.75, Val loss 23054202.00\n","Epoch 453, Loss 245192372.75, Val loss 23085038.00\n","Epoch 454, Loss 244715867.88, Val loss 23093956.00\n","Epoch 455, Loss 244260306.50, Val loss 23111630.00\n","Epoch 456, Loss 243843016.00, Val loss 23137288.00\n","Epoch 457, Loss 243406643.12, Val loss 23148488.00\n","Epoch 458, Loss 242980298.25, Val loss 23177752.00\n","Epoch 459, Loss 242508868.38, Val loss 23190268.00\n","Epoch 460, Loss 241993676.12, Val loss 23209302.00\n","Epoch 461, Loss 241618616.75, Val loss 23242280.00\n","Epoch 462, Loss 241105319.75, Val loss 23266238.00\n","Epoch 463, Loss 240653817.75, Val loss 23299912.00\n","Epoch 464, Loss 240189242.75, Val loss 23328500.00\n","Epoch 465, Loss 239860010.88, Val loss 23363114.00\n","Epoch 466, Loss 239310521.25, Val loss 23394262.00\n","Epoch 467, Loss 238875432.00, Val loss 23420650.00\n","Epoch 468, Loss 238476399.25, Val loss 23469738.00\n","Epoch 469, Loss 237979376.75, Val loss 23480642.00\n","Epoch 470, Loss 237572908.25, Val loss 23546526.00\n","Epoch 471, Loss 236969720.38, Val loss 23565760.00\n","Epoch 472, Loss 236485781.25, Val loss 23629698.00\n","Epoch 473, Loss 236169183.62, Val loss 23638142.00\n","Epoch 474, Loss 235654507.25, Val loss 23696324.00\n","Epoch 475, Loss 235122312.38, Val loss 23708728.00\n","Epoch 476, Loss 234554187.38, Val loss 23769530.00\n","Epoch 477, Loss 233980009.75, Val loss 23794294.00\n","Epoch 478, Loss 233312282.88, Val loss 23848698.00\n","Epoch 479, Loss 233035570.75, Val loss 23873778.00\n","Epoch 480, Loss 232433646.62, Val loss 23924898.00\n","Epoch 481, Loss 231821654.50, Val loss 23948900.00\n","Epoch 482, Loss 231140937.88, Val loss 23996946.00\n","Epoch 483, Loss 230404327.50, Val loss 24019026.00\n","Epoch 484, Loss 229951891.00, Val loss 24062550.00\n","Epoch 485, Loss 229415724.12, Val loss 24085876.00\n","Epoch 486, Loss 228684217.50, Val loss 24118178.00\n","Epoch 487, Loss 227866106.62, Val loss 24130846.00\n","Epoch 488, Loss 227687242.62, Val loss 24279138.00\n","Epoch 489, Loss 226429622.00, Val loss 24145408.00\n","Epoch 490, Loss 225504474.88, Val loss 24119772.00\n","Epoch 491, Loss 224505149.88, Val loss 24090382.00\n","Epoch 492, Loss 223368952.25, Val loss 24041790.00\n","Epoch 493, Loss 222098249.25, Val loss 23953234.00\n","Epoch 494, Loss 220657114.75, Val loss 23819440.00\n","Epoch 495, Loss 218987469.00, Val loss 23604160.00\n","Epoch 496, Loss 217119511.88, Val loss 23313618.00\n","Epoch 497, Loss 214928817.12, Val loss 22881112.00\n","Epoch 498, Loss 212387049.25, Val loss 22290122.00\n","Epoch 499, Loss 209427645.88, Val loss 21470878.00\n","Epoch 500, Loss 206024376.38, Val loss 20437174.00\n","==================================================================\n","Saved best model\n","Epoch 501, Loss 188926827.38, Val loss 18107000.00\n","==================================================================\n","Saved best model\n","Epoch 502, Loss 197972014.25, Val loss 17060010.00\n","==================================================================\n","Saved best model\n","Epoch 503, Loss 191792351.12, Val loss 15501501.00\n","==================================================================\n","Saved best model\n","Epoch 504, Loss 186938016.38, Val loss 14380006.00\n","==================================================================\n","Saved best model\n","Epoch 505, Loss 200159042.88, Val loss 13616762.00\n","==================================================================\n","Saved best model\n","Epoch 506, Loss 183003937.00, Val loss 13586348.00\n","==================================================================\n","Saved best model\n","Epoch 507, Loss 180323735.50, Val loss 13226848.00\n","==================================================================\n","Saved best model\n","Epoch 508, Loss 178502296.62, Val loss 13100503.00\n","==================================================================\n","Saved best model\n","Epoch 509, Loss 177414488.38, Val loss 12972287.00\n","==================================================================\n","Saved best model\n","Epoch 510, Loss 176880530.38, Val loss 12930836.00\n","Epoch 511, Loss 177122928.62, Val loss 12968130.00\n","Epoch 512, Loss 178110990.75, Val loss 13158048.00\n","Epoch 513, Loss 180015279.75, Val loss 13527472.00\n","Epoch 514, Loss 182574761.88, Val loss 14059290.00\n","Epoch 515, Loss 185085934.25, Val loss 14572279.00\n","Epoch 516, Loss 186937464.12, Val loss 14926650.00\n","Epoch 517, Loss 179221284.50, Val loss 14748273.00\n","Epoch 518, Loss 180270754.00, Val loss 15669277.00\n","Epoch 519, Loss 185087158.38, Val loss 16234273.00\n","Epoch 520, Loss 176871920.75, Val loss 15822500.00\n","Epoch 521, Loss 183882164.12, Val loss 16389902.00\n","Epoch 522, Loss 185541109.12, Val loss 16525828.00\n","Epoch 523, Loss 178728930.75, Val loss 13950902.00\n","Epoch 524, Loss 167030943.75, Val loss 14594712.00\n","Epoch 525, Loss 178230178.62, Val loss 15872630.00\n","Epoch 526, Loss 174917474.50, Val loss 15735494.00\n","Epoch 527, Loss 172653827.50, Val loss 15450091.00\n","Epoch 528, Loss 169896256.00, Val loss 15076059.00\n","Epoch 529, Loss 168287984.75, Val loss 14698072.00\n","Epoch 530, Loss 166576884.50, Val loss 14294934.00\n","Epoch 531, Loss 165750365.12, Val loss 13937534.00\n","Epoch 532, Loss 164741055.12, Val loss 13575789.00\n","Epoch 533, Loss 165117111.88, Val loss 13325301.00\n","Epoch 534, Loss 164829238.75, Val loss 13139570.00\n","Epoch 535, Loss 166683343.12, Val loss 13081941.00\n","==================================================================\n","Saved best model\n","Epoch 536, Loss 166367183.75, Val loss 12815147.00\n","Epoch 537, Loss 175103418.62, Val loss 13385429.00\n","==================================================================\n","Saved best model\n","Epoch 538, Loss 165092758.50, Val loss 12435621.00\n","Epoch 539, Loss 169993350.88, Val loss 12746727.00\n","Epoch 540, Loss 168172989.00, Val loss 12598823.00\n","Epoch 541, Loss 168048298.88, Val loss 12580823.00\n","Epoch 542, Loss 167731957.38, Val loss 12555078.00\n","Epoch 543, Loss 167601308.62, Val loss 12562397.00\n","Epoch 544, Loss 166659903.62, Val loss 12497078.00\n","Epoch 545, Loss 166644298.25, Val loss 12471040.00\n","Epoch 546, Loss 174364732.88, Val loss 13095444.00\n","==================================================================\n","Saved best model\n","Epoch 547, Loss 161791121.50, Val loss 12246423.00\n","Epoch 548, Loss 166854577.12, Val loss 12537328.00\n","Epoch 549, Loss 164482190.75, Val loss 12418862.00\n","Epoch 550, Loss 164865767.25, Val loss 12421850.00\n","Epoch 551, Loss 164201955.88, Val loss 12408843.00\n","Epoch 552, Loss 163709803.12, Val loss 12339763.00\n","Epoch 553, Loss 163835853.88, Val loss 12341490.00\n","Epoch 554, Loss 170502925.88, Val loss 12851058.00\n","Epoch 555, Loss 162166067.88, Val loss 12254237.00\n","Epoch 556, Loss 163105802.75, Val loss 12364327.00\n","Epoch 557, Loss 161653650.12, Val loss 12298999.00\n","Epoch 558, Loss 161828078.62, Val loss 12258913.00\n","Epoch 559, Loss 161859902.12, Val loss 12254860.00\n","==================================================================\n","Saved best model\n","Epoch 560, Loss 161569835.25, Val loss 12225728.00\n","Epoch 561, Loss 168441916.12, Val loss 12714324.00\n","==================================================================\n","Saved best model\n","Epoch 562, Loss 157510750.75, Val loss 12024911.00\n","Epoch 563, Loss 162137425.12, Val loss 12309483.00\n","Epoch 564, Loss 159571436.38, Val loss 12167719.00\n","Epoch 565, Loss 160432966.12, Val loss 12184357.00\n","Epoch 566, Loss 159942300.00, Val loss 12153710.00\n","Epoch 567, Loss 160084428.38, Val loss 12146131.00\n","Epoch 568, Loss 166304723.50, Val loss 12581032.00\n","==================================================================\n","Saved best model\n","Epoch 569, Loss 155699776.88, Val loss 11969481.00\n","Epoch 570, Loss 160268907.25, Val loss 12203114.00\n","Epoch 571, Loss 158135828.00, Val loss 12083474.00\n","Epoch 572, Loss 158859811.50, Val loss 12102476.00\n","Epoch 573, Loss 158529058.62, Val loss 12083600.00\n","Epoch 574, Loss 164785100.75, Val loss 12522412.00\n","==================================================================\n","Saved best model\n","Epoch 575, Loss 154246576.50, Val loss 11900661.00\n","Epoch 576, Loss 158863507.75, Val loss 12121826.00\n","Epoch 577, Loss 156924303.88, Val loss 12026645.00\n","Epoch 578, Loss 157520076.25, Val loss 12042479.00\n","Epoch 579, Loss 157262601.62, Val loss 12023346.00\n","Epoch 580, Loss 163170806.00, Val loss 12432626.00\n","==================================================================\n","Saved best model\n","Epoch 581, Loss 153047725.12, Val loss 11835582.00\n","Epoch 582, Loss 157634520.00, Val loss 12065309.00\n","Epoch 583, Loss 155540098.50, Val loss 11958825.00\n","Epoch 584, Loss 156312304.38, Val loss 11977355.00\n","Epoch 585, Loss 155985601.38, Val loss 11952027.00\n","Epoch 586, Loss 161676845.25, Val loss 12338213.00\n","==================================================================\n","Saved best model\n","Epoch 587, Loss 151687360.38, Val loss 11769639.00\n","Epoch 588, Loss 158669646.88, Val loss 11989136.00\n","Epoch 589, Loss 155492762.88, Val loss 11811248.00\n","Epoch 590, Loss 156843524.25, Val loss 11844503.00\n","Epoch 591, Loss 156060124.50, Val loss 12746369.00\n","Epoch 592, Loss 160255673.75, Val loss 12724937.00\n","Epoch 593, Loss 155581350.00, Val loss 11799060.00\n","Epoch 594, Loss 153182839.50, Val loss 12496686.00\n","Epoch 595, Loss 150431126.50, Val loss 12289013.00\n","Epoch 596, Loss 159873853.75, Val loss 11819617.00\n","Epoch 597, Loss 153302733.25, Val loss 12397722.00\n","Epoch 598, Loss 150688516.62, Val loss 12251258.00\n","Epoch 599, Loss 156861005.75, Val loss 12624228.00\n","Epoch 600, Loss 151101737.75, Val loss 12197947.00\n","Epoch 601, Loss 158744541.12, Val loss 12787036.00\n","Epoch 602, Loss 147007145.88, Val loss 11966845.00\n","Epoch 603, Loss 157856840.12, Val loss 12712825.00\n","Epoch 604, Loss 149167101.38, Val loss 12099329.00\n","Epoch 605, Loss 153860453.25, Val loss 12500655.00\n","Epoch 606, Loss 154515075.75, Val loss 12610329.00\n","Epoch 607, Loss 145808157.88, Val loss 11886923.00\n","Epoch 608, Loss 153938727.00, Val loss 12783632.00\n","Epoch 609, Loss 153765742.62, Val loss 12553543.00\n","Epoch 610, Loss 148229302.62, Val loss 12890837.00\n","Epoch 611, Loss 155582889.25, Val loss 13686051.00\n","Epoch 612, Loss 143505947.50, Val loss 11876748.00\n","Epoch 613, Loss 184562484.12, Val loss 13565846.00\n","Epoch 614, Loss 144768213.50, Val loss 13659207.00\n","Epoch 615, Loss 141242648.50, Val loss 12157789.00\n","Epoch 616, Loss 146768971.62, Val loss 13192700.00\n","Epoch 617, Loss 145116592.25, Val loss 12516194.00\n","Epoch 618, Loss 148449278.00, Val loss 12747290.00\n","Epoch 619, Loss 146457702.75, Val loss 13027965.00\n","Epoch 620, Loss 148787362.88, Val loss 12326040.00\n","Epoch 621, Loss 147346013.88, Val loss 12187723.00\n","Epoch 622, Loss 148282383.75, Val loss 13098556.00\n","Epoch 623, Loss 150188375.75, Val loss 12992450.00\n","Epoch 624, Loss 143002342.38, Val loss 12252725.00\n","Epoch 625, Loss 160135751.25, Val loss 12693987.00\n","Epoch 626, Loss 144092229.50, Val loss 12523209.00\n","Epoch 627, Loss 142746599.38, Val loss 12700442.00\n","Epoch 628, Loss 142434081.38, Val loss 12430436.00\n","Epoch 629, Loss 149137423.88, Val loss 13080120.00\n","Epoch 630, Loss 144890517.38, Val loss 12271012.00\n","Epoch 631, Loss 153101043.00, Val loss 13942454.00\n","Epoch 632, Loss 194858371.62, Val loss 15181321.00\n","Epoch 633, Loss 140673425.50, Val loss 12159703.00\n","Epoch 634, Loss 141562911.75, Val loss 12795270.00\n","Epoch 635, Loss 158689472.75, Val loss 13203061.00\n","Epoch 636, Loss 144960309.50, Val loss 12188312.00\n","Epoch 637, Loss 145716833.50, Val loss 12669235.00\n","Epoch 638, Loss 143893674.12, Val loss 12410140.00\n","Epoch 639, Loss 145445265.00, Val loss 12453171.00\n","Epoch 640, Loss 145737948.38, Val loss 12407394.00\n","Epoch 641, Loss 146475196.00, Val loss 12389400.00\n","Epoch 642, Loss 146994672.25, Val loss 12400228.00\n","Epoch 643, Loss 151099665.25, Val loss 12257038.00\n","Epoch 644, Loss 145020369.62, Val loss 12705111.00\n","Epoch 645, Loss 143721178.25, Val loss 12149104.00\n","Epoch 646, Loss 150826181.12, Val loss 12273747.00\n","Epoch 647, Loss 141685686.38, Val loss 12538817.00\n","Epoch 648, Loss 141452915.62, Val loss 12488619.00\n","Epoch 649, Loss 152352802.00, Val loss 13562536.00\n","Epoch 650, Loss 137080889.88, Val loss 12181474.00\n","==================================================================\n","Saved best model\n","Epoch 651, Loss 160668048.00, Val loss 11509303.00\n","Epoch 652, Loss 150290229.00, Val loss 12339964.00\n","Epoch 653, Loss 146765027.75, Val loss 12312457.00\n","Epoch 654, Loss 150213834.62, Val loss 12716984.00\n","Epoch 655, Loss 142689571.12, Val loss 12195937.00\n","Epoch 656, Loss 146431584.12, Val loss 12778526.00\n","Epoch 657, Loss 147174289.50, Val loss 12550393.00\n","Epoch 658, Loss 143331434.38, Val loss 12323084.00\n","Epoch 659, Loss 145928425.88, Val loss 12765298.00\n","Epoch 660, Loss 146215555.88, Val loss 12723757.00\n","Epoch 661, Loss 142035459.00, Val loss 12328883.00\n","Epoch 662, Loss 145410777.50, Val loss 12851398.00\n","Epoch 663, Loss 144042665.50, Val loss 12614479.00\n","Epoch 664, Loss 142059493.88, Val loss 12415092.00\n","Epoch 665, Loss 147551274.00, Val loss 12794322.00\n","Epoch 666, Loss 141794724.25, Val loss 12394761.00\n","Epoch 667, Loss 145275651.12, Val loss 12733898.00\n","Epoch 668, Loss 143312527.25, Val loss 12587255.00\n","Epoch 669, Loss 146861993.62, Val loss 12377442.00\n","Epoch 670, Loss 143658872.75, Val loss 12219200.00\n","Epoch 671, Loss 144062923.50, Val loss 12328531.00\n","Epoch 672, Loss 143348250.12, Val loss 12364425.00\n","Epoch 673, Loss 142737236.12, Val loss 12326581.00\n","Epoch 674, Loss 142926627.00, Val loss 12415460.00\n","Epoch 675, Loss 146162383.00, Val loss 12501834.00\n","Epoch 676, Loss 140891076.38, Val loss 12216464.00\n","Epoch 677, Loss 143540384.00, Val loss 12374113.00\n","Epoch 678, Loss 142510059.75, Val loss 12393836.00\n","Epoch 679, Loss 142136180.75, Val loss 12337437.00\n","Epoch 680, Loss 142393338.00, Val loss 12426689.00\n","Epoch 681, Loss 141377409.50, Val loss 12333980.00\n","Epoch 682, Loss 141862417.38, Val loss 12378134.00\n","Epoch 683, Loss 141984213.00, Val loss 12329305.00\n","Epoch 684, Loss 142295866.75, Val loss 12429067.00\n","Epoch 685, Loss 141004744.62, Val loss 12331800.00\n","Epoch 686, Loss 141258059.62, Val loss 12424099.00\n","Epoch 687, Loss 149735925.00, Val loss 13020545.00\n","Epoch 688, Loss 136713383.38, Val loss 12236287.00\n","Epoch 689, Loss 139060594.38, Val loss 12164177.00\n","Epoch 690, Loss 140903022.25, Val loss 12383685.00\n","Epoch 691, Loss 139419022.75, Val loss 12264565.00\n","Epoch 692, Loss 140778650.75, Val loss 12437261.00\n","Epoch 693, Loss 138791607.62, Val loss 12293716.00\n","Epoch 694, Loss 145443570.38, Val loss 12820807.00\n","Epoch 695, Loss 136001385.12, Val loss 12034760.00\n","Epoch 696, Loss 140888677.75, Val loss 12500833.00\n","Epoch 697, Loss 142245184.25, Val loss 12119233.00\n","Epoch 698, Loss 141148797.12, Val loss 12650718.00\n","Epoch 699, Loss 140493030.12, Val loss 12731912.00\n","Epoch 700, Loss 142230056.25, Val loss 12350303.00\n","Epoch 701, Loss 140878625.00, Val loss 12212320.00\n","Epoch 702, Loss 142651723.38, Val loss 12507406.00\n","Epoch 703, Loss 140179299.00, Val loss 12347728.00\n","Epoch 704, Loss 141756335.62, Val loss 12521253.00\n","Epoch 705, Loss 140122798.75, Val loss 12394961.00\n","Epoch 706, Loss 141481511.88, Val loss 12539381.00\n","Epoch 707, Loss 140007696.50, Val loss 12413210.00\n","Epoch 708, Loss 141351673.50, Val loss 12551316.00\n","Epoch 709, Loss 139956000.38, Val loss 12436357.00\n","Epoch 710, Loss 141101863.75, Val loss 12551712.00\n","Epoch 711, Loss 139777216.88, Val loss 12431733.00\n","Epoch 712, Loss 141056463.75, Val loss 12563129.00\n","Epoch 713, Loss 139603380.62, Val loss 12439514.00\n","Epoch 714, Loss 140886296.88, Val loss 12561606.00\n","Epoch 715, Loss 138861601.50, Val loss 12433341.00\n","Epoch 716, Loss 139557250.12, Val loss 12531470.00\n","Epoch 717, Loss 137486998.38, Val loss 12398012.00\n","Epoch 718, Loss 137959135.88, Val loss 12479978.00\n","Epoch 719, Loss 141451122.62, Val loss 12772525.00\n","Epoch 720, Loss 136704935.62, Val loss 12582702.00\n","==================================================================\n","Saved best model\n","Epoch 721, Loss 252791984.00, Val loss 10777098.00\n","Epoch 722, Loss 149862918.25, Val loss 13607571.00\n","Epoch 723, Loss 130725358.31, Val loss 12142742.00\n","Epoch 724, Loss 128974657.88, Val loss 11915567.00\n","Epoch 725, Loss 129103629.88, Val loss 11742942.00\n","Epoch 726, Loss 129856336.62, Val loss 11611627.00\n","Epoch 727, Loss 131277756.50, Val loss 11511170.00\n","Epoch 728, Loss 133084940.62, Val loss 12442385.00\n","Epoch 729, Loss 132106424.25, Val loss 13167615.00\n","Epoch 730, Loss 132886819.88, Val loss 12638150.00\n","Epoch 731, Loss 135424402.00, Val loss 12612468.00\n","Epoch 732, Loss 136371063.62, Val loss 12552447.00\n","Epoch 733, Loss 138083104.50, Val loss 12608948.00\n","Epoch 734, Loss 137649040.00, Val loss 12517881.00\n","Epoch 735, Loss 138950551.50, Val loss 12580621.00\n","Epoch 736, Loss 138887385.62, Val loss 12570447.00\n","Epoch 737, Loss 138232676.62, Val loss 12497780.00\n","Epoch 738, Loss 139422648.25, Val loss 12590651.00\n","Epoch 739, Loss 139127077.38, Val loss 12573716.00\n","Epoch 740, Loss 138369470.62, Val loss 12518296.00\n","Epoch 741, Loss 139244471.75, Val loss 12580292.00\n","Epoch 742, Loss 139255114.75, Val loss 12608028.00\n","Epoch 743, Loss 138068120.75, Val loss 12507147.00\n","Epoch 744, Loss 139196363.25, Val loss 12607095.00\n","Epoch 745, Loss 138844784.50, Val loss 12586625.00\n","Epoch 746, Loss 138062243.38, Val loss 12536807.00\n","Epoch 747, Loss 138824826.12, Val loss 12585698.00\n","Epoch 748, Loss 138959638.25, Val loss 12628153.00\n","Epoch 749, Loss 137631713.75, Val loss 12511379.00\n","Epoch 750, Loss 138897148.38, Val loss 12626243.00\n","Epoch 751, Loss 138450595.25, Val loss 12594091.00\n","Epoch 752, Loss 137779198.88, Val loss 12556263.00\n","Epoch 753, Loss 138083806.00, Val loss 12605865.00\n","Epoch 754, Loss 137516260.25, Val loss 12527185.00\n","Epoch 755, Loss 138668724.38, Val loss 12600344.00\n","Epoch 756, Loss 138573494.88, Val loss 12642162.00\n","Epoch 757, Loss 136787762.25, Val loss 12511418.00\n","Epoch 758, Loss 139022919.62, Val loss 12791539.00\n","Epoch 759, Loss 135219278.25, Val loss 12342185.00\n","Epoch 760, Loss 139384194.75, Val loss 12661550.00\n","Epoch 761, Loss 137339194.25, Val loss 12549939.00\n","Epoch 762, Loss 138136859.75, Val loss 12627401.00\n","Epoch 763, Loss 136683707.50, Val loss 12514072.00\n","Epoch 764, Loss 137766432.62, Val loss 12651372.00\n","Epoch 765, Loss 136397676.12, Val loss 12480929.00\n","Epoch 766, Loss 138521629.75, Val loss 12646674.00\n","Epoch 767, Loss 137500695.12, Val loss 12603584.00\n","Epoch 768, Loss 136943548.25, Val loss 12556028.00\n","Epoch 769, Loss 137187009.62, Val loss 12638111.00\n","Epoch 770, Loss 136541968.38, Val loss 12510471.00\n","Epoch 771, Loss 137942370.12, Val loss 12633902.00\n","Epoch 772, Loss 137592497.00, Val loss 12634595.00\n","Epoch 773, Loss 137114603.75, Val loss 12616451.00\n","Epoch 774, Loss 137330378.62, Val loss 12642312.00\n","Epoch 775, Loss 136989769.75, Val loss 12633838.00\n","Epoch 776, Loss 137048404.88, Val loss 12645712.00\n","Epoch 777, Loss 136929503.88, Val loss 12647646.00\n","Epoch 778, Loss 136815215.75, Val loss 12644508.00\n","Epoch 779, Loss 136898404.50, Val loss 12664898.00\n","Epoch 780, Loss 136635831.88, Val loss 12647461.00\n","Epoch 781, Loss 136774869.88, Val loss 12667728.00\n","Epoch 782, Loss 135514440.12, Val loss 12586496.00\n","Epoch 783, Loss 136345373.75, Val loss 12660112.00\n","Epoch 784, Loss 135636939.88, Val loss 12542535.00\n","Epoch 785, Loss 136982897.38, Val loss 12698226.00\n","Epoch 786, Loss 135363255.25, Val loss 12532564.00\n","Epoch 787, Loss 136999960.50, Val loss 12699118.00\n","Epoch 788, Loss 135284491.12, Val loss 12537382.00\n","Epoch 789, Loss 136913160.00, Val loss 12701791.00\n","Epoch 790, Loss 135198133.88, Val loss 12539207.00\n","Epoch 791, Loss 136840514.50, Val loss 12703772.00\n","Epoch 792, Loss 135107490.75, Val loss 12536945.00\n","Epoch 793, Loss 136770645.75, Val loss 12706786.00\n","Epoch 794, Loss 135013056.25, Val loss 12542427.00\n","Epoch 795, Loss 136681744.38, Val loss 12707873.00\n","Epoch 796, Loss 134916568.25, Val loss 12545036.00\n","Epoch 797, Loss 136582269.75, Val loss 12708714.00\n","Epoch 798, Loss 134814770.25, Val loss 12546150.00\n","Epoch 799, Loss 136500243.62, Val loss 12711186.00\n","Epoch 800, Loss 134717030.12, Val loss 12547709.00\n","Epoch 801, Loss 136435843.62, Val loss 12717891.00\n","Epoch 802, Loss 134652233.38, Val loss 12556599.00\n","Epoch 803, Loss 136364902.88, Val loss 12738133.00\n","Epoch 804, Loss 134448525.50, Val loss 12561743.00\n","Epoch 805, Loss 136183294.38, Val loss 12733595.00\n","Epoch 806, Loss 134322091.50, Val loss 12556632.00\n","Epoch 807, Loss 136077673.75, Val loss 12724038.00\n","Epoch 808, Loss 134227945.75, Val loss 12554072.00\n","Epoch 809, Loss 135948590.62, Val loss 12723923.00\n","Epoch 810, Loss 134105309.00, Val loss 12553931.00\n","Epoch 811, Loss 135827405.38, Val loss 12720103.00\n","Epoch 812, Loss 132234833.00, Val loss 12511659.00\n","Epoch 813, Loss 138807205.12, Val loss 13054984.00\n","Epoch 814, Loss 127689246.88, Val loss 12202996.00\n","Epoch 815, Loss 165837475.38, Val loss 13495624.00\n","Epoch 816, Loss 129824927.81, Val loss 13098325.00\n","Epoch 817, Loss 129370500.25, Val loss 13304029.00\n","Epoch 818, Loss 128549752.25, Val loss 13056576.00\n","Epoch 819, Loss 130666296.88, Val loss 13074167.00\n","Epoch 820, Loss 130905574.12, Val loss 12989405.00\n","Epoch 821, Loss 132319241.50, Val loss 13001633.00\n","Epoch 822, Loss 132533983.88, Val loss 12973313.00\n","Epoch 823, Loss 133398869.25, Val loss 12981600.00\n","Epoch 824, Loss 133366299.88, Val loss 12967887.00\n","Epoch 825, Loss 133957405.00, Val loss 12983152.00\n","Epoch 826, Loss 133730786.88, Val loss 12970386.00\n","Epoch 827, Loss 134166817.50, Val loss 12986964.00\n","Epoch 828, Loss 133841680.25, Val loss 12977325.00\n","Epoch 829, Loss 134187656.38, Val loss 12992906.00\n","Epoch 830, Loss 133845093.00, Val loss 12985113.00\n","Epoch 831, Loss 134113912.38, Val loss 12997396.00\n","Epoch 832, Loss 133801148.88, Val loss 12993343.00\n","Epoch 833, Loss 133988265.50, Val loss 12999689.00\n","Epoch 834, Loss 133745846.38, Val loss 13002470.00\n","Epoch 835, Loss 133849286.75, Val loss 13003605.00\n","Epoch 836, Loss 133678494.00, Val loss 13008321.00\n","Epoch 837, Loss 133726088.12, Val loss 13004104.00\n","Epoch 838, Loss 133595154.62, Val loss 13014021.00\n","Epoch 839, Loss 133609045.25, Val loss 13008887.00\n","Epoch 840, Loss 133511200.00, Val loss 13022687.00\n","Epoch 841, Loss 133479824.12, Val loss 13011390.00\n","Epoch 842, Loss 133431066.25, Val loss 13031613.00\n","Epoch 843, Loss 133354355.25, Val loss 13014981.00\n","Epoch 844, Loss 133346334.38, Val loss 13038878.00\n","Epoch 845, Loss 133242186.88, Val loss 13015425.00\n","Epoch 846, Loss 133275876.88, Val loss 13044529.00\n","Epoch 847, Loss 134055207.25, Val loss 12906199.00\n","Epoch 848, Loss 133663377.50, Val loss 13037140.00\n","Epoch 849, Loss 133317443.38, Val loss 13026528.00\n","Epoch 850, Loss 133154750.88, Val loss 13049024.00\n","Epoch 851, Loss 133032990.00, Val loss 13037684.00\n","Epoch 852, Loss 133021461.62, Val loss 13059814.00\n","Epoch 853, Loss 132878144.62, Val loss 13045912.00\n","Epoch 854, Loss 132914055.12, Val loss 13065234.00\n","Epoch 855, Loss 132781942.25, Val loss 13052432.00\n","Epoch 856, Loss 132822238.88, Val loss 13069504.00\n","Epoch 857, Loss 132688966.50, Val loss 13057634.00\n","Epoch 858, Loss 132741921.38, Val loss 13073068.00\n","Epoch 859, Loss 132601455.25, Val loss 13062816.00\n","Epoch 860, Loss 132663792.50, Val loss 13078134.00\n","Epoch 861, Loss 132522934.62, Val loss 13065996.00\n","Epoch 862, Loss 132583274.38, Val loss 13080243.00\n","Epoch 863, Loss 132442630.00, Val loss 13070361.00\n","Epoch 864, Loss 132502126.50, Val loss 13084790.00\n","Epoch 865, Loss 132369370.38, Val loss 13072732.00\n","Epoch 866, Loss 132416813.38, Val loss 13088484.00\n","Epoch 867, Loss 132301326.12, Val loss 13079303.00\n","Epoch 868, Loss 132336303.50, Val loss 13090418.00\n","Epoch 869, Loss 132227946.38, Val loss 13083532.00\n","Epoch 870, Loss 132260202.38, Val loss 13092475.00\n","Epoch 871, Loss 132161944.62, Val loss 13087358.00\n","Epoch 872, Loss 132182245.00, Val loss 13096397.00\n","Epoch 873, Loss 132095072.25, Val loss 13088395.00\n","Epoch 874, Loss 132116390.50, Val loss 13096242.00\n","Epoch 875, Loss 132030896.38, Val loss 13094212.00\n","Epoch 876, Loss 132077735.00, Val loss 13095348.00\n","Epoch 877, Loss 132004963.62, Val loss 13102681.00\n","Epoch 878, Loss 132098419.25, Val loss 13087256.00\n","Epoch 879, Loss 132130548.38, Val loss 13133303.00\n","Epoch 880, Loss 131653330.62, Val loss 13098055.00\n","Epoch 881, Loss 132152008.88, Val loss 13080799.00\n","Epoch 882, Loss 132185175.88, Val loss 13161493.00\n","Epoch 883, Loss 131313205.88, Val loss 13086610.00\n","Epoch 884, Loss 132094234.88, Val loss 13085826.00\n","Epoch 885, Loss 132018141.88, Val loss 13155505.00\n","Epoch 886, Loss 131220281.12, Val loss 13083917.00\n","Epoch 887, Loss 131935732.38, Val loss 13070361.00\n","Epoch 888, Loss 131912186.75, Val loss 13147878.00\n","Epoch 889, Loss 131068197.50, Val loss 13066356.00\n","Epoch 890, Loss 131892277.50, Val loss 13069395.00\n","Epoch 891, Loss 131731121.88, Val loss 13135771.00\n","Epoch 892, Loss 131021064.50, Val loss 13066174.00\n","Epoch 893, Loss 131729057.75, Val loss 13062468.00\n","Epoch 894, Loss 131667168.88, Val loss 13137791.00\n","Epoch 895, Loss 130844362.25, Val loss 13059915.00\n","Epoch 896, Loss 131686823.12, Val loss 13069267.00\n","Epoch 897, Loss 131478033.50, Val loss 13134251.00\n","Epoch 898, Loss 130819674.75, Val loss 13064646.00\n","Epoch 899, Loss 131483454.25, Val loss 13067541.00\n","Epoch 900, Loss 131468616.50, Val loss 13138509.00\n","Epoch 901, Loss 130595574.00, Val loss 13063836.00\n","Epoch 902, Loss 131508550.88, Val loss 13076051.00\n","Epoch 903, Loss 131223758.75, Val loss 13134505.00\n","Epoch 904, Loss 130598387.12, Val loss 13073610.00\n","Epoch 905, Loss 130920280.12, Val loss 13094380.00\n","Epoch 906, Loss 130874431.00, Val loss 13095462.00\n","Epoch 907, Loss 130828767.12, Val loss 13096078.00\n","Epoch 908, Loss 130811683.25, Val loss 13103936.00\n","Epoch 909, Loss 130754324.75, Val loss 13095704.00\n","Epoch 910, Loss 130781835.38, Val loss 13111875.00\n","Epoch 911, Loss 130613580.25, Val loss 13099814.00\n","Epoch 912, Loss 130665223.75, Val loss 13104139.00\n","Epoch 913, Loss 130670966.50, Val loss 13100242.00\n","Epoch 914, Loss 130652352.75, Val loss 13114919.00\n","Epoch 915, Loss 130414957.38, Val loss 13104783.00\n","Epoch 916, Loss 130523840.38, Val loss 13095738.00\n","Epoch 917, Loss 130525194.75, Val loss 13119772.00\n","Epoch 918, Loss 130344090.38, Val loss 13095083.00\n","Epoch 919, Loss 130444368.75, Val loss 13118362.00\n","Epoch 920, Loss 130320845.75, Val loss 13098722.00\n","Epoch 921, Loss 130351363.62, Val loss 13119408.00\n","Epoch 922, Loss 130249231.38, Val loss 13099223.00\n","Epoch 923, Loss 130309551.50, Val loss 13121000.00\n","Epoch 924, Loss 130150855.75, Val loss 13102047.00\n","Epoch 925, Loss 130193116.75, Val loss 13120572.00\n","Epoch 926, Loss 130181991.75, Val loss 13098504.00\n","Epoch 927, Loss 130167967.12, Val loss 13126489.00\n","Epoch 928, Loss 129983475.88, Val loss 13101334.00\n","Epoch 929, Loss 130111912.75, Val loss 13122970.00\n","Epoch 930, Loss 129943355.00, Val loss 13106893.00\n","Epoch 931, Loss 130055785.75, Val loss 13115103.00\n","Epoch 932, Loss 129955359.75, Val loss 13112978.00\n","Epoch 933, Loss 129887220.75, Val loss 13118183.00\n","Epoch 934, Loss 129964458.12, Val loss 13111700.00\n","Epoch 935, Loss 129859746.38, Val loss 13124949.00\n","Epoch 936, Loss 129738992.25, Val loss 13111388.00\n","Epoch 937, Loss 129818545.12, Val loss 13122928.00\n","Epoch 938, Loss 129705659.62, Val loss 13118283.00\n","Epoch 939, Loss 129741660.50, Val loss 13109178.00\n","Epoch 940, Loss 129741872.25, Val loss 13127085.00\n","Epoch 941, Loss 129564830.62, Val loss 13112908.00\n","Epoch 942, Loss 129672200.00, Val loss 13110802.00\n","Epoch 943, Loss 129693309.88, Val loss 13126996.00\n","Epoch 944, Loss 129461301.12, Val loss 13115676.00\n","Epoch 945, Loss 129501153.38, Val loss 13120882.00\n","Epoch 946, Loss 129523502.12, Val loss 13121346.00\n","Epoch 947, Loss 129403341.25, Val loss 13118742.00\n","Epoch 948, Loss 129491511.75, Val loss 13125604.00\n","Epoch 949, Loss 129362932.62, Val loss 13119316.00\n","Epoch 950, Loss 129402748.12, Val loss 13131965.00\n","Epoch 951, Loss 129312639.38, Val loss 13116381.00\n","Epoch 952, Loss 129474364.25, Val loss 13140439.00\n","Epoch 953, Loss 129287909.00, Val loss 13111960.00\n","Epoch 954, Loss 129441679.88, Val loss 13155004.00\n","Epoch 955, Loss 129268913.62, Val loss 13103134.00\n","Epoch 956, Loss 129546648.50, Val loss 13158727.00\n","Epoch 957, Loss 129233803.88, Val loss 13110202.00\n","Epoch 958, Loss 129558671.00, Val loss 13155012.00\n","Epoch 959, Loss 129231940.62, Val loss 13127802.00\n","Epoch 960, Loss 129215328.25, Val loss 13146810.00\n","Epoch 961, Loss 129110101.50, Val loss 13137027.00\n","Epoch 962, Loss 129188372.62, Val loss 13156322.00\n","Epoch 963, Loss 128995676.12, Val loss 13144437.00\n","Epoch 964, Loss 129201709.75, Val loss 13165816.00\n","Epoch 965, Loss 128928078.12, Val loss 13154895.00\n","Epoch 966, Loss 129119413.50, Val loss 13170981.00\n","Epoch 967, Loss 128860257.12, Val loss 13161632.00\n","Epoch 968, Loss 129070671.75, Val loss 13159648.00\n","Epoch 969, Loss 125764847.50, Val loss 13073563.00\n","Epoch 970, Loss 206009022.88, Val loss 13553031.00\n","Epoch 971, Loss 133541279.00, Val loss 14485090.00\n","Epoch 972, Loss 119302270.88, Val loss 13326264.00\n","Epoch 973, Loss 121352374.62, Val loss 13457291.00\n","Epoch 974, Loss 121872476.12, Val loss 13260757.00\n","Epoch 975, Loss 122569790.62, Val loss 13109711.00\n","Epoch 976, Loss 123912239.62, Val loss 13056125.00\n","Epoch 977, Loss 124665582.25, Val loss 12964313.00\n","Epoch 978, Loss 125855313.88, Val loss 12961893.00\n","Epoch 979, Loss 126335951.25, Val loss 12900263.00\n","Epoch 980, Loss 127217594.25, Val loss 12926133.00\n","Epoch 981, Loss 127405491.25, Val loss 12880031.00\n","Epoch 982, Loss 127978145.25, Val loss 12914794.00\n","Epoch 983, Loss 127961889.62, Val loss 12875169.00\n","Epoch 984, Loss 128348116.25, Val loss 12913858.00\n","Epoch 985, Loss 128208350.62, Val loss 12874551.00\n","Epoch 986, Loss 128505583.88, Val loss 12915858.00\n","Epoch 987, Loss 128294360.00, Val loss 12875328.00\n","Epoch 988, Loss 128554463.00, Val loss 12918587.00\n","Epoch 989, Loss 128304363.62, Val loss 12877116.00\n","Epoch 990, Loss 128543317.12, Val loss 12919908.00\n","Epoch 991, Loss 128279257.00, Val loss 12880397.00\n","Epoch 992, Loss 128498016.62, Val loss 12922014.00\n","Epoch 993, Loss 128230626.25, Val loss 12882247.00\n","Epoch 994, Loss 128448618.38, Val loss 12924314.00\n","Epoch 995, Loss 128170749.50, Val loss 12883762.00\n","Epoch 996, Loss 128392667.75, Val loss 12925713.00\n","Epoch 997, Loss 128107770.62, Val loss 12884992.00\n","Epoch 998, Loss 128341484.12, Val loss 12926792.00\n","Epoch 999, Loss 128050598.38, Val loss 12887703.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"status":"ok","timestamp":1648752324345,"user_tz":300,"elapsed":267,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"2a8e7f32-8502-49ea-f069-ded7ad073533"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["277227712.0\n","tensor([2710.8604, 2848.6257, 2981.0640, 3111.3496, 3250.3206, 3390.3533,\n","        3533.4963, 3671.5396, 3809.4250, 3945.7668, 4082.0212, 4212.5977,\n","        4332.6919, 4452.0728, 4571.3066], grad_fn=<SelectBackward0>)\n","tensor([2908., 2871., 3187., 3222., 3421., 3617., 3844., 3908., 3987., 4006.,\n","        4123., 4544., 4193., 4722., 4866.])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","     # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_number_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_number_infected_tensor':labeled_output # (52, 15)\n","}\n","\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)\n","\n","\n","# print(model_predictions_number_infected['Alabama'])\n","# print(ground_truth_number_infected['Alabama'])"],"metadata":{"id":"s3-8Yge6CAWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_squared_error = criterion(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","mae_function = torch.nn.L1Loss()\n","mean_absolute_error = mae_function(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","print(\"mean squared error: \", mean_squared_error)\n","print(\"mean absolute error: \", mean_absolute_error)\n","\n","# With seed = 0 - trial 1\n","# mean squared error:  277227712.0\n","# mean absolute error:  6635.75830078125\n","\n","# With seed = 0 - trial 2\n","# mean squared error:  277227712.0\n","# mean absolute error:  6635.75830078125"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"epsUB5-PBOHP","executionInfo":{"status":"ok","timestamp":1648752324350,"user_tz":300,"elapsed":39,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"650f4a8d-b070-4ace-c4eb-1e8e5c771612"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean squared error:  277227712.0\n","mean absolute error:  6635.75830078125\n"]}]},{"cell_type":"code","source":["time_end = time.time()\n","time_seconds = time_end - time_start\n","(t_min, t_sec) = divmod(time_seconds,60)\n","(t_hour,t_min) = divmod(t_min,60) \n","print('Time passed: {} hours:{} minutes:{} seconds'.format(t_hour,t_min,t_sec))\n","# Time passed: 0.0 hours:6.0 minutes:50.026517391204834 seconds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFdb-o9NR6co","executionInfo":{"status":"ok","timestamp":1648752324353,"user_tz":300,"elapsed":39,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"aaa94071-376a-4a24-bd45-f2ba4298c135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Time passed: 0.0 hours:6.0 minutes:29.605124473571777 seconds\n"]}]}]}