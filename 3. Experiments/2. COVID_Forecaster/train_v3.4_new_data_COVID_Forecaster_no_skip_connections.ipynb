{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.4_new_data_COVID_Forecaster_no_skip_connections.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN2r9qmmMZ7wbx8fVd0Mvna"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1648743631722,"user_tz":300,"elapsed":747,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"c2757aec-b31b-422e-f590-532f0c212bc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fedcfc75-f654-410e-bedd-e4c20a11a28b","executionInfo":{"status":"ok","timestamp":1648743659077,"user_tz":300,"elapsed":27364,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.13)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.6.0)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: torch-geometric-temporal in /usr/local/lib/python3.7/dist-packages (0.51.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.13)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.3)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","PyTorch has version 1.10.0+cu111\n","Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 24\n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3.3_to_3.5_new_data.pickle'\n","save_model_relative_path = './saved_models/v3.4_new_data_COVID_Forecaster_no_skip_connections'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3.4_archived_output.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1648743659078,"user_tz":300,"elapsed":13,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"abb7bb79-0837-46dd-e990-e523f30fcd5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["# Try to ensure reproducibility\n","torch.manual_seed(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zK_8i7ChknBQ","executionInfo":{"status":"ok","timestamp":1648743659204,"user_tz":300,"elapsed":133,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"8fbf7de9-b8aa-468e-ad28-c5615839ead0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f762408dbd0>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_confirmed_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_confirmed_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_confirmed_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class COVID_forecaster_no_skip(torch.nn.Module):\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","      # self.linear2 = Linear(history_window, outputLayer_num_features)\n","      self.linear3 = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x) # + self.linear2(data.x[:, 0:history_window])\n","\n","      x = F.elu(x)\n","      x = self.linear3(x)\n","\n","      return x\n","\n","model = COVID_forecaster_no_skip().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648743659375,"user_tz":300,"elapsed":42,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"0723acc2-a60c-490e-be5f-3e0ce56ccb03"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["COVID_forecaster_no_skip(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n","  (linear3): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648745284143,"user_tz":300,"elapsed":1624783,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"1bb437fe-5854-4fcb-90a1-479dcae03df3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 94081830569472.00, Val loss 4626484559872.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 93959255288064.00, Val loss 4622414512128.00\n","==================================================================\n","Saved best model\n","Epoch 2, Loss 93732288338432.00, Val loss 4615775977472.00\n","==================================================================\n","Saved best model\n","Epoch 3, Loss 93401604307712.00, Val loss 4606657560576.00\n","==================================================================\n","Saved best model\n","Epoch 4, Loss 92973555450624.00, Val loss 4595206586368.00\n","==================================================================\n","Saved best model\n","Epoch 5, Loss 92456056790528.00, Val loss 4581585059840.00\n","==================================================================\n","Saved best model\n","Epoch 6, Loss 91857582640896.00, Val loss 4565962326016.00\n","==================================================================\n","Saved best model\n","Epoch 7, Loss 91186612429312.00, Val loss 4548506157056.00\n","==================================================================\n","Saved best model\n","Epoch 8, Loss 90451253819136.00, Val loss 4529371217920.00\n","==================================================================\n","Saved best model\n","Epoch 9, Loss 89659996851968.00, Val loss 4508718989312.00\n","==================================================================\n","Saved best model\n","Epoch 10, Loss 88820686750464.00, Val loss 4486716719104.00\n","==================================================================\n","Saved best model\n","Epoch 11, Loss 87940348560640.00, Val loss 4463486042112.00\n","==================================================================\n","Saved best model\n","Epoch 12, Loss 87025176204544.00, Val loss 4439160651776.00\n","==================================================================\n","Saved best model\n","Epoch 13, Loss 86081913713920.00, Val loss 4413840687104.00\n","==================================================================\n","Saved best model\n","Epoch 14, Loss 85116014637312.00, Val loss 4387647782912.00\n","==================================================================\n","Saved best model\n","Epoch 15, Loss 84162599853568.00, Val loss 4360709079040.00\n","==================================================================\n","Saved best model\n","Epoch 16, Loss 83168717947136.00, Val loss 4333324730368.00\n","==================================================================\n","Saved best model\n","Epoch 17, Loss 82582425318400.00, Val loss 4306347491328.00\n","==================================================================\n","Saved best model\n","Epoch 18, Loss 81412109137920.00, Val loss 4278034104320.00\n","==================================================================\n","Saved best model\n","Epoch 19, Loss 80399647838208.00, Val loss 4249313083392.00\n","==================================================================\n","Saved best model\n","Epoch 20, Loss 79445008101376.00, Val loss 4220382871552.00\n","==================================================================\n","Saved best model\n","Epoch 21, Loss 78508642553856.00, Val loss 4191916916736.00\n","==================================================================\n","Saved best model\n","Epoch 22, Loss 77413982356480.00, Val loss 4163269296128.00\n","==================================================================\n","Saved best model\n","Epoch 23, Loss 76516778088448.00, Val loss 4134703464448.00\n","==================================================================\n","Saved best model\n","Epoch 24, Loss 75744867645440.00, Val loss 4106139467776.00\n","==================================================================\n","Saved best model\n","Epoch 25, Loss 75348338046976.00, Val loss 4097913651200.00\n","==================================================================\n","Saved best model\n","Epoch 26, Loss 74552378331136.00, Val loss 4070390628352.00\n","==================================================================\n","Saved best model\n","Epoch 27, Loss 73219054036992.00, Val loss 4021776285696.00\n","==================================================================\n","Saved best model\n","Epoch 28, Loss 72773549365248.00, Val loss 3994037256192.00\n","==================================================================\n","Saved best model\n","Epoch 29, Loss 72571045117952.00, Val loss 3990301704192.00\n","==================================================================\n","Saved best model\n","Epoch 30, Loss 72669940813824.00, Val loss 3987003670528.00\n","==================================================================\n","Saved best model\n","Epoch 31, Loss 71560124692480.00, Val loss 3960775376896.00\n","==================================================================\n","Saved best model\n","Epoch 32, Loss 71342494410752.00, Val loss 3934028038144.00\n","==================================================================\n","Saved best model\n","Epoch 33, Loss 69276079820800.00, Val loss 3882604036096.00\n","Epoch 34, Loss 70452336570368.00, Val loss 3940065214464.00\n","==================================================================\n","Saved best model\n","Epoch 35, Loss 68689131249664.00, Val loss 3855727722496.00\n","Epoch 36, Loss 68260934320128.00, Val loss 3859307823104.00\n","==================================================================\n","Saved best model\n","Epoch 37, Loss 67022203506688.00, Val loss 3804718170112.00\n","Epoch 38, Loss 67477924085760.00, Val loss 3810333818880.00\n","Epoch 39, Loss 67561665411072.00, Val loss 3850970857472.00\n","Epoch 40, Loss 66541294659584.00, Val loss 3824647143424.00\n","==================================================================\n","Saved best model\n","Epoch 41, Loss 65846796812288.00, Val loss 3799737696256.00\n","Epoch 42, Loss 68251078481920.00, Val loss 3891084132352.00\n","==================================================================\n","Saved best model\n","Epoch 43, Loss 65891941087232.00, Val loss 3746630729728.00\n","Epoch 44, Loss 64772942194688.00, Val loss 3759734784000.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 63634005686272.00, Val loss 3695391277056.00\n","Epoch 46, Loss 63663076048896.00, Val loss 3711822987264.00\n","==================================================================\n","Saved best model\n","Epoch 47, Loss 64132338120704.00, Val loss 3694163394560.00\n","==================================================================\n","Saved best model\n","Epoch 48, Loss 62297317756928.00, Val loss 3669104263168.00\n","Epoch 49, Loss 62563394324480.00, Val loss 3688210890752.00\n","==================================================================\n","Saved best model\n","Epoch 50, Loss 61244998381568.00, Val loss 3630518763520.00\n","Epoch 51, Loss 64376856465408.00, Val loss 3753275817984.00\n","Epoch 52, Loss 62498090782720.00, Val loss 3675832975360.00\n","Epoch 53, Loss 62882506778624.00, Val loss 3705941000192.00\n","Epoch 54, Loss 63384795381760.00, Val loss 3732714029056.00\n","Epoch 55, Loss 62422638544896.00, Val loss 3674575732736.00\n","==================================================================\n","Saved best model\n","Epoch 56, Loss 59976637089792.00, Val loss 3585097334784.00\n","==================================================================\n","Saved best model\n","Epoch 57, Loss 59735655659520.00, Val loss 3561729556480.00\n","==================================================================\n","Saved best model\n","Epoch 58, Loss 59117201528832.00, Val loss 3536613015552.00\n","Epoch 59, Loss 60701401206784.00, Val loss 3579329380352.00\n","==================================================================\n","Saved best model\n","Epoch 60, Loss 58217565704192.00, Val loss 3531313774592.00\n","==================================================================\n","Saved best model\n","Epoch 61, Loss 57269010567168.00, Val loss 3454431920128.00\n","Epoch 62, Loss 59102823518208.00, Val loss 3549968465920.00\n","==================================================================\n","Saved best model\n","Epoch 63, Loss 55871134838784.00, Val loss 3403412930560.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 57261388595200.00, Val loss 3396203184128.00\n","Epoch 65, Loss 56543200256000.00, Val loss 3412676837376.00\n","==================================================================\n","Saved best model\n","Epoch 66, Loss 56620608126976.00, Val loss 3390752423936.00\n","Epoch 67, Loss 56168689637376.00, Val loss 3434450255872.00\n","Epoch 68, Loss 56511139858432.00, Val loss 3442610012160.00\n","Epoch 69, Loss 55120190806016.00, Val loss 3419463745536.00\n","Epoch 70, Loss 55294899034112.00, Val loss 3414558244864.00\n","Epoch 71, Loss 55701998868480.00, Val loss 3407829794816.00\n","==================================================================\n","Saved best model\n","Epoch 72, Loss 54774229725184.00, Val loss 3386923810816.00\n","==================================================================\n","Saved best model\n","Epoch 73, Loss 54186420064256.00, Val loss 3369123184640.00\n","==================================================================\n","Saved best model\n","Epoch 74, Loss 53738341611520.00, Val loss 3345423007744.00\n","==================================================================\n","Saved best model\n","Epoch 75, Loss 53420623241216.00, Val loss 3329648492544.00\n","==================================================================\n","Saved best model\n","Epoch 76, Loss 52982509737984.00, Val loss 3307026776064.00\n","==================================================================\n","Saved best model\n","Epoch 77, Loss 52160547217408.00, Val loss 3218982305792.00\n","Epoch 78, Loss 53009973569536.00, Val loss 3277470564352.00\n","Epoch 79, Loss 52117166919680.00, Val loss 3259586838528.00\n","Epoch 80, Loss 55518097528832.00, Val loss 3391006441472.00\n","Epoch 81, Loss 54667091715072.00, Val loss 3368157446144.00\n","Epoch 82, Loss 53814519494656.00, Val loss 3349417295872.00\n","Epoch 83, Loss 53061989023744.00, Val loss 3321364742144.00\n","Epoch 84, Loss 52654881390592.00, Val loss 3285501870080.00\n","==================================================================\n","Saved best model\n","Epoch 85, Loss 51676711852032.00, Val loss 3208420786176.00\n","==================================================================\n","Saved best model\n","Epoch 86, Loss 51632490897408.00, Val loss 3192997019648.00\n","Epoch 87, Loss 52465876135936.00, Val loss 3260078096384.00\n","==================================================================\n","Saved best model\n","Epoch 88, Loss 49798866923520.00, Val loss 3081353297920.00\n","Epoch 89, Loss 54003886188544.00, Val loss 3237551800320.00\n","Epoch 90, Loss 53467406955520.00, Val loss 3283893092352.00\n","Epoch 91, Loss 49889658772480.00, Val loss 3120483270656.00\n","Epoch 92, Loss 53700079060992.00, Val loss 3292003303424.00\n","Epoch 93, Loss 54335152435200.00, Val loss 3377862017024.00\n","Epoch 94, Loss 54161072608256.00, Val loss 3312318414848.00\n","Epoch 95, Loss 55163066615808.00, Val loss 3413556330496.00\n","Epoch 96, Loss 55834257255424.00, Val loss 3467539120128.00\n","Epoch 97, Loss 54911707256832.00, Val loss 3443589906432.00\n","Epoch 98, Loss 54175202557952.00, Val loss 3361253097472.00\n","Epoch 99, Loss 50544169318400.00, Val loss 3240964390912.00\n","Epoch 100, Loss 50048936448000.00, Val loss 3227164868608.00\n","Epoch 101, Loss 54835279597568.00, Val loss 3407803580416.00\n","Epoch 102, Loss 52924310278144.00, Val loss 3381251538944.00\n","Epoch 103, Loss 54510560029696.00, Val loss 3422168809472.00\n","Epoch 104, Loss 53773323928576.00, Val loss 3395734994944.00\n","Epoch 105, Loss 53241090410496.00, Val loss 3369878421504.00\n","Epoch 106, Loss 52687043604480.00, Val loss 3347626328064.00\n","Epoch 107, Loss 53390627716608.00, Val loss 3272422195200.00\n","Epoch 108, Loss 51877441044480.00, Val loss 3250109546496.00\n","Epoch 109, Loss 52515759896576.00, Val loss 3322945208320.00\n","Epoch 110, Loss 54330477310976.00, Val loss 3402645897216.00\n","Epoch 111, Loss 53294198735872.00, Val loss 3382625435648.00\n","Epoch 112, Loss 52557682817024.00, Val loss 3344393306112.00\n","Epoch 113, Loss 52288786676736.00, Val loss 3323466088448.00\n","Epoch 114, Loss 50395785873408.00, Val loss 3210675224576.00\n","Epoch 115, Loss 53218456375296.00, Val loss 3342286192640.00\n","Epoch 116, Loss 51330613312512.00, Val loss 3314013700096.00\n","Epoch 117, Loss 50735335158784.00, Val loss 3288348753920.00\n","Epoch 118, Loss 50144067519488.00, Val loss 3261594337280.00\n","Epoch 119, Loss 49591470582784.00, Val loss 3228113567744.00\n","==================================================================\n","Saved best model\n","Epoch 120, Loss 46863194310656.00, Val loss 3072069206016.00\n","Epoch 121, Loss 50386780403712.00, Val loss 3196908208128.00\n","==================================================================\n","Saved best model\n","Epoch 122, Loss 48086538499072.00, Val loss 3038153277440.00\n","Epoch 123, Loss 50540286568448.00, Val loss 3169876443136.00\n","Epoch 124, Loss 52047686860800.00, Val loss 3236421959680.00\n","Epoch 125, Loss 49437639546880.00, Val loss 3212277972992.00\n","Epoch 126, Loss 51786323615744.00, Val loss 3313432264704.00\n","Epoch 127, Loss 50641228839936.00, Val loss 3285103149056.00\n","Epoch 128, Loss 49941139219456.00, Val loss 3264253001728.00\n","Epoch 129, Loss 49432258965504.00, Val loss 3248620044288.00\n","Epoch 130, Loss 52568106133504.00, Val loss 3343515123712.00\n","Epoch 131, Loss 49865229279744.00, Val loss 3198894473216.00\n","Epoch 132, Loss 53484285804544.00, Val loss 3360158384128.00\n","Epoch 133, Loss 51247876134400.00, Val loss 3330129788928.00\n","Epoch 134, Loss 50569714634752.00, Val loss 3301598560256.00\n","Epoch 135, Loss 49968717954048.00, Val loss 3273859530752.00\n","Epoch 136, Loss 49491616383488.00, Val loss 3247560720384.00\n","Epoch 137, Loss 49086168204288.00, Val loss 3127307927552.00\n","Epoch 138, Loss 49994631001088.00, Val loss 3197487546368.00\n","Epoch 139, Loss 48211332100096.00, Val loss 3172718870528.00\n","Epoch 140, Loss 48412444762112.00, Val loss 3172668538880.00\n","Epoch 141, Loss 58264602440704.00, Val loss 3515078410240.00\n","Epoch 142, Loss 55485914475008.00, Val loss 3478980395008.00\n","Epoch 143, Loss 54549865536768.00, Val loss 3445114535936.00\n","Epoch 144, Loss 53686869553408.00, Val loss 3412882096128.00\n","Epoch 145, Loss 51033338374912.00, Val loss 3234125578240.00\n","Epoch 146, Loss 54334812454912.00, Val loss 3381647900672.00\n","Epoch 147, Loss 52156797782784.00, Val loss 3350490513408.00\n","Epoch 148, Loss 51437173046016.00, Val loss 3320994332672.00\n","Epoch 149, Loss 50766762818048.00, Val loss 3292367683584.00\n","Epoch 150, Loss 49366489628160.00, Val loss 3211032788992.00\n","Epoch 151, Loss 52786932301824.00, Val loss 3333017567232.00\n","Epoch 152, Loss 51057108495872.00, Val loss 3311380463616.00\n","Epoch 153, Loss 54233954718464.00, Val loss 3408005955584.00\n","Epoch 154, Loss 53450627956736.00, Val loss 3409921179648.00\n","Epoch 155, Loss 52626475441664.00, Val loss 3377810374656.00\n","Epoch 156, Loss 49750714135552.00, Val loss 3145566519296.00\n","Epoch 157, Loss 61080007895040.00, Val loss 3322606780416.00\n","Epoch 158, Loss 49771809206784.00, Val loss 3142688178176.00\n","Epoch 159, Loss 51290089296896.00, Val loss 3111419904000.00\n","Epoch 160, Loss 53122475301888.00, Val loss 3321496076288.00\n","Epoch 161, Loss 47660577862656.00, Val loss 3038824890368.00\n","Epoch 162, Loss 50730660929536.00, Val loss 3160383946752.00\n","Epoch 163, Loss 48020189546496.00, Val loss 3140293230592.00\n","Epoch 164, Loss 49543147294720.00, Val loss 3116274024448.00\n","Epoch 165, Loss 49891956785152.00, Val loss 3244066340864.00\n","Epoch 166, Loss 48871311840256.00, Val loss 3216180248576.00\n","==================================================================\n","Saved best model\n","Epoch 167, Loss 46789869370368.00, Val loss 3031534403584.00\n","==================================================================\n","Saved best model\n","Epoch 168, Loss 46184471240704.00, Val loss 3016791949312.00\n","==================================================================\n","Saved best model\n","Epoch 169, Loss 45177695738880.00, Val loss 2890330537984.00\n","==================================================================\n","Saved best model\n","Epoch 170, Loss 43396321753088.00, Val loss 2879112347648.00\n","==================================================================\n","Saved best model\n","Epoch 171, Loss 43129526260736.00, Val loss 2867084656640.00\n","==================================================================\n","Saved best model\n","Epoch 172, Loss 43153187426304.00, Val loss 2855295778816.00\n","==================================================================\n","Saved best model\n","Epoch 173, Loss 44207210660864.00, Val loss 2849104986112.00\n","Epoch 174, Loss 44969288645632.00, Val loss 2857636462592.00\n","==================================================================\n","Saved best model\n","Epoch 175, Loss 42564663244800.00, Val loss 2808814764032.00\n","Epoch 176, Loss 43298216963072.00, Val loss 2815226281984.00\n","==================================================================\n","Saved best model\n","Epoch 177, Loss 42069805731840.00, Val loss 2790188122112.00\n","==================================================================\n","Saved best model\n","Epoch 178, Loss 41960797699072.00, Val loss 2770592071680.00\n","==================================================================\n","Saved best model\n","Epoch 179, Loss 41598422398976.00, Val loss 2726518849536.00\n","Epoch 180, Loss 42216454025216.00, Val loss 2728154103808.00\n","Epoch 181, Loss 43335386599424.00, Val loss 2775345004544.00\n","Epoch 182, Loss 42739223043072.00, Val loss 2862005092352.00\n","==================================================================\n","Saved best model\n","Epoch 183, Loss 40542609160192.00, Val loss 2704560095232.00\n","Epoch 184, Loss 41707692499968.00, Val loss 2733862289408.00\n","Epoch 185, Loss 42357997043712.00, Val loss 2850196553728.00\n","==================================================================\n","Saved best model\n","Epoch 186, Loss 39823519250432.00, Val loss 2695398424576.00\n","Epoch 187, Loss 41572861124608.00, Val loss 2730796515328.00\n","==================================================================\n","Saved best model\n","Epoch 188, Loss 40721475780608.00, Val loss 2693537988608.00\n","==================================================================\n","Saved best model\n","Epoch 189, Loss 41069616687104.00, Val loss 2687255969792.00\n","Epoch 190, Loss 43653083215872.00, Val loss 2795869306880.00\n","Epoch 191, Loss 42748896655360.00, Val loss 2793179709440.00\n","Epoch 192, Loss 42104326124544.00, Val loss 2762263232512.00\n","Epoch 193, Loss 44486163716096.00, Val loss 2924145541120.00\n","Epoch 194, Loss 42727118797312.00, Val loss 2900422295552.00\n","Epoch 195, Loss 46619549718528.00, Val loss 3025346494464.00\n","Epoch 196, Loss 45469319507456.00, Val loss 3001895878656.00\n","Epoch 197, Loss 45003093928448.00, Val loss 2977407959040.00\n","Epoch 198, Loss 46635852007424.00, Val loss 3025950998528.00\n","Epoch 199, Loss 46316758043648.00, Val loss 3024728883200.00\n","Epoch 200, Loss 45793004450304.00, Val loss 3008815169536.00\n","Epoch 201, Loss 45350498223104.00, Val loss 2989159350272.00\n","Epoch 202, Loss 45106693515264.00, Val loss 2969475219456.00\n","Epoch 203, Loss 44789593821696.00, Val loss 2912057819136.00\n","Epoch 204, Loss 45782869240832.00, Val loss 2913207582720.00\n","Epoch 205, Loss 50086716591616.00, Val loss 3084005146624.00\n","Epoch 206, Loss 48042512685568.00, Val loss 3064224808960.00\n","Epoch 207, Loss 46772269316096.00, Val loss 3046765756416.00\n","Epoch 208, Loss 48152168614912.00, Val loss 2968030543872.00\n","Epoch 209, Loss 47473048043520.00, Val loss 3002171129856.00\n","Epoch 210, Loss 47588061277184.00, Val loss 3001501351936.00\n","Epoch 211, Loss 52427170576384.00, Val loss 3018059677696.00\n","Epoch 212, Loss 46338397681664.00, Val loss 3028168998912.00\n","Epoch 213, Loss 45824400695296.00, Val loss 3025130225664.00\n","Epoch 214, Loss 45694744168448.00, Val loss 3017074540544.00\n","Epoch 215, Loss 46396067689472.00, Val loss 2997450178560.00\n","Epoch 216, Loss 44856969220096.00, Val loss 2983521157120.00\n","Epoch 217, Loss 44549025165312.00, Val loss 2960874274816.00\n","Epoch 218, Loss 44251102294016.00, Val loss 2937415270400.00\n","Epoch 219, Loss 44157500481536.00, Val loss 2916089331712.00\n","Epoch 220, Loss 42695108247552.00, Val loss 2772062699520.00\n","Epoch 221, Loss 43692832745472.00, Val loss 2877091741696.00\n","Epoch 222, Loss 43214984964096.00, Val loss 2926925316096.00\n","Epoch 223, Loss 45418828001280.00, Val loss 3019254267904.00\n","Epoch 224, Loss 44479621767168.00, Val loss 2995315015680.00\n","Epoch 225, Loss 44023138315264.00, Val loss 2971895070720.00\n","Epoch 226, Loss 43951788689408.00, Val loss 2941552164864.00\n","Epoch 227, Loss 43169954366464.00, Val loss 2918753501184.00\n","Epoch 228, Loss 43035449729024.00, Val loss 2904043814912.00\n","Epoch 229, Loss 42749636065280.00, Val loss 2882888269824.00\n","Epoch 230, Loss 42905470442496.00, Val loss 2864675028992.00\n","Epoch 231, Loss 42609002696704.00, Val loss 2852772118528.00\n","Epoch 232, Loss 42566936311808.00, Val loss 2859111022592.00\n","Epoch 233, Loss 42237738464256.00, Val loss 2842627145728.00\n","Epoch 234, Loss 42014084511744.00, Val loss 2820015390720.00\n","Epoch 235, Loss 41971545975808.00, Val loss 2811779612672.00\n","Epoch 236, Loss 42125329638400.00, Val loss 2803453394944.00\n","Epoch 237, Loss 47124158568448.00, Val loss 2779389100032.00\n","Epoch 238, Loss 41354155185152.00, Val loss 2761227501568.00\n","Epoch 239, Loss 41651715872768.00, Val loss 2751675498496.00\n","Epoch 240, Loss 50064970204160.00, Val loss 3062508552192.00\n","Epoch 241, Loss 44160593701888.00, Val loss 2849919467520.00\n","Epoch 242, Loss 41062348869632.00, Val loss 2828619218944.00\n","Epoch 243, Loss 40830681102336.00, Val loss 2805804826624.00\n","Epoch 244, Loss 40972448055296.00, Val loss 2812626337792.00\n","Epoch 245, Loss 41399747832832.00, Val loss 2831532425216.00\n","Epoch 246, Loss 39799442489344.00, Val loss 2755969941504.00\n","Epoch 247, Loss 39830950852608.00, Val loss 2735149678592.00\n","Epoch 248, Loss 40102199027712.00, Val loss 2720999669760.00\n","Epoch 249, Loss 39576598231040.00, Val loss 2710391488512.00\n","Epoch 250, Loss 39555709353984.00, Val loss 2691696951296.00\n","==================================================================\n","Saved best model\n","Epoch 251, Loss 39468456484864.00, Val loss 2683646771200.00\n","==================================================================\n","Saved best model\n","Epoch 252, Loss 39374879171584.00, Val loss 2665396043776.00\n","==================================================================\n","Saved best model\n","Epoch 253, Loss 39342327760896.00, Val loss 2657990737920.00\n","==================================================================\n","Saved best model\n","Epoch 254, Loss 39162364816384.00, Val loss 2641203036160.00\n","==================================================================\n","Saved best model\n","Epoch 255, Loss 39151264778240.00, Val loss 2634455449600.00\n","==================================================================\n","Saved best model\n","Epoch 256, Loss 38925172785152.00, Val loss 2618494287872.00\n","==================================================================\n","Saved best model\n","Epoch 257, Loss 38998449582080.00, Val loss 2613132656640.00\n","==================================================================\n","Saved best model\n","Epoch 258, Loss 38724693652480.00, Val loss 2598228721664.00\n","==================================================================\n","Saved best model\n","Epoch 259, Loss 38833446719488.00, Val loss 2593060814848.00\n","==================================================================\n","Saved best model\n","Epoch 260, Loss 38550689634304.00, Val loss 2578955894784.00\n","==================================================================\n","Saved best model\n","Epoch 261, Loss 38667587115008.00, Val loss 2574895284224.00\n","==================================================================\n","Saved best model\n","Epoch 262, Loss 38339929233408.00, Val loss 2561728577536.00\n","Epoch 263, Loss 38789103427584.00, Val loss 2594066661376.00\n","==================================================================\n","Saved best model\n","Epoch 264, Loss 38129091222528.00, Val loss 2540508545024.00\n","Epoch 265, Loss 48642452768768.00, Val loss 2880636715008.00\n","Epoch 266, Loss 51727851726848.00, Val loss 3143877787648.00\n","Epoch 267, Loss 45917305014272.00, Val loss 3108259233792.00\n","Epoch 268, Loss 45019506400256.00, Val loss 3071418302464.00\n","Epoch 269, Loss 44295627045888.00, Val loss 3038370594816.00\n","Epoch 270, Loss 48952798432256.00, Val loss 3163596259328.00\n","Epoch 271, Loss 46110418715648.00, Val loss 3130053099520.00\n","Epoch 272, Loss 57950372218880.00, Val loss 3483047559168.00\n","Epoch 273, Loss 56877124358144.00, Val loss 3471723986944.00\n","Epoch 274, Loss 56568793761792.00, Val loss 3460316528640.00\n","Epoch 275, Loss 56294225470464.00, Val loss 3448646664192.00\n","Epoch 276, Loss 56035823942656.00, Val loss 3436997246976.00\n","Epoch 277, Loss 55781004274688.00, Val loss 3425035878400.00\n","Epoch 278, Loss 55185356237824.00, Val loss 3378525765632.00\n","Epoch 279, Loss 55171384114176.00, Val loss 3368000159744.00\n","Epoch 280, Loss 52431037919232.00, Val loss 3176675409920.00\n","Epoch 281, Loss 41946440771584.00, Val loss 2866623545344.00\n","Epoch 282, Loss 42169199429632.00, Val loss 2832382558208.00\n","Epoch 283, Loss 41172478470144.00, Val loss 2809041780736.00\n","Epoch 284, Loss 41051221286912.00, Val loss 2785419984896.00\n","Epoch 285, Loss 41037397897216.00, Val loss 2765345259520.00\n","Epoch 286, Loss 40941693153280.00, Val loss 2755789848576.00\n","Epoch 287, Loss 40926159599616.00, Val loss 2734855290880.00\n","Epoch 288, Loss 40609213108224.00, Val loss 2726611386368.00\n","Epoch 289, Loss 40537677107200.00, Val loss 2743528062976.00\n","Epoch 290, Loss 40071122800640.00, Val loss 2688752025600.00\n","Epoch 291, Loss 40452117399552.00, Val loss 2695440629760.00\n","Epoch 292, Loss 41463570126848.00, Val loss 2785551056896.00\n","Epoch 293, Loss 40888873773056.00, Val loss 2727168966656.00\n","Epoch 294, Loss 41696456443904.00, Val loss 2733713915904.00\n","Epoch 295, Loss 39570160601088.00, Val loss 2674773721088.00\n","Epoch 296, Loss 40324944592896.00, Val loss 2623453790208.00\n","Epoch 297, Loss 40071304679424.00, Val loss 2622203101184.00\n","Epoch 298, Loss 39499420033024.00, Val loss 2643160203264.00\n","Epoch 299, Loss 39131933552640.00, Val loss 2595078012928.00\n","Epoch 300, Loss 40024137113600.00, Val loss 2615430610944.00\n","Epoch 301, Loss 39698010451968.00, Val loss 2597381996544.00\n","Epoch 302, Loss 38937062701056.00, Val loss 2593658765312.00\n","Epoch 303, Loss 39332936751104.00, Val loss 2584984682496.00\n","Epoch 304, Loss 39315109261312.00, Val loss 2576973037568.00\n","Epoch 305, Loss 44715765606400.00, Val loss 2721468121088.00\n","Epoch 306, Loss 38552285952000.00, Val loss 2697898754048.00\n","Epoch 307, Loss 37915651295232.00, Val loss 2679885791232.00\n","Epoch 308, Loss 38122896287744.00, Val loss 2658745188352.00\n","Epoch 309, Loss 37655102431232.00, Val loss 2629315592192.00\n","Epoch 310, Loss 37869834346496.00, Val loss 2563737124864.00\n","Epoch 311, Loss 42873962338304.00, Val loss 2554241744896.00\n","Epoch 312, Loss 38520562427904.00, Val loss 2613807415296.00\n","Epoch 313, Loss 37866932080640.00, Val loss 2598860488704.00\n","Epoch 314, Loss 37847347156992.00, Val loss 2591597264896.00\n","Epoch 315, Loss 37566022313984.00, Val loss 2577898143744.00\n","Epoch 316, Loss 37605043466240.00, Val loss 2571701583872.00\n","Epoch 317, Loss 37324055527424.00, Val loss 2558669881344.00\n","Epoch 318, Loss 37442350426112.00, Val loss 2552968773632.00\n","Epoch 319, Loss 37093035560960.00, Val loss 2540651151360.00\n","==================================================================\n","Saved best model\n","Epoch 320, Loss 37414829443072.00, Val loss 2535381008384.00\n","==================================================================\n","Saved best model\n","Epoch 321, Loss 36910914646016.00, Val loss 2523478622208.00\n","==================================================================\n","Saved best model\n","Epoch 322, Loss 37073935732736.00, Val loss 2519587356672.00\n","==================================================================\n","Saved best model\n","Epoch 323, Loss 36647732119552.00, Val loss 2507881054208.00\n","==================================================================\n","Saved best model\n","Epoch 324, Loss 36897425664000.00, Val loss 2504437530624.00\n","==================================================================\n","Saved best model\n","Epoch 325, Loss 36535838621696.00, Val loss 2493372432384.00\n","==================================================================\n","Saved best model\n","Epoch 326, Loss 36764876808192.00, Val loss 2490304036864.00\n","==================================================================\n","Saved best model\n","Epoch 327, Loss 36383881091072.00, Val loss 2479629795328.00\n","==================================================================\n","Saved best model\n","Epoch 328, Loss 36691384905728.00, Val loss 2476524961792.00\n","==================================================================\n","Saved best model\n","Epoch 329, Loss 36208799961088.00, Val loss 2465867497472.00\n","==================================================================\n","Saved best model\n","Epoch 330, Loss 36769172848640.00, Val loss 2461198450688.00\n","==================================================================\n","Saved best model\n","Epoch 331, Loss 36779903143936.00, Val loss 2452409024512.00\n","==================================================================\n","Saved best model\n","Epoch 332, Loss 36636212639744.00, Val loss 2449021599744.00\n","==================================================================\n","Saved best model\n","Epoch 333, Loss 36660780281856.00, Val loss 2441521922048.00\n","==================================================================\n","Saved best model\n","Epoch 334, Loss 36273038088192.00, Val loss 2436069851136.00\n","==================================================================\n","Saved best model\n","Epoch 335, Loss 36363661688832.00, Val loss 2427309260800.00\n","==================================================================\n","Saved best model\n","Epoch 336, Loss 36194562738176.00, Val loss 2422907076608.00\n","==================================================================\n","Saved best model\n","Epoch 337, Loss 36169586860032.00, Val loss 2415263219712.00\n","==================================================================\n","Saved best model\n","Epoch 338, Loss 36095095967744.00, Val loss 2411610243072.00\n","==================================================================\n","Saved best model\n","Epoch 339, Loss 36465648713728.00, Val loss 2403204857856.00\n","==================================================================\n","Saved best model\n","Epoch 340, Loss 35915848071168.00, Val loss 2399672467456.00\n","==================================================================\n","Saved best model\n","Epoch 341, Loss 35899671691264.00, Val loss 2393229492224.00\n","==================================================================\n","Saved best model\n","Epoch 342, Loss 35854843019264.00, Val loss 2389928050688.00\n","==================================================================\n","Saved best model\n","Epoch 343, Loss 35852608909312.00, Val loss 2383445229568.00\n","Epoch 344, Loss 38984109029376.00, Val loss 2520588484608.00\n","Epoch 345, Loss 37185156708352.00, Val loss 2386016337920.00\n","==================================================================\n","Saved best model\n","Epoch 346, Loss 35201210193920.00, Val loss 2360987353088.00\n","==================================================================\n","Saved best model\n","Epoch 347, Loss 35735717754880.00, Val loss 2356365230080.00\n","==================================================================\n","Saved best model\n","Epoch 348, Loss 35510851534848.00, Val loss 2353969758208.00\n","==================================================================\n","Saved best model\n","Epoch 349, Loss 35634022660096.00, Val loss 2349512523776.00\n","==================================================================\n","Saved best model\n","Epoch 350, Loss 35398019035136.00, Val loss 2343566835712.00\n","==================================================================\n","Saved best model\n","Epoch 351, Loss 35591794755584.00, Val loss 2341808111616.00\n","==================================================================\n","Saved best model\n","Epoch 352, Loss 35304195362816.00, Val loss 2335418613760.00\n","==================================================================\n","Saved best model\n","Epoch 353, Loss 35500154705920.00, Val loss 2332952625152.00\n","==================================================================\n","Saved best model\n","Epoch 354, Loss 35215673413632.00, Val loss 2326609526784.00\n","Epoch 355, Loss 35624673787904.00, Val loss 2338136522752.00\n","Epoch 356, Loss 35290552410112.00, Val loss 2332748677120.00\n","Epoch 357, Loss 35550991020032.00, Val loss 2330769227776.00\n","==================================================================\n","Saved best model\n","Epoch 358, Loss 35266361278464.00, Val loss 2325340487680.00\n","==================================================================\n","Saved best model\n","Epoch 359, Loss 35503528949760.00, Val loss 2324881735680.00\n","Epoch 360, Loss 50983157571584.00, Val loss 2939669970944.00\n","Epoch 361, Loss 48779294868480.00, Val loss 2939640086528.00\n","Epoch 362, Loss 48544165103616.00, Val loss 2930368315392.00\n","Epoch 363, Loss 48739911597056.00, Val loss 2930904662016.00\n","Epoch 364, Loss 48486234351616.00, Val loss 2923444568064.00\n","Epoch 365, Loss 48693105271808.00, Val loss 2926360920064.00\n","Epoch 366, Loss 48715609999360.00, Val loss 2917569134592.00\n","Epoch 367, Loss 49936260203520.00, Val loss 3009626505216.00\n","Epoch 368, Loss 49371630332928.00, Val loss 3019321376768.00\n","Epoch 369, Loss 49376326518784.00, Val loss 3021119684608.00\n","Epoch 370, Loss 49172423429120.00, Val loss 3015132315648.00\n","Epoch 371, Loss 49215036788736.00, Val loss 3012189749248.00\n","Epoch 372, Loss 49047720275968.00, Val loss 3005448454144.00\n","Epoch 373, Loss 49091670889472.00, Val loss 3000142135296.00\n","Epoch 374, Loss 48927202518016.00, Val loss 2994768445440.00\n","Epoch 375, Loss 48993399346176.00, Val loss 2992258416640.00\n","Epoch 376, Loss 48729539318784.00, Val loss 2963655098368.00\n","==================================================================\n","Saved best model\n","Epoch 377, Loss 35686418276352.00, Val loss 2278710050816.00\n","==================================================================\n","Saved best model\n","Epoch 378, Loss 36627081922560.00, Val loss 2263346315264.00\n","Epoch 379, Loss 39003807819776.00, Val loss 2566877085696.00\n","Epoch 380, Loss 36515026153472.00, Val loss 2526089052160.00\n","Epoch 381, Loss 36548227764224.00, Val loss 2501706514432.00\n","Epoch 382, Loss 36136523307008.00, Val loss 2467243229184.00\n","Epoch 383, Loss 36102065715200.00, Val loss 2472301821952.00\n","Epoch 384, Loss 35675602264064.00, Val loss 2445320912896.00\n","Epoch 385, Loss 35404809930752.00, Val loss 2436768202752.00\n","Epoch 386, Loss 35166879424512.00, Val loss 2320991256576.00\n","Epoch 387, Loss 40298632484864.00, Val loss 2560331350016.00\n","Epoch 388, Loss 38209695240192.00, Val loss 2568085831680.00\n","Epoch 389, Loss 37890777178112.00, Val loss 2533162483712.00\n","Epoch 390, Loss 38063629273088.00, Val loss 2524449865728.00\n","Epoch 391, Loss 37591845806080.00, Val loss 2478799060992.00\n","Epoch 392, Loss 39163386759168.00, Val loss 2577485266944.00\n","Epoch 393, Loss 38579949645824.00, Val loss 2596523737088.00\n","Epoch 394, Loss 37494183120896.00, Val loss 2549707177984.00\n","Epoch 395, Loss 37488150634496.00, Val loss 2533877088256.00\n","Epoch 396, Loss 38655969355776.00, Val loss 2614183067648.00\n","Epoch 397, Loss 36992291956736.00, Val loss 2494484709376.00\n","Epoch 398, Loss 36781466697728.00, Val loss 2484431486976.00\n","Epoch 399, Loss 37120897265664.00, Val loss 2477678133248.00\n","Epoch 400, Loss 36773625460736.00, Val loss 2440329428992.00\n","Epoch 401, Loss 36126949509120.00, Val loss 2427588706304.00\n","Epoch 402, Loss 36404644796416.00, Val loss 2447490154496.00\n","Epoch 403, Loss 36015001589760.00, Val loss 2407184728064.00\n","Epoch 404, Loss 35856869042176.00, Val loss 2398033281024.00\n","Epoch 405, Loss 36211379511296.00, Val loss 2420105805824.00\n","Epoch 406, Loss 35360437897216.00, Val loss 2321400987648.00\n","Epoch 407, Loss 36737488295936.00, Val loss 2376145567744.00\n","Epoch 408, Loss 37097092751360.00, Val loss 2396824797184.00\n","Epoch 409, Loss 36347623376896.00, Val loss 2378447192064.00\n","Epoch 410, Loss 36003206932480.00, Val loss 2361435357184.00\n","Epoch 411, Loss 36780349675520.00, Val loss 2411311923200.00\n","Epoch 412, Loss 39036245843968.00, Val loss 2493022732288.00\n","Epoch 413, Loss 37900164489216.00, Val loss 2539612012544.00\n","Epoch 414, Loss 37646674188288.00, Val loss 2530415738880.00\n","Epoch 415, Loss 37173212170240.00, Val loss 2533464997888.00\n","Epoch 416, Loss 37563709900800.00, Val loss 2533395529728.00\n","Epoch 417, Loss 37466032936960.00, Val loss 2518130360320.00\n","Epoch 418, Loss 36926576203776.00, Val loss 2504359673856.00\n","Epoch 419, Loss 37327720415232.00, Val loss 2502568968192.00\n","Epoch 420, Loss 37060960339968.00, Val loss 2506614636544.00\n","Epoch 421, Loss 37366020362240.00, Val loss 2505057239040.00\n","Epoch 422, Loss 36883813386240.00, Val loss 2495765020672.00\n","Epoch 423, Loss 37232644216832.00, Val loss 2494603984896.00\n","Epoch 424, Loss 36772043964416.00, Val loss 2484691009536.00\n","Epoch 425, Loss 37708321155072.00, Val loss 2482538020864.00\n","Epoch 426, Loss 37367187920896.00, Val loss 2473399418880.00\n","Epoch 427, Loss 37421159936000.00, Val loss 2474842259456.00\n","Epoch 428, Loss 37143571519488.00, Val loss 2447864233984.00\n","Epoch 429, Loss 37167060856832.00, Val loss 2412539543552.00\n","Epoch 430, Loss 37517186902016.00, Val loss 2434772500480.00\n","Epoch 431, Loss 37679478433792.00, Val loss 2471815282688.00\n","Epoch 432, Loss 35400919928832.00, Val loss 2352368058368.00\n","Epoch 433, Loss 36410567282688.00, Val loss 2393366331392.00\n","Epoch 434, Loss 51074980966400.00, Val loss 2941842096128.00\n","Epoch 435, Loss 48147043594240.00, Val loss 2910217306112.00\n","Epoch 436, Loss 44111944636416.00, Val loss 2649224380416.00\n","Epoch 437, Loss 38249839785984.00, Val loss 2585325993984.00\n","Epoch 438, Loss 37523357163520.00, Val loss 2580284440576.00\n","Epoch 439, Loss 36911786930176.00, Val loss 2543222784000.00\n","Epoch 440, Loss 36561571127296.00, Val loss 2520665817088.00\n","Epoch 441, Loss 36662236536832.00, Val loss 2500569071616.00\n","Epoch 442, Loss 36188379043840.00, Val loss 2480807346176.00\n","Epoch 443, Loss 36500496271360.00, Val loss 2466468331520.00\n","Epoch 444, Loss 36298881697792.00, Val loss 2449958502400.00\n","Epoch 445, Loss 37657091604480.00, Val loss 2444013862912.00\n","Epoch 446, Loss 36274482657280.00, Val loss 2423746461696.00\n","Epoch 447, Loss 36780617240576.00, Val loss 2422874308608.00\n","Epoch 448, Loss 36108292587520.00, Val loss 2404588453888.00\n","==================================================================\n","Saved best model\n","Epoch 449, Loss 34406473373696.00, Val loss 2174283546624.00\n","Epoch 450, Loss 36630719340544.00, Val loss 2203713011712.00\n","Epoch 451, Loss 39785238478848.00, Val loss 2427926085632.00\n","Epoch 452, Loss 36521836421120.00, Val loss 2431118999552.00\n","Epoch 453, Loss 36106218594304.00, Val loss 2390565847040.00\n","Epoch 454, Loss 35955839840256.00, Val loss 2410375806976.00\n","Epoch 455, Loss 35724003934208.00, Val loss 2354989236224.00\n","Epoch 456, Loss 35856512094208.00, Val loss 2395686567936.00\n","Epoch 457, Loss 35639931322368.00, Val loss 2356670627840.00\n","Epoch 458, Loss 36048083660800.00, Val loss 2374855557120.00\n","Epoch 459, Loss 35574696247296.00, Val loss 2324157169664.00\n","Epoch 460, Loss 35608096411648.00, Val loss 2371065479168.00\n","Epoch 461, Loss 35401862545408.00, Val loss 2326684499968.00\n","Epoch 462, Loss 35318345310208.00, Val loss 2356900003840.00\n","Epoch 463, Loss 35262485327872.00, Val loss 2289769644032.00\n","Epoch 464, Loss 35616493400064.00, Val loss 2349126385664.00\n","Epoch 465, Loss 35532159066112.00, Val loss 2306015756288.00\n","Epoch 466, Loss 35371125915648.00, Val loss 2350932295680.00\n","Epoch 467, Loss 35066130464768.00, Val loss 2236305375232.00\n","Epoch 468, Loss 36587007430656.00, Val loss 2303816105984.00\n","Epoch 469, Loss 35612085366784.00, Val loss 2231228170240.00\n","==================================================================\n","Saved best model\n","Epoch 470, Loss 33339815530496.00, Val loss 2085324455936.00\n","Epoch 471, Loss 35468312166400.00, Val loss 2257599332352.00\n","Epoch 472, Loss 35278507225088.00, Val loss 2329747128320.00\n","Epoch 473, Loss 35118800150528.00, Val loss 2250731421696.00\n","Epoch 474, Loss 35235048394752.00, Val loss 2318312669184.00\n","Epoch 475, Loss 35172146806784.00, Val loss 2255955427328.00\n","Epoch 476, Loss 35163327266816.00, Val loss 2308408868864.00\n","Epoch 477, Loss 35196641058816.00, Val loss 2239220154368.00\n","Epoch 478, Loss 35127058890752.00, Val loss 2300957949952.00\n","Epoch 479, Loss 35454662545408.00, Val loss 2276808720384.00\n","Epoch 480, Loss 35550910631936.00, Val loss 2262647963648.00\n","Epoch 481, Loss 34884948504576.00, Val loss 2273854881792.00\n","Epoch 482, Loss 35310255652864.00, Val loss 2280602206208.00\n","Epoch 483, Loss 34498388287488.00, Val loss 2237785440256.00\n","Epoch 484, Loss 35860452073472.00, Val loss 2281092677632.00\n","Epoch 485, Loss 35690778664960.00, Val loss 2300569714688.00\n","Epoch 486, Loss 34942330318848.00, Val loss 2236078620672.00\n","Epoch 487, Loss 34883938910208.00, Val loss 2254740389888.00\n","Epoch 488, Loss 35848614612992.00, Val loss 2305507196928.00\n","Epoch 489, Loss 34756217077760.00, Val loss 2205510795264.00\n","Epoch 490, Loss 35988102520832.00, Val loss 2277154226176.00\n","Epoch 491, Loss 34668556668928.00, Val loss 2206871322624.00\n","Epoch 492, Loss 34843105722368.00, Val loss 2190398849024.00\n","Epoch 493, Loss 35238089469952.00, Val loss 2184349089792.00\n","Epoch 494, Loss 36172578238464.00, Val loss 2247396163584.00\n","Epoch 495, Loss 35277595340800.00, Val loss 2177705050112.00\n","Epoch 496, Loss 35617626218496.00, Val loss 2230500720640.00\n","Epoch 497, Loss 36283849842688.00, Val loss 2251322294272.00\n","Epoch 498, Loss 35566350401536.00, Val loss 2324869677056.00\n","Epoch 499, Loss 34705902268416.00, Val loss 2196874592256.00\n","Epoch 500, Loss 36944223227904.00, Val loss 2356313587712.00\n","Epoch 501, Loss 35971333885952.00, Val loss 2265456836608.00\n","Epoch 502, Loss 36619077902336.00, Val loss 2358996893696.00\n","Epoch 503, Loss 42744789315584.00, Val loss 2584469831680.00\n","Epoch 504, Loss 38374290100224.00, Val loss 2456555618304.00\n","Epoch 505, Loss 37461332856832.00, Val loss 2346995941376.00\n","Epoch 506, Loss 38147339153408.00, Val loss 2358624911360.00\n","Epoch 507, Loss 37993425559552.00, Val loss 2336930922496.00\n","Epoch 508, Loss 37150464487424.00, Val loss 2354366906368.00\n","Epoch 509, Loss 37133424013312.00, Val loss 2280733016064.00\n","Epoch 510, Loss 36048372830208.00, Val loss 2305924005888.00\n","Epoch 511, Loss 38577992462336.00, Val loss 2376034680832.00\n","Epoch 512, Loss 35921386848256.00, Val loss 2371878387712.00\n","Epoch 513, Loss 36152709124096.00, Val loss 2373201166336.00\n","Epoch 514, Loss 35896124751872.00, Val loss 2388107722752.00\n","Epoch 515, Loss 36910616625152.00, Val loss 2395214708736.00\n","Epoch 516, Loss 35857893453824.00, Val loss 2350438678528.00\n","Epoch 517, Loss 35945439199232.00, Val loss 2342675546112.00\n","Epoch 518, Loss 36300158136320.00, Val loss 2368234323968.00\n","Epoch 519, Loss 36322365542400.00, Val loss 2336171229184.00\n","Epoch 520, Loss 35535179972608.00, Val loss 2340337745920.00\n","Epoch 521, Loss 36431541411840.00, Val loss 2358623076352.00\n","Epoch 522, Loss 35945309147136.00, Val loss 2371003875328.00\n","Epoch 523, Loss 36171101685760.00, Val loss 2331069644800.00\n","Epoch 524, Loss 35724555485184.00, Val loss 2365050060800.00\n","Epoch 525, Loss 36573307469824.00, Val loss 2321272012800.00\n","Epoch 526, Loss 35360391182336.00, Val loss 2340583112704.00\n","Epoch 527, Loss 35285044350976.00, Val loss 2324191772672.00\n","Epoch 528, Loss 35416590856192.00, Val loss 2329477906432.00\n","Epoch 529, Loss 35129070379008.00, Val loss 2300380446720.00\n","Epoch 530, Loss 35279047045120.00, Val loss 2322599510016.00\n","Epoch 531, Loss 34875377567744.00, Val loss 2289000513536.00\n","Epoch 532, Loss 35230755823616.00, Val loss 2314650517504.00\n","Epoch 533, Loss 34610849169408.00, Val loss 2248363474944.00\n","Epoch 534, Loss 35297316083712.00, Val loss 2305181614080.00\n","Epoch 535, Loss 34329204332544.00, Val loss 2246225166336.00\n","Epoch 536, Loss 35514753517568.00, Val loss 2293759213568.00\n","Epoch 537, Loss 34542965422080.00, Val loss 2270066376704.00\n","Epoch 538, Loss 34739440214016.00, Val loss 2288823566336.00\n","Epoch 539, Loss 34566887811072.00, Val loss 2275585294336.00\n","Epoch 540, Loss 34592362743808.00, Val loss 2285511901184.00\n","Epoch 541, Loss 34631190587392.00, Val loss 2271510003712.00\n","Epoch 542, Loss 34467934828544.00, Val loss 2276489691136.00\n","Epoch 543, Loss 34192874317824.00, Val loss 2236309569536.00\n","Epoch 544, Loss 34472497520640.00, Val loss 2238096343040.00\n","Epoch 545, Loss 34514390466560.00, Val loss 2222458667008.00\n","Epoch 546, Loss 34686384678912.00, Val loss 2235164524544.00\n","Epoch 547, Loss 34652724287488.00, Val loss 2240966295552.00\n","Epoch 548, Loss 34834468538368.00, Val loss 2250284204032.00\n","Epoch 549, Loss 34474537302016.00, Val loss 2239783239680.00\n","Epoch 550, Loss 34467322345472.00, Val loss 2248608841728.00\n","Epoch 551, Loss 34452227825664.00, Val loss 2240708083712.00\n","Epoch 552, Loss 34387483076608.00, Val loss 2245397315584.00\n","Epoch 553, Loss 34506510196736.00, Val loss 2216766472192.00\n","Epoch 554, Loss 34413751304192.00, Val loss 2240645431296.00\n","Epoch 555, Loss 34428231616512.00, Val loss 2235798126592.00\n","Epoch 556, Loss 34266760015872.00, Val loss 2236160933888.00\n","Epoch 557, Loss 34670981847040.00, Val loss 2203799781376.00\n","Epoch 558, Loss 34413607776256.00, Val loss 2233201065984.00\n","Epoch 559, Loss 34514721359872.00, Val loss 2228773453824.00\n","Epoch 560, Loss 34209806362624.00, Val loss 2229259468800.00\n","Epoch 561, Loss 34431306956800.00, Val loss 2224022093824.00\n","==================================================================\n","Saved best model\n","Epoch 562, Loss 33888920082432.00, Val loss 2042232569856.00\n","Epoch 563, Loss 35840888758272.00, Val loss 2211387015168.00\n","Epoch 564, Loss 34188738283520.00, Val loss 2202593918976.00\n","Epoch 565, Loss 34710838876160.00, Val loss 2213825478656.00\n","Epoch 566, Loss 34199846285312.00, Val loss 2223423094784.00\n","Epoch 567, Loss 33989723879424.00, Val loss 2215091634176.00\n","Epoch 568, Loss 34298676279296.00, Val loss 2219293278208.00\n","Epoch 569, Loss 34090428780544.00, Val loss 2211723870208.00\n","Epoch 570, Loss 34062509012992.00, Val loss 2217067675648.00\n","Epoch 571, Loss 34437734328320.00, Val loss 2209305067520.00\n","Epoch 572, Loss 34122007963648.00, Val loss 2214749536256.00\n","Epoch 573, Loss 33871235020800.00, Val loss 2208733331456.00\n","Epoch 574, Loss 33950530467840.00, Val loss 2212199399424.00\n","Epoch 575, Loss 33929597552640.00, Val loss 2205968236544.00\n","Epoch 576, Loss 34079539349504.00, Val loss 2209765130240.00\n","Epoch 577, Loss 34355432394752.00, Val loss 2201882198016.00\n","Epoch 578, Loss 33992252669952.00, Val loss 2207815303168.00\n","Epoch 579, Loss 33905057656832.00, Val loss 2199493017600.00\n","Epoch 580, Loss 34100660187136.00, Val loss 2202076971008.00\n","Epoch 581, Loss 34886162608128.00, Val loss 2186276503552.00\n","Epoch 582, Loss 34000909242368.00, Val loss 2202872053760.00\n","Epoch 583, Loss 33699447549952.00, Val loss 2200868487168.00\n","Epoch 584, Loss 33832115015680.00, Val loss 2200059772928.00\n","Epoch 585, Loss 33745744390144.00, Val loss 2196733952000.00\n","Epoch 586, Loss 34292969805824.00, Val loss 2196378484736.00\n","Epoch 587, Loss 33870874966016.00, Val loss 2182720782336.00\n","Epoch 588, Loss 33728148738048.00, Val loss 2196871315456.00\n","Epoch 589, Loss 33842164393984.00, Val loss 2193744592896.00\n","Epoch 590, Loss 34037466226688.00, Val loss 2194613338112.00\n","Epoch 591, Loss 34220240947200.00, Val loss 2184451850240.00\n","Epoch 592, Loss 33860693989376.00, Val loss 2193912496128.00\n","Epoch 593, Loss 33505870204928.00, Val loss 2187525226496.00\n","Epoch 594, Loss 34822133483520.00, Val loss 2232472305664.00\n","Epoch 595, Loss 33830595614720.00, Val loss 2212446339072.00\n","Epoch 596, Loss 33732728119296.00, Val loss 2218863362048.00\n","Epoch 597, Loss 33588981497856.00, Val loss 2210864562176.00\n","Epoch 598, Loss 34398044315648.00, Val loss 2209126285312.00\n","Epoch 599, Loss 33685024135168.00, Val loss 2202386825216.00\n","Epoch 600, Loss 34125882271744.00, Val loss 2228540932096.00\n","Epoch 601, Loss 34007688960000.00, Val loss 2221118586880.00\n","Epoch 602, Loss 33993739685888.00, Val loss 2222504804352.00\n","Epoch 603, Loss 34935043715072.00, Val loss 2194984009728.00\n","Epoch 604, Loss 34103122575360.00, Val loss 2216778792960.00\n","Epoch 605, Loss 35876059820032.00, Val loss 2286204485632.00\n","Epoch 606, Loss 34140171198464.00, Val loss 2278094798848.00\n","Epoch 607, Loss 34459826896896.00, Val loss 2286821572608.00\n","Epoch 608, Loss 34796041551872.00, Val loss 2274533310464.00\n","Epoch 609, Loss 34431478276096.00, Val loss 2250034380800.00\n","Epoch 610, Loss 34656757618688.00, Val loss 2267367866368.00\n","Epoch 611, Loss 34383842420736.00, Val loss 2294068019200.00\n","Epoch 612, Loss 33689383047168.00, Val loss 2298523942912.00\n","Epoch 613, Loss 33703005386752.00, Val loss 2277905006592.00\n","Epoch 614, Loss 33724501686272.00, Val loss 2290331418624.00\n","Epoch 615, Loss 34201005764608.00, Val loss 2280044101632.00\n","Epoch 616, Loss 33578638006272.00, Val loss 2282039541760.00\n","Epoch 617, Loss 33531101448192.00, Val loss 2276934287360.00\n","Epoch 618, Loss 32899865100288.00, Val loss 2248195178496.00\n","Epoch 619, Loss 33306953842688.00, Val loss 2230051405824.00\n","Epoch 620, Loss 33243541174272.00, Val loss 2242085126144.00\n","Epoch 621, Loss 33501836361728.00, Val loss 2236060794880.00\n","Epoch 622, Loss 33254858031104.00, Val loss 2234724384768.00\n","Epoch 623, Loss 33087286200320.00, Val loss 2215108673536.00\n","Epoch 624, Loss 33137486809088.00, Val loss 2228596768768.00\n","Epoch 625, Loss 33424918484992.00, Val loss 2221171539968.00\n","Epoch 626, Loss 33570507362304.00, Val loss 2223305392128.00\n","Epoch 627, Loss 33435504605184.00, Val loss 2222296399872.00\n","Epoch 628, Loss 33096351574016.00, Val loss 2213409718272.00\n","Epoch 629, Loss 33428674537472.00, Val loss 2218488758272.00\n","Epoch 630, Loss 32948470956032.00, Val loss 2212825137152.00\n","Epoch 631, Loss 33442280372224.00, Val loss 2213760991232.00\n","Epoch 632, Loss 33233326524416.00, Val loss 2183280852992.00\n","Epoch 633, Loss 33586252621824.00, Val loss 2206822825984.00\n","Epoch 634, Loss 33024604533760.00, Val loss 2207944278016.00\n","Epoch 635, Loss 33383832545280.00, Val loss 2199327604736.00\n","Epoch 636, Loss 32981215177728.00, Val loss 2204729868288.00\n","Epoch 637, Loss 33600367787008.00, Val loss 2186385424384.00\n","Epoch 638, Loss 32998735099904.00, Val loss 2199328915456.00\n","Epoch 639, Loss 33475497192448.00, Val loss 2197121662976.00\n","Epoch 640, Loss 32782573340160.00, Val loss 2163901071360.00\n","Epoch 641, Loss 33551574374400.00, Val loss 2172550381568.00\n","Epoch 642, Loss 34246896636416.00, Val loss 2170926137344.00\n","Epoch 643, Loss 34129877762048.00, Val loss 2236161720320.00\n","Epoch 644, Loss 33284181583872.00, Val loss 2231340367872.00\n","Epoch 645, Loss 33705383714816.00, Val loss 2249381117952.00\n","Epoch 646, Loss 33311982710784.00, Val loss 2242720825344.00\n","Epoch 647, Loss 33471341527040.00, Val loss 2248822489088.00\n","Epoch 648, Loss 33461673033728.00, Val loss 2218789699584.00\n","Epoch 649, Loss 33530136322048.00, Val loss 2267946942464.00\n","Epoch 650, Loss 33161185013760.00, Val loss 2241496350720.00\n","Epoch 651, Loss 33059880001536.00, Val loss 2240144736256.00\n","Epoch 652, Loss 33306513369600.00, Val loss 2261284552704.00\n","Epoch 653, Loss 33298069965312.00, Val loss 2272493043712.00\n","Epoch 654, Loss 33609354737152.00, Val loss 2189772718080.00\n","Epoch 655, Loss 33746864059904.00, Val loss 2208144556032.00\n","Epoch 656, Loss 33307459352576.00, Val loss 2161804181504.00\n","Epoch 657, Loss 33859277314560.00, Val loss 2208963231744.00\n","Epoch 658, Loss 34282494084608.00, Val loss 2242271248384.00\n","Epoch 659, Loss 33391872128512.00, Val loss 2280830271488.00\n","Epoch 660, Loss 32905481486336.00, Val loss 2227218153472.00\n","Epoch 661, Loss 33509733591040.00, Val loss 2274139308032.00\n","Epoch 662, Loss 32931464433152.00, Val loss 2239905136640.00\n","Epoch 663, Loss 33296069262336.00, Val loss 2271509217280.00\n","Epoch 664, Loss 32841477847040.00, Val loss 2217055879168.00\n","Epoch 665, Loss 33556204754432.00, Val loss 2263103307776.00\n","Epoch 666, Loss 32956774125568.00, Val loss 2244855726080.00\n","Epoch 667, Loss 33136763489792.00, Val loss 2259733708800.00\n","Epoch 668, Loss 33491010113024.00, Val loss 2183854817280.00\n","Epoch 669, Loss 33467055994368.00, Val loss 2236636463104.00\n","Epoch 670, Loss 32921595670528.00, Val loss 2205061480448.00\n","Epoch 671, Loss 33975446015488.00, Val loss 2249887580160.00\n","Epoch 672, Loss 33394294548992.00, Val loss 2209978777600.00\n","Epoch 673, Loss 33277021336576.00, Val loss 2246244040704.00\n","Epoch 674, Loss 33530292958208.00, Val loss 2229243478016.00\n","Epoch 675, Loss 33816717907968.00, Val loss 2283007115264.00\n","Epoch 676, Loss 33237433585152.00, Val loss 2218970841088.00\n","Epoch 677, Loss 33719323698688.00, Val loss 2234313867264.00\n","Epoch 678, Loss 33491464677376.00, Val loss 2243758653440.00\n","Epoch 679, Loss 33559891735552.00, Val loss 2273788821504.00\n","Epoch 680, Loss 33157720953344.00, Val loss 2220911230976.00\n","Epoch 681, Loss 33666578356736.00, Val loss 2249826500608.00\n","Epoch 682, Loss 33301990371328.00, Val loss 2240447250432.00\n","Epoch 683, Loss 33448092146176.00, Val loss 2251102093312.00\n","Epoch 684, Loss 33219513281024.00, Val loss 2208315473920.00\n","Epoch 685, Loss 33636323948032.00, Val loss 2266057408512.00\n","Epoch 686, Loss 33085970931712.00, Val loss 2218459136000.00\n","Epoch 687, Loss 33517511652352.00, Val loss 2253087834112.00\n","Epoch 688, Loss 33177676842496.00, Val loss 2235830108160.00\n","Epoch 689, Loss 33348800829440.00, Val loss 2243564404736.00\n","Epoch 690, Loss 33183386014720.00, Val loss 2218943840256.00\n","Epoch 691, Loss 33438774154752.00, Val loss 2248573452288.00\n","Epoch 692, Loss 33082588277760.00, Val loss 2214565249024.00\n","Epoch 693, Loss 33409887116800.00, Val loss 2241653899264.00\n","Epoch 694, Loss 33109577992704.00, Val loss 2225027940352.00\n","Epoch 695, Loss 33242625478144.00, Val loss 2223592964096.00\n","Epoch 696, Loss 33186802378752.00, Val loss 2210463481856.00\n","Epoch 697, Loss 33334201236480.00, Val loss 2229012267008.00\n","Epoch 698, Loss 33016809556992.00, Val loss 2175656263680.00\n","Epoch 699, Loss 33455843192320.00, Val loss 2212331782144.00\n","Epoch 700, Loss 33112525435392.00, Val loss 2206608130048.00\n","Epoch 701, Loss 33168960787456.00, Val loss 2198047293440.00\n","Epoch 702, Loss 33115112166400.00, Val loss 2191036776448.00\n","Epoch 703, Loss 33204026649088.00, Val loss 2203266580480.00\n","Epoch 704, Loss 32818208753664.00, Val loss 2054350831616.00\n","Epoch 705, Loss 33668543966720.00, Val loss 2198332899328.00\n","Epoch 706, Loss 33001726600704.00, Val loss 2193104961536.00\n","Epoch 707, Loss 33030350792192.00, Val loss 2189950189568.00\n","Epoch 708, Loss 32988812498944.00, Val loss 2185494790144.00\n","Epoch 709, Loss 33099894644736.00, Val loss 2199670489088.00\n","Epoch 710, Loss 32850959428608.00, Val loss 2179084058624.00\n","Epoch 711, Loss 33047087224320.00, Val loss 2191277686784.00\n","Epoch 712, Loss 32924261008896.00, Val loss 2189046710272.00\n","Epoch 713, Loss 32938830323200.00, Val loss 2188514295808.00\n","Epoch 714, Loss 32952760094720.00, Val loss 2182014959616.00\n","Epoch 715, Loss 32964596750336.00, Val loss 2195204472832.00\n","Epoch 716, Loss 32803381370368.00, Val loss 2176746913792.00\n","Epoch 717, Loss 32995449774592.00, Val loss 2174203985920.00\n","Epoch 718, Loss 33072966189056.00, Val loss 2172396896256.00\n","Epoch 719, Loss 33070928660992.00, Val loss 2172013117440.00\n","Epoch 720, Loss 33032837413888.00, Val loss 2167017439232.00\n","Epoch 721, Loss 33167399290880.00, Val loss 2179342925824.00\n","Epoch 722, Loss 32956719290368.00, Val loss 2164252999680.00\n","Epoch 723, Loss 33109615305216.00, Val loss 2173558194176.00\n","Epoch 724, Loss 32979008787968.00, Val loss 2169079595008.00\n","Epoch 725, Loss 33050676121600.00, Val loss 2172064628736.00\n","Epoch 726, Loss 35878199205888.00, Val loss 2208967688192.00\n","Epoch 727, Loss 34499151314944.00, Val loss 2279371177984.00\n","Epoch 728, Loss 33659195601920.00, Val loss 2191602352128.00\n","Epoch 729, Loss 34243901053952.00, Val loss 2264274305024.00\n","Epoch 730, Loss 33834943705088.00, Val loss 2201804865536.00\n","Epoch 731, Loss 33968410148352.00, Val loss 2230931161088.00\n","Epoch 732, Loss 34553761829376.00, Val loss 2267884290048.00\n","Epoch 733, Loss 33494892853760.00, Val loss 2197446328320.00\n","Epoch 734, Loss 35599387722752.00, Val loss 2307723100160.00\n","Epoch 735, Loss 35659633637376.00, Val loss 2354914787328.00\n","Epoch 736, Loss 34868804301824.00, Val loss 2253960249344.00\n","Epoch 737, Loss 33393036242944.00, Val loss 2245522882560.00\n","Epoch 738, Loss 34769233523200.00, Val loss 2278238715904.00\n","Epoch 739, Loss 34507891731456.00, Val loss 2314733879296.00\n","Epoch 740, Loss 33788832786944.00, Val loss 2237773905920.00\n","Epoch 741, Loss 34003817819648.00, Val loss 2242242674688.00\n","Epoch 742, Loss 35951504769024.00, Val loss 2227462209536.00\n","Epoch 743, Loss 34029583045120.00, Val loss 2225070669824.00\n","Epoch 744, Loss 34800195457536.00, Val loss 2237135060992.00\n","Epoch 745, Loss 33715116096512.00, Val loss 2222761443328.00\n","Epoch 746, Loss 34407764168192.00, Val loss 2235720007680.00\n","Epoch 747, Loss 34633353167360.00, Val loss 2288224567296.00\n","Epoch 748, Loss 34007425988608.00, Val loss 2179479764992.00\n","Epoch 749, Loss 32858934588416.00, Val loss 2178965569536.00\n","Epoch 750, Loss 33155255344640.00, Val loss 2179448307712.00\n","Epoch 751, Loss 34280600336384.00, Val loss 2257379655680.00\n","Epoch 752, Loss 33782959936512.00, Val loss 2217647013888.00\n","Epoch 753, Loss 33708547470336.00, Val loss 2214813499392.00\n","Epoch 754, Loss 33734305635840.00, Val loss 2217711501312.00\n","Epoch 755, Loss 33359527911424.00, Val loss 2193061707776.00\n","Epoch 756, Loss 33086744506368.00, Val loss 2191412428800.00\n","Epoch 757, Loss 33197428441600.00, Val loss 2149499011072.00\n","Epoch 758, Loss 34497453817344.00, Val loss 2197658402816.00\n","Epoch 759, Loss 34540518211584.00, Val loss 2270819778560.00\n","Epoch 760, Loss 33392317494784.00, Val loss 2162087428096.00\n","Epoch 761, Loss 32723635162112.00, Val loss 2161503371264.00\n","Epoch 762, Loss 32848244740608.00, Val loss 2162044829696.00\n","Epoch 763, Loss 33374667441152.00, Val loss 2184723169280.00\n","Epoch 764, Loss 33582310490112.00, Val loss 2205700063232.00\n","Epoch 765, Loss 34142317977088.00, Val loss 2215579222016.00\n","Epoch 766, Loss 33393674982400.00, Val loss 2199998693376.00\n","Epoch 767, Loss 33365805756416.00, Val loss 2213238013952.00\n","Epoch 768, Loss 33437347231232.00, Val loss 2207032541184.00\n","Epoch 769, Loss 33325789558784.00, Val loss 2210779627520.00\n","Epoch 770, Loss 33451970390528.00, Val loss 2205460987904.00\n","Epoch 771, Loss 33480390640640.00, Val loss 2203988000768.00\n","Epoch 772, Loss 33656633824768.00, Val loss 2201199050752.00\n","Epoch 773, Loss 33335859762176.00, Val loss 2210772549632.00\n","Epoch 774, Loss 34440111706624.00, Val loss 2208603570176.00\n","Epoch 775, Loss 33717717884928.00, Val loss 2161926995968.00\n","Epoch 776, Loss 33504861561856.00, Val loss 2228211154944.00\n","Epoch 777, Loss 34621028159488.00, Val loss 2199998955520.00\n","Epoch 778, Loss 33318120218624.00, Val loss 2186319233024.00\n","Epoch 779, Loss 33864179618816.00, Val loss 2200320081920.00\n","Epoch 780, Loss 33036888880128.00, Val loss 2190709882880.00\n","Epoch 781, Loss 33341661445632.00, Val loss 2156170051584.00\n","Epoch 782, Loss 32670847959040.00, Val loss 2163761479680.00\n","Epoch 783, Loss 32724914377728.00, Val loss 2207929073664.00\n","Epoch 784, Loss 32723581876224.00, Val loss 2196581515264.00\n","Epoch 785, Loss 32491267785728.00, Val loss 2182886457344.00\n","Epoch 786, Loss 33646844591616.00, Val loss 2214208471040.00\n","Epoch 787, Loss 32631869627904.00, Val loss 2209241890816.00\n","Epoch 788, Loss 33424933630976.00, Val loss 2183984971776.00\n","Epoch 789, Loss 32364720000000.00, Val loss 2179096248320.00\n","Epoch 790, Loss 32957088450048.00, Val loss 2200495980544.00\n","Epoch 791, Loss 33598261064704.00, Val loss 2236766748672.00\n","Epoch 792, Loss 33879484155392.00, Val loss 2207872712704.00\n","Epoch 793, Loss 33027964060160.00, Val loss 2172311044096.00\n","Epoch 794, Loss 33660024446464.00, Val loss 2168075583488.00\n","Epoch 795, Loss 33266638430208.00, Val loss 2206955995136.00\n","Epoch 796, Loss 33520369120768.00, Val loss 2164284456960.00\n","Epoch 797, Loss 32534825646080.00, Val loss 2162840436736.00\n","Epoch 798, Loss 32876523376128.00, Val loss 2162834931712.00\n","Epoch 799, Loss 33330851427840.00, Val loss 2164033191936.00\n","Epoch 800, Loss 34172737300480.00, Val loss 2259379027968.00\n","Epoch 801, Loss 33244240843264.00, Val loss 2218656792576.00\n","Epoch 802, Loss 33158181426176.00, Val loss 2213830197248.00\n","Epoch 803, Loss 32626551275008.00, Val loss 2171587788800.00\n","Epoch 804, Loss 32919162223104.00, Val loss 2148304551936.00\n","Epoch 805, Loss 33819852231168.00, Val loss 2207391416320.00\n","Epoch 806, Loss 33206966123008.00, Val loss 2095594602496.00\n","Epoch 807, Loss 32899558274048.00, Val loss 2138467467264.00\n","Epoch 808, Loss 33381543772160.00, Val loss 2160988651520.00\n","Epoch 809, Loss 32777542355968.00, Val loss 2160534093824.00\n","Epoch 810, Loss 33487362713600.00, Val loss 2150036013056.00\n","Epoch 811, Loss 32675071831552.00, Val loss 2153835397120.00\n","Epoch 812, Loss 32536248174592.00, Val loss 2077557653504.00\n","Epoch 813, Loss 34650145062912.00, Val loss 2088658796544.00\n","Epoch 814, Loss 35009878642176.00, Val loss 2189490651136.00\n","Epoch 815, Loss 34305733274624.00, Val loss 2148213063680.00\n","Epoch 816, Loss 33706928435200.00, Val loss 2114258337792.00\n","Epoch 817, Loss 33615517616128.00, Val loss 2174768513024.00\n","Epoch 818, Loss 33030320358400.00, Val loss 2172829171712.00\n","Epoch 819, Loss 32971645151232.00, Val loss 2175645253632.00\n","Epoch 820, Loss 32928133400064.00, Val loss 2180464377856.00\n","Epoch 821, Loss 33553214023168.00, Val loss 2175733202944.00\n","Epoch 822, Loss 32991617718784.00, Val loss 2181544280064.00\n","Epoch 823, Loss 32862290304512.00, Val loss 2162541461504.00\n","Epoch 824, Loss 33435537459712.00, Val loss 2182764822528.00\n","Epoch 825, Loss 32780452307968.00, Val loss 2168671436800.00\n","Epoch 826, Loss 33486984639488.00, Val loss 2152692842496.00\n","Epoch 827, Loss 32463674848768.00, Val loss 2138851770368.00\n","Epoch 828, Loss 33024797593600.00, Val loss 2176276103168.00\n","Epoch 829, Loss 32668524892160.00, Val loss 2163934756864.00\n","Epoch 830, Loss 33398142072320.00, Val loss 2179282632704.00\n","Epoch 831, Loss 32440402529280.00, Val loss 2167438704640.00\n","Epoch 832, Loss 32516531445760.00, Val loss 2170356367360.00\n","Epoch 833, Loss 32327016650752.00, Val loss 2164337672192.00\n","Epoch 834, Loss 32427990667776.00, Val loss 2175118213120.00\n","Epoch 835, Loss 33068512832512.00, Val loss 2146205958144.00\n","Epoch 836, Loss 33052596929024.00, Val loss 2170879213568.00\n","Epoch 837, Loss 32189679657472.00, Val loss 2162117181440.00\n","Epoch 838, Loss 32264571388416.00, Val loss 2167388372992.00\n","Epoch 839, Loss 32169055120896.00, Val loss 2158822424576.00\n","Epoch 840, Loss 32412786894848.00, Val loss 2165010071552.00\n","Epoch 841, Loss 32161595352576.00, Val loss 2160150839296.00\n","Epoch 842, Loss 32844061713408.00, Val loss 2129702027264.00\n","Epoch 843, Loss 32263690752512.00, Val loss 2151655931904.00\n","Epoch 844, Loss 33046491837952.00, Val loss 2192671637504.00\n","Epoch 845, Loss 32779007736320.00, Val loss 2185063694336.00\n","Epoch 846, Loss 32829418236928.00, Val loss 2197818048512.00\n","Epoch 847, Loss 32485266096128.00, Val loss 2151115259904.00\n","Epoch 848, Loss 32134511397888.00, Val loss 2153022750720.00\n","Epoch 849, Loss 32125695245824.00, Val loss 2150925860864.00\n","Epoch 850, Loss 32140236864000.00, Val loss 2157292158976.00\n","Epoch 851, Loss 32064701023232.00, Val loss 2148384768000.00\n","Epoch 852, Loss 32953961249280.00, Val loss 2197892759552.00\n","Epoch 853, Loss 32506977328128.00, Val loss 2161440849920.00\n","Epoch 854, Loss 32324116292608.00, Val loss 2169768116224.00\n","Epoch 855, Loss 32222491879424.00, Val loss 2144116670464.00\n","Epoch 856, Loss 32925494704128.00, Val loss 2190773059584.00\n","Epoch 857, Loss 32547378890752.00, Val loss 2164073824256.00\n","Epoch 858, Loss 32226279500288.00, Val loss 2158592786432.00\n","Epoch 859, Loss 32748307709952.00, Val loss 2144555237376.00\n","Epoch 860, Loss 32961546887680.00, Val loss 2182248136704.00\n","Epoch 861, Loss 32598350731264.00, Val loss 2161295491072.00\n","Epoch 862, Loss 32699990636544.00, Val loss 2173328818176.00\n","Epoch 863, Loss 32692332021760.00, Val loss 2185924575232.00\n","Epoch 864, Loss 32548689094144.00, Val loss 2181564989440.00\n","Epoch 865, Loss 32548274834432.00, Val loss 2181719523328.00\n","Epoch 866, Loss 32391733309440.00, Val loss 2111812534272.00\n","Epoch 867, Loss 32957735890944.00, Val loss 2177061093376.00\n","Epoch 868, Loss 32516944840192.00, Val loss 2174394826752.00\n","Epoch 869, Loss 32356363336192.00, Val loss 2138951385088.00\n","Epoch 870, Loss 32162066421248.00, Val loss 2144557203456.00\n","Epoch 871, Loss 32134449134080.00, Val loss 2140888104960.00\n","Epoch 872, Loss 32060333277184.00, Val loss 2137006538752.00\n","Epoch 873, Loss 32018774681600.00, Val loss 2135848648704.00\n","Epoch 874, Loss 32046789393920.00, Val loss 2128022077440.00\n","Epoch 875, Loss 32014501204480.00, Val loss 2127350857728.00\n","Epoch 876, Loss 32030113891328.00, Val loss 2127513124864.00\n","Epoch 877, Loss 31992441408000.00, Val loss 2124134744064.00\n","Epoch 878, Loss 31848488671232.00, Val loss 2117398429696.00\n","Epoch 879, Loss 31902149386240.00, Val loss 2119766245376.00\n","Epoch 880, Loss 31769159470080.00, Val loss 2116700864512.00\n","Epoch 881, Loss 31818646104576.00, Val loss 2113668644864.00\n","Epoch 882, Loss 31743649309696.00, Val loss 2110825037824.00\n","Epoch 883, Loss 31805795825152.00, Val loss 2110542053376.00\n","Epoch 884, Loss 31700383355904.00, Val loss 2108782542848.00\n","Epoch 885, Loss 31773486496768.00, Val loss 2105085001728.00\n","Epoch 886, Loss 31684859663360.00, Val loss 2105203621888.00\n","Epoch 887, Loss 31750633911808.00, Val loss 2103158636544.00\n","Epoch 888, Loss 31653792588288.00, Val loss 2101796929536.00\n","Epoch 889, Loss 31723425836544.00, Val loss 2099951173632.00\n","Epoch 890, Loss 32476981858816.00, Val loss 2132906082304.00\n","Epoch 891, Loss 32126539196416.00, Val loss 2097525555200.00\n","Epoch 892, Loss 31396711993344.00, Val loss 2065635475456.00\n","Epoch 893, Loss 31479351630848.00, Val loss 2078673338368.00\n","Epoch 894, Loss 32408775512576.00, Val loss 2049005060096.00\n","==================================================================\n","Saved best model\n","Epoch 895, Loss 33713344388608.00, Val loss 2038992076800.00\n","Epoch 896, Loss 34770357295616.00, Val loss 2138435092480.00\n","==================================================================\n","Saved best model\n","Epoch 897, Loss 34286249963008.00, Val loss 2033628610560.00\n","Epoch 898, Loss 34187231534592.00, Val loss 2107223703552.00\n","Epoch 899, Loss 33507802612736.00, Val loss 2171599192064.00\n","Epoch 900, Loss 32686710068224.00, Val loss 2175061196800.00\n","Epoch 901, Loss 32011880239104.00, Val loss 2123007262720.00\n","Epoch 902, Loss 32378995283968.00, Val loss 2149267668992.00\n","Epoch 903, Loss 32204432278016.00, Val loss 2144308822016.00\n","Epoch 904, Loss 32059369814528.00, Val loss 2110800920576.00\n","Epoch 905, Loss 32325783554560.00, Val loss 2138183696384.00\n","Epoch 906, Loss 33416023590400.00, Val loss 2133409923072.00\n","Epoch 907, Loss 32052997795328.00, Val loss 2129942544384.00\n","Epoch 908, Loss 32047248477696.00, Val loss 2129759174656.00\n","Epoch 909, Loss 32025385510400.00, Val loss 2117761105920.00\n","Epoch 910, Loss 32075499468800.00, Val loss 2117262639104.00\n","Epoch 911, Loss 32095458007552.00, Val loss 2124998246400.00\n","Epoch 912, Loss 31955969975296.00, Val loss 2100686094336.00\n","Epoch 913, Loss 32120791986176.00, Val loss 2121242640384.00\n","Epoch 914, Loss 31976603035136.00, Val loss 2120827535360.00\n","Epoch 915, Loss 31976489643520.00, Val loss 2110689247232.00\n","Epoch 916, Loss 32006982709248.00, Val loss 2117499355136.00\n","Epoch 917, Loss 31968588940800.00, Val loss 2114819194880.00\n","Epoch 918, Loss 31955046315008.00, Val loss 2094441824256.00\n","Epoch 919, Loss 32091742218752.00, Val loss 2111400181760.00\n","Epoch 920, Loss 31979038366208.00, Val loss 2112890732544.00\n","Epoch 921, Loss 31971236486656.00, Val loss 2105943654400.00\n","Epoch 922, Loss 31973462965760.00, Val loss 2107111768064.00\n","Epoch 923, Loss 31982685389312.00, Val loss 2108073705472.00\n","Epoch 924, Loss 31980448185856.00, Val loss 2103816749056.00\n","Epoch 925, Loss 31924640052224.00, Val loss 2080018399232.00\n","Epoch 926, Loss 32177832824320.00, Val loss 2101326118912.00\n","Epoch 927, Loss 32004306832896.00, Val loss 2101881077760.00\n","Epoch 928, Loss 31966323714048.00, Val loss 2093279084544.00\n","Epoch 929, Loss 32041766747648.00, Val loss 2103477403648.00\n","Epoch 930, Loss 31978037091840.00, Val loss 2090457628672.00\n","Epoch 931, Loss 32035748854272.00, Val loss 2101731786752.00\n","Epoch 932, Loss 31898035565056.00, Val loss 2087431569408.00\n","Epoch 933, Loss 32080098088960.00, Val loss 2096196354048.00\n","Epoch 934, Loss 31896787285504.00, Val loss 2088984510464.00\n","Epoch 935, Loss 32376433312768.00, Val loss 2097370890240.00\n","Epoch 936, Loss 31928659960832.00, Val loss 2083166355456.00\n","Epoch 937, Loss 32021981593088.00, Val loss 2094640267264.00\n","Epoch 938, Loss 31882026265088.00, Val loss 2086526779392.00\n","Epoch 939, Loss 31973261336064.00, Val loss 2088044462080.00\n","Epoch 940, Loss 31909447691264.00, Val loss 2085041471488.00\n","Epoch 941, Loss 31924784961024.00, Val loss 2090957275136.00\n","Epoch 942, Loss 31907254019072.00, Val loss 2082778775552.00\n","Epoch 943, Loss 31909116192256.00, Val loss 2088544501760.00\n","Epoch 944, Loss 31841685336576.00, Val loss 2080198098944.00\n","Epoch 945, Loss 31958741958656.00, Val loss 2086613811200.00\n","Epoch 946, Loss 31834260320256.00, Val loss 2078568349696.00\n","Epoch 947, Loss 31893803249664.00, Val loss 2083459956736.00\n","Epoch 948, Loss 31873137886720.00, Val loss 2076567928832.00\n","Epoch 949, Loss 31888508902400.00, Val loss 2081654964224.00\n","Epoch 950, Loss 31797484768256.00, Val loss 2075554611200.00\n","Epoch 951, Loss 31913889415168.00, Val loss 2080470859776.00\n","Epoch 952, Loss 31764906952192.00, Val loss 2075106738176.00\n","Epoch 953, Loss 31846668446208.00, Val loss 2080423280640.00\n","Epoch 954, Loss 31784397094400.00, Val loss 2071501209600.00\n","Epoch 955, Loss 31813063224832.00, Val loss 2068394016768.00\n","Epoch 956, Loss 31796482193408.00, Val loss 2074542211072.00\n","Epoch 957, Loss 31998724558336.00, Val loss 2074485194752.00\n","Epoch 958, Loss 31742796213248.00, Val loss 2072837488640.00\n","Epoch 959, Loss 31742344452096.00, Val loss 2077693575168.00\n","Epoch 960, Loss 32320222299136.00, Val loss 2108821995520.00\n","Epoch 961, Loss 31666169675776.00, Val loss 2104237096960.00\n","Epoch 962, Loss 31681925033984.00, Val loss 2103612407808.00\n","Epoch 963, Loss 31685126856704.00, Val loss 2103042768896.00\n","Epoch 964, Loss 31605397114880.00, Val loss 2096408690688.00\n","Epoch 965, Loss 31678415337472.00, Val loss 2103446470656.00\n","Epoch 966, Loss 31506952895488.00, Val loss 2100599193600.00\n","Epoch 967, Loss 31505335080448.00, Val loss 2096959848448.00\n","Epoch 968, Loss 31434450318336.00, Val loss 2087878393856.00\n","Epoch 969, Loss 31510929161216.00, Val loss 2090669441024.00\n","Epoch 970, Loss 31466716235776.00, Val loss 2089628860416.00\n","Epoch 971, Loss 31509672655872.00, Val loss 2090075947008.00\n","Epoch 972, Loss 31493907944448.00, Val loss 2083133587456.00\n","Epoch 973, Loss 31410872015872.00, Val loss 2081821818880.00\n","Epoch 974, Loss 31481878180352.00, Val loss 2082810626048.00\n","Epoch 975, Loss 31399645402112.00, Val loss 2075279360000.00\n","Epoch 976, Loss 31503638818304.00, Val loss 2080431276032.00\n","Epoch 977, Loss 31380995936256.00, Val loss 2076875685888.00\n","Epoch 978, Loss 31505440471040.00, Val loss 2072163909632.00\n","Epoch 979, Loss 31441533437440.00, Val loss 2073509232640.00\n","Epoch 980, Loss 31526380891648.00, Val loss 2073051660288.00\n","Epoch 981, Loss 31401245489152.00, Val loss 2070055223296.00\n","Epoch 982, Loss 31497768367616.00, Val loss 2069395406848.00\n","Epoch 983, Loss 31387216838144.00, Val loss 2068148387840.00\n","Epoch 984, Loss 31439221906944.00, Val loss 2059983126528.00\n","==================================================================\n","Saved best model\n","Epoch 985, Loss 31139917818880.00, Val loss 2023987740672.00\n","Epoch 986, Loss 31727955202560.00, Val loss 2061852606464.00\n","Epoch 987, Loss 31177652558336.00, Val loss 2062517665792.00\n","Epoch 988, Loss 31949107564544.00, Val loss 2100801699840.00\n","Epoch 989, Loss 31479728249344.00, Val loss 2062739832832.00\n","Epoch 990, Loss 31278066904064.00, Val loss 2063152971776.00\n","Epoch 991, Loss 32125125560832.00, Val loss 2102398681088.00\n","Epoch 992, Loss 31647066852352.00, Val loss 2098269257728.00\n","Epoch 993, Loss 31640269708800.00, Val loss 2100857929728.00\n","Epoch 994, Loss 31614124249600.00, Val loss 2098691047424.00\n","Epoch 995, Loss 31561509209088.00, Val loss 2078267408384.00\n","Epoch 996, Loss 31595012353536.00, Val loss 2063758917632.00\n","Epoch 997, Loss 31267317526528.00, Val loss 2062644936704.00\n","Epoch 998, Loss 31291609927168.00, Val loss 2062165344256.00\n","Epoch 999, Loss 32121599972864.00, Val loss 2089563193344.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"status":"ok","timestamp":1648745284347,"user_tz":300,"elapsed":220,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"c51489c6-13ed-40c6-a188-720da11b211c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2431431737344.0\n","tensor([13463.0586, 13508.6543, 13549.7822, 13589.4062, 13630.3613, 13674.8369,\n","        13720.2861, 13765.4502, 13813.0449, 13870.2744, 13908.8525, 13954.3213,\n","        14002.5654, 14055.3535, 14102.5186], grad_fn=<SelectBackward0>)\n","tensor([292187., 293697., 293697., 293697., 293697., 293697., 295701., 296870.,\n","        297729., 297729., 297729., 298362., 298626., 298808., 298993.])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","    # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_cumulative_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_cumulative_infected_tensor':labeled_output # (52, 15)\n","}\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)"],"metadata":{"id":"s3-8Yge6CAWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_squared_error = criterion(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","mae_function = torch.nn.L1Loss()\n","mean_absolute_error = mae_function(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","print(\"mean squared error: \", mean_squared_error)\n","print(\"mean absolute error: \", mean_absolute_error)\n","\n","# With seed = 0 - trial 1\n","# mean squared error:  2431431737344.0\n","# mean absolute error:  978706.625\n","\n","# With seed = 0 - trial 2\n","# mean squared error:  2431431737344.0\n","# mean absolute error:  978706.625"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzdfnLLl7JwD","executionInfo":{"status":"ok","timestamp":1648745284350,"user_tz":300,"elapsed":12,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"f8ea61ac-3a13-4fc5-ee50-75d5cf2b4003"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean squared error:  2431431737344.0\n","mean absolute error:  978706.625\n"]}]}]}