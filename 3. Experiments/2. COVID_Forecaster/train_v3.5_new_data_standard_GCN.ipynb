{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.5_new_data_standard_GCN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMSdCiy2Lws0K4TIf7wT8e9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1648740735790,"user_tz":300,"elapsed":1183,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"9edb0ce4-f4dc-4331-bc6f-2d1508a74bab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb055482-f017-4847-d5b4-536cf888e86d","executionInfo":{"status":"ok","timestamp":1648740764693,"user_tz":300,"elapsed":28907,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.13)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.6.0)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: torch-geometric-temporal in /usr/local/lib/python3.7/dist-packages (0.51.0)\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.13)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (1.1.0)\n","Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.3)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","PyTorch has version 1.10.0+cu111\n","Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 24\n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3.3_to_3.5_new_data.pickle'\n","save_model_relative_path = './saved_models/v3.5_new_data_standard_GCN'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3.5_archived_output.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1648740764695,"user_tz":300,"elapsed":50,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"70de808d-bddc-4466-8298-a0d5ea127626"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["# Try to ensure reproducibility\n","torch.manual_seed(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-xmrOl1khJk","executionInfo":{"status":"ok","timestamp":1648740764700,"user_tz":300,"elapsed":44,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"e3eefec2-f59b-4c4a-b543-7c313ffabeb3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd81f01f9f0>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_confirmed_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_confirmed_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_confirmed_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class GCN_standard(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = GCNConv(inputLayer_num_features, hiddenLayer1_num_features)\n","        self.conv2 = GCNConv(hiddenLayer1_num_features, outputLayer_num_features)\n","        self.linear = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      \n","      x = self.linear(x)\n","\n","      return x\n","\n","model = GCN_standard().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648740764705,"user_tz":300,"elapsed":36,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"8fb81585-bef1-4415-b114-fae0307d8f35"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GCN_standard(\n","  (conv1): GCNConv(24, 24)\n","  (conv2): GCNConv(24, 15)\n","  (linear): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648741057728,"user_tz":300,"elapsed":293054,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"f4cdcb2b-1deb-4f00-ae4e-5945c0f7bb56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 37021355954688.00, Val loss 1830307233792.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 36960693410944.00, Val loss 1829706268672.00\n","==================================================================\n","Saved best model\n","Epoch 2, Loss 36956518577408.00, Val loss 1828473929728.00\n","Epoch 3, Loss 36955478758912.00, Val loss 1829356175360.00\n","==================================================================\n","Saved best model\n","Epoch 4, Loss 36955295650688.00, Val loss 1826053160960.00\n","Epoch 5, Loss 36957299465984.00, Val loss 1826192228352.00\n","Epoch 6, Loss 36957511467136.00, Val loss 1827387473920.00\n","==================================================================\n","Saved best model\n","Epoch 7, Loss 36956715975936.00, Val loss 1824808108032.00\n","Epoch 8, Loss 36958961561344.00, Val loss 1829702336512.00\n","==================================================================\n","Saved best model\n","Epoch 9, Loss 36956886923392.00, Val loss 1824723959808.00\n","Epoch 10, Loss 36957818263168.00, Val loss 1828668047360.00\n","Epoch 11, Loss 36956166342272.00, Val loss 1825009696768.00\n","Epoch 12, Loss 36956278959872.00, Val loss 1827849633792.00\n","Epoch 13, Loss 36955460780928.00, Val loss 1825937162240.00\n","Epoch 14, Loss 36954819843072.00, Val loss 1826595667968.00\n","Epoch 15, Loss 36954293339648.00, Val loss 1826645082112.00\n","Epoch 16, Loss 36953839158272.00, Val loss 1826857943040.00\n","Epoch 17, Loss 36953573192320.00, Val loss 1826957557760.00\n","Epoch 18, Loss 36953404932352.00, Val loss 1826742206464.00\n","Epoch 19, Loss 36953159688320.00, Val loss 1825842921472.00\n","==================================================================\n","Saved best model\n","Epoch 20, Loss 36952603288320.00, Val loss 1824291291136.00\n","==================================================================\n","Saved best model\n","Epoch 21, Loss 36951991012480.00, Val loss 1823083200512.00\n","==================================================================\n","Saved best model\n","Epoch 22, Loss 36952347611776.00, Val loss 1822418796544.00\n","Epoch 23, Loss 37004289704960.00, Val loss 1827079979008.00\n","Epoch 24, Loss 36953487060736.00, Val loss 1825994440704.00\n","Epoch 25, Loss 36952625109376.00, Val loss 1826942484480.00\n","Epoch 26, Loss 36955651104384.00, Val loss 1826228666368.00\n","Epoch 27, Loss 36955404565888.00, Val loss 1825189658624.00\n","Epoch 28, Loss 36953625163136.00, Val loss 1825096859648.00\n","Epoch 29, Loss 36952857327744.00, Val loss 1825138409472.00\n","Epoch 30, Loss 36952962581120.00, Val loss 1824991346688.00\n","Epoch 31, Loss 36952756608512.00, Val loss 1824719634432.00\n","Epoch 32, Loss 36952072434944.00, Val loss 1824456704000.00\n","Epoch 33, Loss 36951176169600.00, Val loss 1824251707392.00\n","Epoch 34, Loss 36950311749120.00, Val loss 1824098615296.00\n","Epoch 35, Loss 36949546286336.00, Val loss 1823983665152.00\n","Epoch 36, Loss 36948848257024.00, Val loss 1823895322624.00\n","Epoch 37, Loss 36948142793984.00, Val loss 1823820480512.00\n","Epoch 38, Loss 36947348601984.00, Val loss 1823741050880.00\n","Epoch 39, Loss 36946409062272.00, Val loss 1823639470080.00\n","Epoch 40, Loss 36945291462272.00, Val loss 1823498960896.00\n","Epoch 41, Loss 36943977812608.00, Val loss 1823306547200.00\n","Epoch 42, Loss 36942459208960.00, Val loss 1823053971456.00\n","Epoch 43, Loss 36940731055616.00, Val loss 1822743461888.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 36938816709760.00, Val loss 1822400053248.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 36936795949824.00, Val loss 1822081417216.00\n","==================================================================\n","Saved best model\n","Epoch 46, Loss 36934824724480.00, Val loss 1821869867008.00\n","==================================================================\n","Saved best model\n","Epoch 47, Loss 36933073557888.00, Val loss 1821804986368.00\n","Epoch 48, Loss 36931525467904.00, Val loss 1821811277824.00\n","==================================================================\n","Saved best model\n","Epoch 49, Loss 36929842814080.00, Val loss 1821730537472.00\n","==================================================================\n","Saved best model\n","Epoch 50, Loss 36927585319808.00, Val loss 1821444407296.00\n","==================================================================\n","Saved best model\n","Epoch 51, Loss 36924576535936.00, Val loss 1820977397760.00\n","==================================================================\n","Saved best model\n","Epoch 52, Loss 36921071657088.00, Val loss 1820531884032.00\n","==================================================================\n","Saved best model\n","Epoch 53, Loss 36917643681024.00, Val loss 1820288745472.00\n","==================================================================\n","Saved best model\n","Epoch 54, Loss 36914549583104.00, Val loss 1820128837632.00\n","==================================================================\n","Saved best model\n","Epoch 55, Loss 36911304708096.00, Val loss 1819817279488.00\n","==================================================================\n","Saved best model\n","Epoch 56, Loss 36907425316992.00, Val loss 1819350401024.00\n","==================================================================\n","Saved best model\n","Epoch 57, Loss 36903038102528.00, Val loss 1818836336640.00\n","==================================================================\n","Saved best model\n","Epoch 58, Loss 36898382025728.00, Val loss 1818333806592.00\n","==================================================================\n","Saved best model\n","Epoch 59, Loss 36893534865280.00, Val loss 1817858539520.00\n","==================================================================\n","Saved best model\n","Epoch 60, Loss 36888512233472.00, Val loss 1817411846144.00\n","==================================================================\n","Saved best model\n","Epoch 61, Loss 36883317867776.00, Val loss 1816990187520.00\n","==================================================================\n","Saved best model\n","Epoch 62, Loss 36877951160960.00, Val loss 1816589762560.00\n","==================================================================\n","Saved best model\n","Epoch 63, Loss 36872431167616.00, Val loss 1816207425536.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 36866778634624.00, Val loss 1815841079296.00\n","==================================================================\n","Saved best model\n","Epoch 65, Loss 36861008991232.00, Val loss 1815488233472.00\n","==================================================================\n","Saved best model\n","Epoch 66, Loss 36855154060032.00, Val loss 1815147839488.00\n","==================================================================\n","Saved best model\n","Epoch 67, Loss 36849228827904.00, Val loss 1814817669120.00\n","==================================================================\n","Saved best model\n","Epoch 68, Loss 36843254725248.00, Val loss 1814496804864.00\n","==================================================================\n","Saved best model\n","Epoch 69, Loss 36837245504640.00, Val loss 1814184067072.00\n","==================================================================\n","Saved best model\n","Epoch 70, Loss 36831179809280.00, Val loss 1813877620736.00\n","==================================================================\n","Saved best model\n","Epoch 71, Loss 36824930549376.00, Val loss 1813577072640.00\n","==================================================================\n","Saved best model\n","Epoch 72, Loss 36818434475520.00, Val loss 1813273640960.00\n","==================================================================\n","Saved best model\n","Epoch 73, Loss 36811374817920.00, Val loss 1812938620928.00\n","==================================================================\n","Saved best model\n","Epoch 74, Loss 36802903865856.00, Val loss 1812511457280.00\n","==================================================================\n","Saved best model\n","Epoch 75, Loss 36791895080192.00, Val loss 1811962265600.00\n","==================================================================\n","Saved best model\n","Epoch 76, Loss 36778514424704.00, Val loss 1811466813440.00\n","Epoch 77, Loss 36763680344832.00, Val loss 1811471269888.00\n","==================================================================\n","Saved best model\n","Epoch 78, Loss 36747485114624.00, Val loss 1811396427776.00\n","==================================================================\n","Saved best model\n","Epoch 79, Loss 36734960621952.00, Val loss 1811026018304.00\n","==================================================================\n","Saved best model\n","Epoch 80, Loss 36733562692480.00, Val loss 1810880397312.00\n","==================================================================\n","Saved best model\n","Epoch 81, Loss 36734324808064.00, Val loss 1810027905024.00\n","Epoch 82, Loss 36702029633792.00, Val loss 1811515703296.00\n","Epoch 83, Loss 36682940100864.00, Val loss 1810056478720.00\n","Epoch 84, Loss 36672029890432.00, Val loss 1810592301056.00\n","==================================================================\n","Saved best model\n","Epoch 85, Loss 36692547226112.00, Val loss 1809650024448.00\n","Epoch 86, Loss 36641522913024.00, Val loss 1809774936064.00\n","==================================================================\n","Saved best model\n","Epoch 87, Loss 36625453737216.00, Val loss 1808797270016.00\n","==================================================================\n","Saved best model\n","Epoch 88, Loss 36610374931584.00, Val loss 1808058548224.00\n","==================================================================\n","Saved best model\n","Epoch 89, Loss 36600488450304.00, Val loss 1807689842688.00\n","==================================================================\n","Saved best model\n","Epoch 90, Loss 36576849088128.00, Val loss 1807094644736.00\n","==================================================================\n","Saved best model\n","Epoch 91, Loss 36562836826624.00, Val loss 1806560133120.00\n","==================================================================\n","Saved best model\n","Epoch 92, Loss 36541124574208.00, Val loss 1806037549056.00\n","==================================================================\n","Saved best model\n","Epoch 93, Loss 36523334003968.00, Val loss 1805461094400.00\n","==================================================================\n","Saved best model\n","Epoch 94, Loss 36501042890240.00, Val loss 1804889227264.00\n","==================================================================\n","Saved best model\n","Epoch 95, Loss 36479903235840.00, Val loss 1804293242880.00\n","==================================================================\n","Saved best model\n","Epoch 96, Loss 36457196103424.00, Val loss 1803674583040.00\n","==================================================================\n","Saved best model\n","Epoch 97, Loss 36434347730048.00, Val loss 1803054481408.00\n","==================================================================\n","Saved best model\n","Epoch 98, Loss 36409634094208.00, Val loss 1802461511680.00\n","==================================================================\n","Saved best model\n","Epoch 99, Loss 36383965340544.00, Val loss 1801808117760.00\n","==================================================================\n","Saved best model\n","Epoch 100, Loss 36358732013056.00, Val loss 1801061138432.00\n","Epoch 101, Loss 36335714394112.00, Val loss 1803921522688.00\n","Epoch 102, Loss 36329988436352.00, Val loss 1805992329216.00\n","Epoch 103, Loss 36487658789888.00, Val loss 1805965066240.00\n","==================================================================\n","Saved best model\n","Epoch 104, Loss 36410369096704.00, Val loss 1800331067392.00\n","Epoch 105, Loss 36273043165824.00, Val loss 1806841151488.00\n","Epoch 106, Loss 36822791482624.00, Val loss 1801432203264.00\n","Epoch 107, Loss 36842223094528.00, Val loss 1802176430080.00\n","Epoch 108, Loss 36391511378304.00, Val loss 1801643098112.00\n","Epoch 109, Loss 36361365994624.00, Val loss 1801285926912.00\n","Epoch 110, Loss 36357224880384.00, Val loss 1800744075264.00\n","==================================================================\n","Saved best model\n","Epoch 111, Loss 36339492832256.00, Val loss 1800124891136.00\n","==================================================================\n","Saved best model\n","Epoch 112, Loss 36321202487040.00, Val loss 1799940079616.00\n","Epoch 113, Loss 36303854481792.00, Val loss 1800160280576.00\n","Epoch 114, Loss 36286257521920.00, Val loss 1800170897408.00\n","==================================================================\n","Saved best model\n","Epoch 115, Loss 36265543505280.00, Val loss 1798757023744.00\n","==================================================================\n","Saved best model\n","Epoch 116, Loss 36245502321664.00, Val loss 1797544345600.00\n","==================================================================\n","Saved best model\n","Epoch 117, Loss 36234422209536.00, Val loss 1796544397312.00\n","==================================================================\n","Saved best model\n","Epoch 118, Loss 36219259354496.00, Val loss 1795700162560.00\n","==================================================================\n","Saved best model\n","Epoch 119, Loss 36203831675136.00, Val loss 1794940862464.00\n","==================================================================\n","Saved best model\n","Epoch 120, Loss 36185382722176.00, Val loss 1794229796864.00\n","==================================================================\n","Saved best model\n","Epoch 121, Loss 36165514492672.00, Val loss 1793536032768.00\n","==================================================================\n","Saved best model\n","Epoch 122, Loss 36144732084096.00, Val loss 1792845938688.00\n","==================================================================\n","Saved best model\n","Epoch 123, Loss 36124285673216.00, Val loss 1792161611776.00\n","==================================================================\n","Saved best model\n","Epoch 124, Loss 36102692491264.00, Val loss 1791497076736.00\n","==================================================================\n","Saved best model\n","Epoch 125, Loss 36082551084288.00, Val loss 1790847877120.00\n","==================================================================\n","Saved best model\n","Epoch 126, Loss 36061663305728.00, Val loss 1790203658240.00\n","==================================================================\n","Saved best model\n","Epoch 127, Loss 36046102718976.00, Val loss 1789555245056.00\n","==================================================================\n","Saved best model\n","Epoch 128, Loss 36028801415936.00, Val loss 1789268197376.00\n","==================================================================\n","Saved best model\n","Epoch 129, Loss 36009735822080.00, Val loss 1788913123328.00\n","==================================================================\n","Saved best model\n","Epoch 130, Loss 35982933306624.00, Val loss 1788607987712.00\n","==================================================================\n","Saved best model\n","Epoch 131, Loss 35962005947392.00, Val loss 1788318580736.00\n","==================================================================\n","Saved best model\n","Epoch 132, Loss 35939103087360.00, Val loss 1787941486592.00\n","==================================================================\n","Saved best model\n","Epoch 133, Loss 35916766945536.00, Val loss 1787652079616.00\n","==================================================================\n","Saved best model\n","Epoch 134, Loss 35896841742336.00, Val loss 1787102101504.00\n","==================================================================\n","Saved best model\n","Epoch 135, Loss 35863368284416.00, Val loss 1786763673600.00\n","==================================================================\n","Saved best model\n","Epoch 136, Loss 35845194773504.00, Val loss 1786702200832.00\n","==================================================================\n","Saved best model\n","Epoch 137, Loss 35830958833152.00, Val loss 1786663534592.00\n","==================================================================\n","Saved best model\n","Epoch 138, Loss 35818379709696.00, Val loss 1786041335808.00\n","==================================================================\n","Saved best model\n","Epoch 139, Loss 35782411674624.00, Val loss 1785338396672.00\n","==================================================================\n","Saved best model\n","Epoch 140, Loss 35752625109504.00, Val loss 1784371216384.00\n","==================================================================\n","Saved best model\n","Epoch 141, Loss 35723133610496.00, Val loss 1784108285952.00\n","==================================================================\n","Saved best model\n","Epoch 142, Loss 35705806919424.00, Val loss 1784019419136.00\n","==================================================================\n","Saved best model\n","Epoch 143, Loss 35692096930304.00, Val loss 1783395385344.00\n","==================================================================\n","Saved best model\n","Epoch 144, Loss 35655859900672.00, Val loss 1782207479808.00\n","==================================================================\n","Saved best model\n","Epoch 145, Loss 35622350051328.00, Val loss 1781144485888.00\n","==================================================================\n","Saved best model\n","Epoch 146, Loss 35586519523840.00, Val loss 1780527923200.00\n","==================================================================\n","Saved best model\n","Epoch 147, Loss 35563720458752.00, Val loss 1779619987456.00\n","==================================================================\n","Saved best model\n","Epoch 148, Loss 35539558335744.00, Val loss 1778504302592.00\n","==================================================================\n","Saved best model\n","Epoch 149, Loss 35508935505408.00, Val loss 1777524932608.00\n","==================================================================\n","Saved best model\n","Epoch 150, Loss 35480715853568.00, Val loss 1776203726848.00\n","==================================================================\n","Saved best model\n","Epoch 151, Loss 35451175666176.00, Val loss 1774877278208.00\n","==================================================================\n","Saved best model\n","Epoch 152, Loss 35411404875520.00, Val loss 1774309474304.00\n","==================================================================\n","Saved best model\n","Epoch 153, Loss 35386432362240.00, Val loss 1773647691776.00\n","==================================================================\n","Saved best model\n","Epoch 154, Loss 35363758355456.00, Val loss 1772867289088.00\n","==================================================================\n","Saved best model\n","Epoch 155, Loss 35339369200128.00, Val loss 1771982422016.00\n","==================================================================\n","Saved best model\n","Epoch 156, Loss 35309956895744.00, Val loss 1771025072128.00\n","==================================================================\n","Saved best model\n","Epoch 157, Loss 35278868821504.00, Val loss 1770232741888.00\n","==================================================================\n","Saved best model\n","Epoch 158, Loss 35250506732544.00, Val loss 1769551429632.00\n","==================================================================\n","Saved best model\n","Epoch 159, Loss 35227396651776.00, Val loss 1768789901312.00\n","==================================================================\n","Saved best model\n","Epoch 160, Loss 35206032270336.00, Val loss 1767483637760.00\n","==================================================================\n","Saved best model\n","Epoch 161, Loss 35166433567488.00, Val loss 1766738231296.00\n","==================================================================\n","Saved best model\n","Epoch 162, Loss 35147235289856.00, Val loss 1765479022592.00\n","==================================================================\n","Saved best model\n","Epoch 163, Loss 35104899379200.00, Val loss 1765160648704.00\n","==================================================================\n","Saved best model\n","Epoch 164, Loss 35095242760704.00, Val loss 1764319428608.00\n","==================================================================\n","Saved best model\n","Epoch 165, Loss 35102645104128.00, Val loss 1762256093184.00\n","==================================================================\n","Saved best model\n","Epoch 166, Loss 35061747079936.00, Val loss 1760431439872.00\n","Epoch 167, Loss 35154087107328.00, Val loss 1760833306624.00\n","==================================================================\n","Saved best model\n","Epoch 168, Loss 34873923362816.00, Val loss 1759187304448.00\n","==================================================================\n","Saved best model\n","Epoch 169, Loss 34998325135360.00, Val loss 1758868406272.00\n","==================================================================\n","Saved best model\n","Epoch 170, Loss 35015373837568.00, Val loss 1756977168384.00\n","==================================================================\n","Saved best model\n","Epoch 171, Loss 34910398840576.00, Val loss 1756244082688.00\n","==================================================================\n","Saved best model\n","Epoch 172, Loss 34927129188864.00, Val loss 1754837549056.00\n","==================================================================\n","Saved best model\n","Epoch 173, Loss 34879881677312.00, Val loss 1753324584960.00\n","==================================================================\n","Saved best model\n","Epoch 174, Loss 34832880579584.00, Val loss 1751968382976.00\n","==================================================================\n","Saved best model\n","Epoch 175, Loss 34785569924096.00, Val loss 1750966730752.00\n","==================================================================\n","Saved best model\n","Epoch 176, Loss 34796174213120.00, Val loss 1749463203840.00\n","==================================================================\n","Saved best model\n","Epoch 177, Loss 34705045718784.00, Val loss 1748216578048.00\n","==================================================================\n","Saved best model\n","Epoch 178, Loss 34703090465536.00, Val loss 1747030638592.00\n","==================================================================\n","Saved best model\n","Epoch 179, Loss 34688008046080.00, Val loss 1745720967168.00\n","==================================================================\n","Saved best model\n","Epoch 180, Loss 34604908428032.00, Val loss 1744422174720.00\n","==================================================================\n","Saved best model\n","Epoch 181, Loss 34594155208192.00, Val loss 1743239118848.00\n","==================================================================\n","Saved best model\n","Epoch 182, Loss 34571605919488.00, Val loss 1742088437760.00\n","==================================================================\n","Saved best model\n","Epoch 183, Loss 34517668414976.00, Val loss 1740822806528.00\n","==================================================================\n","Saved best model\n","Epoch 184, Loss 34487695517440.00, Val loss 1739599773696.00\n","==================================================================\n","Saved best model\n","Epoch 185, Loss 34470473383680.00, Val loss 1738441883648.00\n","==================================================================\n","Saved best model\n","Epoch 186, Loss 34449613255168.00, Val loss 1737373646848.00\n","==================================================================\n","Saved best model\n","Epoch 187, Loss 34385518356224.00, Val loss 1736176697344.00\n","==================================================================\n","Saved best model\n","Epoch 188, Loss 34366758794240.00, Val loss 1734968737792.00\n","==================================================================\n","Saved best model\n","Epoch 189, Loss 34328676110592.00, Val loss 1733851086848.00\n","==================================================================\n","Saved best model\n","Epoch 190, Loss 34302159674880.00, Val loss 1732716396544.00\n","==================================================================\n","Saved best model\n","Epoch 191, Loss 34266133192704.00, Val loss 1731732307968.00\n","==================================================================\n","Saved best model\n","Epoch 192, Loss 34226315287808.00, Val loss 1730189066240.00\n","==================================================================\n","Saved best model\n","Epoch 193, Loss 34211093489664.00, Val loss 1728995393536.00\n","==================================================================\n","Saved best model\n","Epoch 194, Loss 34205942146816.00, Val loss 1727953108992.00\n","==================================================================\n","Saved best model\n","Epoch 195, Loss 34162658655488.00, Val loss 1726422712320.00\n","==================================================================\n","Saved best model\n","Epoch 196, Loss 34158680642048.00, Val loss 1725407559680.00\n","==================================================================\n","Saved best model\n","Epoch 197, Loss 34118695860736.00, Val loss 1724255174656.00\n","==================================================================\n","Saved best model\n","Epoch 198, Loss 34086512157696.00, Val loss 1723066089472.00\n","==================================================================\n","Saved best model\n","Epoch 199, Loss 34056190355456.00, Val loss 1721701629952.00\n","==================================================================\n","Saved best model\n","Epoch 200, Loss 34026417210368.00, Val loss 1720079351808.00\n","==================================================================\n","Saved best model\n","Epoch 201, Loss 34039493636096.00, Val loss 1718882271232.00\n","==================================================================\n","Saved best model\n","Epoch 202, Loss 34020775799552.00, Val loss 1717617426432.00\n","==================================================================\n","Saved best model\n","Epoch 203, Loss 33994699453696.00, Val loss 1716255719424.00\n","==================================================================\n","Saved best model\n","Epoch 204, Loss 33985552543232.00, Val loss 1714973573120.00\n","==================================================================\n","Saved best model\n","Epoch 205, Loss 33963747936512.00, Val loss 1713828921344.00\n","==================================================================\n","Saved best model\n","Epoch 206, Loss 33930277625856.00, Val loss 1712625942528.00\n","==================================================================\n","Saved best model\n","Epoch 207, Loss 33908588839424.00, Val loss 1711206563840.00\n","==================================================================\n","Saved best model\n","Epoch 208, Loss 33892025332992.00, Val loss 1710796308480.00\n","==================================================================\n","Saved best model\n","Epoch 209, Loss 33825866055424.00, Val loss 1709753892864.00\n","==================================================================\n","Saved best model\n","Epoch 210, Loss 33789306745600.00, Val loss 1707864883200.00\n","==================================================================\n","Saved best model\n","Epoch 211, Loss 33793757323520.00, Val loss 1706397401088.00\n","==================================================================\n","Saved best model\n","Epoch 212, Loss 33801400738048.00, Val loss 1704707489792.00\n","==================================================================\n","Saved best model\n","Epoch 213, Loss 33796793103360.00, Val loss 1704151613440.00\n","==================================================================\n","Saved best model\n","Epoch 214, Loss 33731993606912.00, Val loss 1703928135680.00\n","==================================================================\n","Saved best model\n","Epoch 215, Loss 33689644696320.00, Val loss 1703035011072.00\n","Epoch 216, Loss 33656242637568.00, Val loss 1703457456128.00\n","==================================================================\n","Saved best model\n","Epoch 217, Loss 33578281297408.00, Val loss 1700690264064.00\n","==================================================================\n","Saved best model\n","Epoch 218, Loss 33598697167872.00, Val loss 1699450060800.00\n","Epoch 219, Loss 33605431198464.00, Val loss 1699689398272.00\n","==================================================================\n","Saved best model\n","Epoch 220, Loss 33542985368832.00, Val loss 1698325463040.00\n","==================================================================\n","Saved best model\n","Epoch 221, Loss 33513621395968.00, Val loss 1698078654464.00\n","==================================================================\n","Saved best model\n","Epoch 222, Loss 33466000429824.00, Val loss 1696594526208.00\n","==================================================================\n","Saved best model\n","Epoch 223, Loss 33457096885504.00, Val loss 1695686066176.00\n","==================================================================\n","Saved best model\n","Epoch 224, Loss 33416432668672.00, Val loss 1693823139840.00\n","==================================================================\n","Saved best model\n","Epoch 225, Loss 33399955680768.00, Val loss 1692565372928.00\n","==================================================================\n","Saved best model\n","Epoch 226, Loss 33419469279488.00, Val loss 1691125284864.00\n","Epoch 227, Loss 33431667294208.00, Val loss 1691652063232.00\n","==================================================================\n","Saved best model\n","Epoch 228, Loss 33339090763008.00, Val loss 1690145128448.00\n","==================================================================\n","Saved best model\n","Epoch 229, Loss 33356992612352.00, Val loss 1688797446144.00\n","==================================================================\n","Saved best model\n","Epoch 230, Loss 33318019927552.00, Val loss 1688223088640.00\n","==================================================================\n","Saved best model\n","Epoch 231, Loss 33283249840128.00, Val loss 1683608961024.00\n","==================================================================\n","Saved best model\n","Epoch 232, Loss 33401370198528.00, Val loss 1682195218432.00\n","Epoch 233, Loss 8836817145291008.00, Val loss 1818551648256.00\n","Epoch 234, Loss 36802598978432.00, Val loss 1819021541376.00\n","Epoch 235, Loss 36802450660096.00, Val loss 1819531542528.00\n","Epoch 236, Loss 36801262941696.00, Val loss 1819727101952.00\n","Epoch 237, Loss 36798565965184.00, Val loss 1819557363712.00\n","Epoch 238, Loss 36796006916352.00, Val loss 1819520532480.00\n","Epoch 239, Loss 36794911511424.00, Val loss 1819874295808.00\n","Epoch 240, Loss 36794365112832.00, Val loss 1820230025216.00\n","Epoch 241, Loss 36792788677120.00, Val loss 1820259123200.00\n","Epoch 242, Loss 36790698695552.00, Val loss 1820320989184.00\n","Epoch 243, Loss 36789405695616.00, Val loss 1820599517184.00\n","Epoch 244, Loss 36788047559808.00, Val loss 1820547350528.00\n","Epoch 245, Loss 36785348376192.00, Val loss 1819974696960.00\n","Epoch 246, Loss 36782515343744.00, Val loss 1819451326464.00\n","Epoch 247, Loss 36780838063744.00, Val loss 1819163623424.00\n","Epoch 248, Loss 36779769811328.00, Val loss 1818849705984.00\n","Epoch 249, Loss 36778849600768.00, Val loss 1818635534336.00\n","Epoch 250, Loss 36778316199936.00, Val loss 1818324762624.00\n","Epoch 251, Loss 36777624524416.00, Val loss 1817950552064.00\n","Epoch 252, Loss 36777285202304.00, Val loss 1817894453248.00\n","Epoch 253, Loss 36777484003968.00, Val loss 1817725894656.00\n","Epoch 254, Loss 36777381204352.00, Val loss 1817589710848.00\n","Epoch 255, Loss 36777593726848.00, Val loss 1817839927296.00\n","Epoch 256, Loss 36778222035968.00, Val loss 1818054623232.00\n","Epoch 257, Loss 36778351762944.00, Val loss 1817986727936.00\n","Epoch 258, Loss 36777963120768.00, Val loss 1817807683584.00\n","Epoch 259, Loss 36777601445760.00, Val loss 1817724977152.00\n","Epoch 260, Loss 36777509936640.00, Val loss 1817776357376.00\n","Epoch 261, Loss 36777583738240.00, Val loss 1817878593536.00\n","Epoch 262, Loss 36777632049920.00, Val loss 1817964445696.00\n","Epoch 263, Loss 36777580513024.00, Val loss 1818018316288.00\n","Epoch 264, Loss 36777441139712.00, Val loss 1818049380352.00\n","Epoch 265, Loss 36777249574528.00, Val loss 1818065502208.00\n","Epoch 266, Loss 36777021748224.00, Val loss 1818073628672.00\n","Epoch 267, Loss 36776769601408.00, Val loss 1818075856896.00\n","Epoch 268, Loss 36776498017152.00, Val loss 1818075201536.00\n","Epoch 269, Loss 36776204701312.00, Val loss 1818072449024.00\n","Epoch 270, Loss 36775892268928.00, Val loss 1818067861504.00\n","Epoch 271, Loss 36775561252352.00, Val loss 1818063142912.00\n","Epoch 272, Loss 36775205359360.00, Val loss 1818057244672.00\n","Epoch 273, Loss 36774837201664.00, Val loss 1818050560000.00\n","Epoch 274, Loss 36774437794944.00, Val loss 1818043088896.00\n","Epoch 275, Loss 36774017925376.00, Val loss 1818034831360.00\n","Epoch 276, Loss 36773573545728.00, Val loss 1818026573824.00\n","Epoch 277, Loss 36773102713216.00, Val loss 1818017792000.00\n","Epoch 278, Loss 36772605389696.00, Val loss 1818008485888.00\n","Epoch 279, Loss 36772078404864.00, Val loss 1817998000128.00\n","Epoch 280, Loss 36771522326656.00, Val loss 1817987514368.00\n","Epoch 281, Loss 36770935021824.00, Val loss 1817975980032.00\n","Epoch 282, Loss 36770315152000.00, Val loss 1817964183552.00\n","Epoch 283, Loss 36769661771008.00, Val loss 1817951600640.00\n","Epoch 284, Loss 36768973049088.00, Val loss 1817938493440.00\n","Epoch 285, Loss 36768248652160.00, Val loss 1817924468736.00\n","Epoch 286, Loss 36767487043328.00, Val loss 1817909395456.00\n","Epoch 287, Loss 36766686360576.00, Val loss 1817893797888.00\n","Epoch 288, Loss 36765845115904.00, Val loss 1817877413888.00\n","Epoch 289, Loss 36764965304832.00, Val loss 1817860505600.00\n","Epoch 290, Loss 36764044232576.00, Val loss 1817842155520.00\n","Epoch 291, Loss 36763079004032.00, Val loss 1817823412224.00\n","Epoch 292, Loss 36762070613504.00, Val loss 1817803227136.00\n","Epoch 293, Loss 36761017906048.00, Val loss 1817781993472.00\n","Epoch 294, Loss 36759921773952.00, Val loss 1817759842304.00\n","Epoch 295, Loss 36758778276992.00, Val loss 1817737035776.00\n","Epoch 296, Loss 36757584148224.00, Val loss 1817712132096.00\n","Epoch 297, Loss 36756341677568.00, Val loss 1817686310912.00\n","Epoch 298, Loss 36755053794688.00, Val loss 1817659047936.00\n","Epoch 299, Loss 36753715427712.00, Val loss 1817630212096.00\n","Epoch 300, Loss 36752327763712.00, Val loss 1817599148032.00\n","Epoch 301, Loss 36750887523840.00, Val loss 1817566511104.00\n","Epoch 302, Loss 36749388530304.00, Val loss 1817531777024.00\n","Epoch 303, Loss 36747826951424.00, Val loss 1817493897216.00\n","Epoch 304, Loss 36746211108480.00, Val loss 1817453395968.00\n","Epoch 305, Loss 36744533957120.00, Val loss 1817410142208.00\n","Epoch 306, Loss 36742796156928.00, Val loss 1817362563072.00\n","Epoch 307, Loss 36740994839680.00, Val loss 1817310920704.00\n","Epoch 308, Loss 36739123041152.00, Val loss 1817254821888.00\n","Epoch 309, Loss 36737176833664.00, Val loss 1817193086976.00\n","Epoch 310, Loss 36735156893952.00, Val loss 1817125060608.00\n","Epoch 311, Loss 36733054001792.00, Val loss 1817049956352.00\n","Epoch 312, Loss 36730866430720.00, Val loss 1816966856704.00\n","Epoch 313, Loss 36728590335744.00, Val loss 1816874713088.00\n","Epoch 314, Loss 36726216396160.00, Val loss 1816772608000.00\n","Epoch 315, Loss 36723747882368.00, Val loss 1816659623936.00\n","Epoch 316, Loss 36721182935040.00, Val loss 1816535236608.00\n","Epoch 317, Loss 36718511419136.00, Val loss 1816399708160.00\n","Epoch 318, Loss 36715730331904.00, Val loss 1816251727872.00\n","Epoch 319, Loss 36712837579776.00, Val loss 1816091820032.00\n","Epoch 320, Loss 36709833831424.00, Val loss 1815922212864.00\n","Epoch 321, Loss 36706714208768.00, Val loss 1815744086016.00\n","Epoch 322, Loss 36703463925248.00, Val loss 1815558225920.00\n","Epoch 323, Loss 36700096549632.00, Val loss 1815368040448.00\n","Epoch 324, Loss 36696614300288.00, Val loss 1815177068544.00\n","Epoch 325, Loss 36692998615680.00, Val loss 1814987407360.00\n","Epoch 326, Loss 36689253109888.00, Val loss 1814801285120.00\n","Epoch 327, Loss 36685358406016.00, Val loss 1814618832896.00\n","Epoch 328, Loss 36681324089984.00, Val loss 1814441230336.00\n","Epoch 329, Loss 36677133729024.00, Val loss 1814269001728.00\n","Epoch 330, Loss 36672785654016.00, Val loss 1814101491712.00\n","Epoch 331, Loss 36668266100480.00, Val loss 1813936996352.00\n","Epoch 332, Loss 36663576756992.00, Val loss 1813776039936.00\n","Epoch 333, Loss 36658710519808.00, Val loss 1813620719616.00\n","Epoch 334, Loss 36653668220672.00, Val loss 1813471428608.00\n","Epoch 335, Loss 36648425023360.00, Val loss 1813326725120.00\n","Epoch 336, Loss 36643002589312.00, Val loss 1813191327744.00\n","Epoch 337, Loss 36637372651264.00, Val loss 1813065760768.00\n","Epoch 338, Loss 36631571011712.00, Val loss 1812960903168.00\n","Epoch 339, Loss 36625540976640.00, Val loss 1812874395648.00\n","Epoch 340, Loss 36619311384064.00, Val loss 1812814364672.00\n","Epoch 341, Loss 36612851968896.00, Val loss 1812775043072.00\n","Epoch 342, Loss 36606180034304.00, Val loss 1812746731520.00\n","Epoch 343, Loss 36599289811456.00, Val loss 1812704919552.00\n","Epoch 344, Loss 36592203896064.00, Val loss 1812615004160.00\n","Epoch 345, Loss 36584907759872.00, Val loss 1812447756288.00\n","Epoch 346, Loss 36577424678400.00, Val loss 1812202782720.00\n","Epoch 347, Loss 36569713576704.00, Val loss 1811906822144.00\n","Epoch 348, Loss 36561833611520.00, Val loss 1811590676480.00\n","Epoch 349, Loss 36553763862272.00, Val loss 1811267715072.00\n","Epoch 350, Loss 36545534699264.00, Val loss 1810924437504.00\n","Epoch 351, Loss 36537245063424.00, Val loss 1810518638592.00\n","Epoch 352, Loss 36528882021376.00, Val loss 1809988190208.00\n","Epoch 353, Loss 36520493825536.00, Val loss 1809312120832.00\n","Epoch 354, Loss 36512238210816.00, Val loss 1808566976512.00\n","Epoch 355, Loss 36504156512512.00, Val loss 1807873867776.00\n","Epoch 356, Loss 36496140933888.00, Val loss 1807295053824.00\n","Epoch 357, Loss 36487859617280.00, Val loss 1806831452160.00\n","Epoch 358, Loss 36479152903680.00, Val loss 1806461173760.00\n","Epoch 359, Loss 36470006940928.00, Val loss 1806140440576.00\n","Epoch 360, Loss 36460517105152.00, Val loss 1805851033600.00\n","Epoch 361, Loss 36450756716032.00, Val loss 1805578141696.00\n","Epoch 362, Loss 36440675457024.00, Val loss 1805314293760.00\n","Epoch 363, Loss 36430516170752.00, Val loss 1805051494400.00\n","Epoch 364, Loss 36420072259328.00, Val loss 1804782927872.00\n","Epoch 365, Loss 36409525810944.00, Val loss 1804515672064.00\n","Epoch 366, Loss 36398827116544.00, Val loss 1804238848000.00\n","Epoch 367, Loss 36388055012096.00, Val loss 1803961499648.00\n","Epoch 368, Loss 36377079631616.00, Val loss 1803679825920.00\n","Epoch 369, Loss 36365931020032.00, Val loss 1803396186112.00\n","Epoch 370, Loss 36354595540992.00, Val loss 1803106385920.00\n","Epoch 371, Loss 36343283401984.00, Val loss 1802820255744.00\n","Epoch 372, Loss 36331660770048.00, Val loss 1802523770880.00\n","Epoch 373, Loss 36319966746368.00, Val loss 1802221256704.00\n","Epoch 374, Loss 36308169024512.00, Val loss 1801917562880.00\n","Epoch 375, Loss 36296083746816.00, Val loss 1801607708672.00\n","Epoch 376, Loss 36283921332224.00, Val loss 1801291563008.00\n","Epoch 377, Loss 36271542735104.00, Val loss 1800968863744.00\n","Epoch 378, Loss 36259062763776.00, Val loss 1800630960128.00\n","Epoch 379, Loss 36246502513408.00, Val loss 1800295809024.00\n","Epoch 380, Loss 36233675767040.00, Val loss 1799947550720.00\n","Epoch 381, Loss 36220790696192.00, Val loss 1799600996352.00\n","Epoch 382, Loss 36207829884672.00, Val loss 1799256932352.00\n","Epoch 383, Loss 36194597364736.00, Val loss 1798903037952.00\n","Epoch 384, Loss 36181321705984.00, Val loss 1798544031744.00\n","Epoch 385, Loss 36167774354944.00, Val loss 1798180306944.00\n","Epoch 386, Loss 36151407933440.00, Val loss 1797755633664.00\n","Epoch 387, Loss 36140488774912.00, Val loss 1797426380800.00\n","Epoch 388, Loss 36126198393856.00, Val loss 1797064753152.00\n","Epoch 389, Loss 36111963855616.00, Val loss 1796688707584.00\n","Epoch 390, Loss 36097752536576.00, Val loss 1796300341248.00\n","Epoch 391, Loss 36083495935744.00, Val loss 1795908698112.00\n","Epoch 392, Loss 36068909736960.00, Val loss 1795508011008.00\n","Epoch 393, Loss 36054446031360.00, Val loss 1795088318464.00\n","Epoch 394, Loss 36039727285504.00, Val loss 1794665218048.00\n","Epoch 395, Loss 36024825054976.00, Val loss 1794251554816.00\n","Epoch 396, Loss 36010070926592.00, Val loss 1793836056576.00\n","Epoch 397, Loss 35994953187584.00, Val loss 1793429340160.00\n","Epoch 398, Loss 35979454934016.00, Val loss 1792956956672.00\n","Epoch 399, Loss 35964245244672.00, Val loss 1792531496960.00\n","Epoch 400, Loss 35948699713024.00, Val loss 1792092012544.00\n","Epoch 401, Loss 35933139680000.00, Val loss 1791605342208.00\n","Epoch 402, Loss 35917993192192.00, Val loss 1791221825536.00\n","Epoch 403, Loss 35901599296256.00, Val loss 1790703173632.00\n","Epoch 404, Loss 35885631737600.00, Val loss 1790304845824.00\n","Epoch 405, Loss 35869469959936.00, Val loss 1789881483264.00\n","Epoch 406, Loss 35852711203072.00, Val loss 1789443964928.00\n","Epoch 407, Loss 35836354507264.00, Val loss 1788997271552.00\n","Epoch 408, Loss 35819983537664.00, Val loss 1788543762432.00\n","Epoch 409, Loss 35803281178368.00, Val loss 1787979890688.00\n","Epoch 410, Loss 35787091059200.00, Val loss 1787497545728.00\n","Epoch 411, Loss 35770367029504.00, Val loss 1787003404288.00\n","Epoch 412, Loss 35753670727168.00, Val loss 1786480295936.00\n","Epoch 413, Loss 35737587706880.00, Val loss 1785987858432.00\n","Epoch 414, Loss 35721992804096.00, Val loss 1785652969472.00\n","Epoch 415, Loss 35704168270848.00, Val loss 1785169313792.00\n","Epoch 416, Loss 35686949352704.00, Val loss 1784666128384.00\n","Epoch 417, Loss 35671061146880.00, Val loss 1784167923712.00\n","Epoch 418, Loss 35655209781504.00, Val loss 1783673782272.00\n","Epoch 419, Loss 35637953082368.00, Val loss 1783175839744.00\n","Epoch 420, Loss 35621058962176.00, Val loss 1782672523264.00\n","Epoch 421, Loss 35604354529792.00, Val loss 1782220587008.00\n","Epoch 422, Loss 35588343634432.00, Val loss 1781770616832.00\n","Epoch 423, Loss 35572443491584.00, Val loss 1781220376576.00\n","Epoch 424, Loss 35556584339712.00, Val loss 1780723613696.00\n","Epoch 425, Loss 35538467749888.00, Val loss 1780129071104.00\n","Epoch 426, Loss 35524463589632.00, Val loss 1779620773888.00\n","Epoch 427, Loss 35507763337984.00, Val loss 1779115753472.00\n","Epoch 428, Loss 35492293355264.00, Val loss 1778640224256.00\n","Epoch 429, Loss 35476804734976.00, Val loss 1778162466816.00\n","Epoch 430, Loss 35462733364224.00, Val loss 1777498587136.00\n","Epoch 431, Loss 35446411729920.00, Val loss 1776972464128.00\n","Epoch 432, Loss 35434875394560.00, Val loss 1776503226368.00\n","Epoch 433, Loss 35419622659072.00, Val loss 1776044343296.00\n","Epoch 434, Loss 35404010654208.00, Val loss 1775556886528.00\n","Epoch 435, Loss 35390287144704.00, Val loss 1775111766016.00\n","Epoch 436, Loss 35372636005376.00, Val loss 1774693777408.00\n","Epoch 437, Loss 35356935511040.00, Val loss 1773960167424.00\n","Epoch 438, Loss 35349795784960.00, Val loss 1773511114752.00\n","Epoch 439, Loss 35330952046336.00, Val loss 1773083426816.00\n","Epoch 440, Loss 35312163370240.00, Val loss 1772355452928.00\n","Epoch 441, Loss 35295855201280.00, Val loss 1772077711360.00\n","Epoch 442, Loss 35277338756096.00, Val loss 1771996839936.00\n","Epoch 443, Loss 35255614852608.00, Val loss 1771534417920.00\n","Epoch 444, Loss 35236019729920.00, Val loss 1771014586368.00\n","Epoch 445, Loss 35220272298496.00, Val loss 1770138763264.00\n","Epoch 446, Loss 35207729107456.00, Val loss 1769390211072.00\n","Epoch 447, Loss 35198011211264.00, Val loss 1768778366976.00\n","Epoch 448, Loss 35181054188544.00, Val loss 1768252899328.00\n","Epoch 449, Loss 35161079833600.00, Val loss 1767680638976.00\n","Epoch 450, Loss 35143187657984.00, Val loss 1767222280192.00\n","Epoch 451, Loss 35124287646720.00, Val loss 1766667059200.00\n","Epoch 452, Loss 35107063763200.00, Val loss 1766078545920.00\n","Epoch 453, Loss 35092329900544.00, Val loss 1765232214016.00\n","Epoch 454, Loss 35086387680000.00, Val loss 1764354686976.00\n","Epoch 455, Loss 35069572364544.00, Val loss 1763693953024.00\n","Epoch 456, Loss 35060097648640.00, Val loss 1762771468288.00\n","Epoch 457, Loss 35045874536448.00, Val loss 1762262253568.00\n","Epoch 458, Loss 35025117035264.00, Val loss 1761783185408.00\n","Epoch 459, Loss 35003083904256.00, Val loss 1761097285632.00\n","Epoch 460, Loss 35001585932800.00, Val loss 1759843581952.00\n","Epoch 461, Loss 35001770619904.00, Val loss 1758100455424.00\n","Epoch 462, Loss 34995807717120.00, Val loss 1757905813504.00\n","Epoch 463, Loss 34968444158976.00, Val loss 1757914071040.00\n","Epoch 464, Loss 34936403961344.00, Val loss 1757357408256.00\n","Epoch 465, Loss 34917103628288.00, Val loss 1756720791552.00\n","Epoch 466, Loss 34903779030528.00, Val loss 1756262694912.00\n","Epoch 467, Loss 34880365420544.00, Val loss 1755132854272.00\n","Epoch 468, Loss 34881331551232.00, Val loss 1754258079744.00\n","Epoch 469, Loss 34873035259904.00, Val loss 1753437569024.00\n","Epoch 470, Loss 34857828363776.00, Val loss 1752731353088.00\n","Epoch 471, Loss 34847964460288.00, Val loss 1752014258176.00\n","Epoch 472, Loss 34838172325632.00, Val loss 1751338844160.00\n","Epoch 473, Loss 34828102361600.00, Val loss 1750428024832.00\n","Epoch 474, Loss 34813462713344.00, Val loss 1750016458752.00\n","Epoch 475, Loss 34785641151232.00, Val loss 1749336719360.00\n","Epoch 476, Loss 34768009020416.00, Val loss 1748637319168.00\n","Epoch 477, Loss 34779513860096.00, Val loss 1748054048768.00\n","Epoch 478, Loss 34756888847616.00, Val loss 1747471171584.00\n","Epoch 479, Loss 34732852947968.00, Val loss 1746802573312.00\n","Epoch 480, Loss 34713847443968.00, Val loss 1746271731712.00\n","Epoch 481, Loss 34693236344320.00, Val loss 1745531305984.00\n","Epoch 482, Loss 34685718672384.00, Val loss 1744869916672.00\n","Epoch 483, Loss 34671124411136.00, Val loss 1744543285248.00\n","Epoch 484, Loss 34655207966208.00, Val loss 1743895396352.00\n","Epoch 485, Loss 34636319499520.00, Val loss 1743206744064.00\n","Epoch 486, Loss 34620876651264.00, Val loss 1742557020160.00\n","Epoch 487, Loss 34613291778816.00, Val loss 1741691944960.00\n","Epoch 488, Loss 34597680750848.00, Val loss 1741057032192.00\n","Epoch 489, Loss 34579075766528.00, Val loss 1740450955264.00\n","Epoch 490, Loss 34562458758144.00, Val loss 1739847761920.00\n","Epoch 491, Loss 34543100923648.00, Val loss 1739076796416.00\n","Epoch 492, Loss 34525960497152.00, Val loss 1738354458624.00\n","Epoch 493, Loss 34512577905920.00, Val loss 1737637363712.00\n","Epoch 494, Loss 34533155399168.00, Val loss 1737303785472.00\n","Epoch 495, Loss 34508311106304.00, Val loss 1737319776256.00\n","Epoch 496, Loss 34435665691392.00, Val loss 1736258879488.00\n","Epoch 497, Loss 34461011464448.00, Val loss 1735612956672.00\n","Epoch 498, Loss 34445440417536.00, Val loss 1735024181248.00\n","Epoch 499, Loss 34428436483072.00, Val loss 1734245875712.00\n","Epoch 500, Loss 34410480264192.00, Val loss 1733594710016.00\n","Epoch 501, Loss 34402985424384.00, Val loss 1733103452160.00\n","Epoch 502, Loss 34382452455936.00, Val loss 1732390420480.00\n","Epoch 503, Loss 34370734725120.00, Val loss 1731755114496.00\n","Epoch 504, Loss 34363826227456.00, Val loss 1731146809344.00\n","Epoch 505, Loss 34347601063168.00, Val loss 1730517401600.00\n","Epoch 506, Loss 34330135552000.00, Val loss 1729868333056.00\n","Epoch 507, Loss 34315775970048.00, Val loss 1729260027904.00\n","Epoch 508, Loss 34300786754304.00, Val loss 1728927367168.00\n","Epoch 509, Loss 34280084427008.00, Val loss 1728127959040.00\n","Epoch 510, Loss 34301096171520.00, Val loss 1727608913920.00\n","Epoch 511, Loss 34286031143424.00, Val loss 1727177949184.00\n","Epoch 512, Loss 34265738876416.00, Val loss 1726789189632.00\n","Epoch 513, Loss 34244024651008.00, Val loss 1726332141568.00\n","Epoch 514, Loss 34222032480768.00, Val loss 1725673504768.00\n","Epoch 515, Loss 34205407194624.00, Val loss 1724881043456.00\n","Epoch 516, Loss 34198770000896.00, Val loss 1724172337152.00\n","Epoch 517, Loss 34212195418112.00, Val loss 1723770732544.00\n","Epoch 518, Loss 34169106577664.00, Val loss 1723161509888.00\n","Epoch 519, Loss 34190660715776.00, Val loss 1722849558528.00\n","Epoch 520, Loss 34171447248384.00, Val loss 1722434453504.00\n","Epoch 521, Loss 34197814516224.00, Val loss 1721785253888.00\n","Epoch 522, Loss 34085125173760.00, Val loss 1720392351744.00\n","Epoch 523, Loss 34137773981696.00, Val loss 1720404934656.00\n","Epoch 524, Loss 34113225083392.00, Val loss 1719916167168.00\n","Epoch 525, Loss 34096989071872.00, Val loss 1719128948736.00\n","Epoch 526, Loss 34091807314688.00, Val loss 1718663380992.00\n","Epoch 527, Loss 34082071451648.00, Val loss 1718321807360.00\n","Epoch 528, Loss 34067286731008.00, Val loss 1717975384064.00\n","Epoch 529, Loss 34045596929536.00, Val loss 1717458305024.00\n","Epoch 530, Loss 34032621292288.00, Val loss 1716692582400.00\n","Epoch 531, Loss 34027443036672.00, Val loss 1716607909888.00\n","Epoch 532, Loss 34002556814080.00, Val loss 1715970768896.00\n","Epoch 533, Loss 33990494991104.00, Val loss 1715327729664.00\n","Epoch 534, Loss 33979521084160.00, Val loss 1714368937984.00\n","Epoch 535, Loss 34027712771328.00, Val loss 1714966364160.00\n","Epoch 536, Loss 33931218070784.00, Val loss 1713804148736.00\n","Epoch 537, Loss 33976343361536.00, Val loss 1713963532288.00\n","Epoch 538, Loss 33907772206848.00, Val loss 1712370876416.00\n","Epoch 539, Loss 34016436179200.00, Val loss 1714039685120.00\n","Epoch 540, Loss 33857877223680.00, Val loss 1711524544512.00\n","Epoch 541, Loss 33957906564864.00, Val loss 1712223813632.00\n","Epoch 542, Loss 33848082212352.00, Val loss 1710395359232.00\n","Epoch 543, Loss 33938938912512.00, Val loss 1710607302656.00\n","Epoch 544, Loss 33891443182848.00, Val loss 1710618050560.00\n","Epoch 545, Loss 33864530227456.00, Val loss 1710122598400.00\n","Epoch 546, Loss 33853461165312.00, Val loss 1709592281088.00\n","Epoch 547, Loss 33845234091520.00, Val loss 1709189627904.00\n","Epoch 548, Loss 33833047660544.00, Val loss 1708298600448.00\n","Epoch 549, Loss 33839387373312.00, Val loss 1708079710208.00\n","Epoch 550, Loss 33820044370944.00, Val loss 1708056248320.00\n","Epoch 551, Loss 33779386032896.00, Val loss 1707463278592.00\n","Epoch 552, Loss 33776133514240.00, Val loss 1706755358720.00\n","Epoch 553, Loss 33781929792256.00, Val loss 1706405658624.00\n","Epoch 554, Loss 33763957034496.00, Val loss 1705902866432.00\n","Epoch 555, Loss 33758654217728.00, Val loss 1705483304960.00\n","Epoch 556, Loss 33748987613440.00, Val loss 1705092055040.00\n","Epoch 557, Loss 33743510823936.00, Val loss 1704887058432.00\n","Epoch 558, Loss 33722717817088.00, Val loss 1704989294592.00\n","Epoch 559, Loss 33697153753088.00, Val loss 1704575107072.00\n","Epoch 560, Loss 33674473253376.00, Val loss 1703466106880.00\n","Epoch 561, Loss 33698764367872.00, Val loss 1703111294976.00\n","Epoch 562, Loss 33685006723072.00, Val loss 1702680723456.00\n","Epoch 563, Loss 33685508899840.00, Val loss 1702667485184.00\n","Epoch 564, Loss 33655067007488.00, Val loss 1702057476096.00\n","Epoch 565, Loss 33662034738432.00, Val loss 1701938462720.00\n","Epoch 566, Loss 33629764375808.00, Val loss 1700767465472.00\n","Epoch 567, Loss 33700320123136.00, Val loss 1701772263424.00\n","Epoch 568, Loss 33582698515456.00, Val loss 1700283940864.00\n","Epoch 569, Loss 33677575654656.00, Val loss 1701550358528.00\n","Epoch 570, Loss 33543133359616.00, Val loss 1699813523456.00\n","Epoch 571, Loss 33649357028352.00, Val loss 1701166055424.00\n","Epoch 572, Loss 33502711361024.00, Val loss 1698929836032.00\n","Epoch 573, Loss 33636056270336.00, Val loss 1700432445440.00\n","Epoch 574, Loss 33507616512256.00, Val loss 1699364732928.00\n","Epoch 575, Loss 33595692892416.00, Val loss 1700282105856.00\n","Epoch 576, Loss 33489069147136.00, Val loss 1698586558464.00\n","Epoch 577, Loss 33588945736448.00, Val loss 1699789799424.00\n","Epoch 578, Loss 33467297831680.00, Val loss 1697874706432.00\n","Epoch 579, Loss 33566501684992.00, Val loss 1697440595968.00\n","Epoch 580, Loss 33544967757056.00, Val loss 1697486995456.00\n","Epoch 581, Loss 33471069262336.00, Val loss 1696483901440.00\n","Epoch 582, Loss 33535036639232.00, Val loss 1697274003456.00\n","Epoch 583, Loss 33435343292416.00, Val loss 1696213368832.00\n","Epoch 584, Loss 33526752861440.00, Val loss 1697085390848.00\n","Epoch 585, Loss 33412947440896.00, Val loss 1695745703936.00\n","Epoch 586, Loss 33505256114176.00, Val loss 1696395558912.00\n","Epoch 587, Loss 33396402921216.00, Val loss 1695208964096.00\n","Epoch 588, Loss 33484957102080.00, Val loss 1695720144896.00\n","Epoch 589, Loss 33379900150016.00, Val loss 1694511005696.00\n","Epoch 590, Loss 33466245555200.00, Val loss 1695027953664.00\n","Epoch 591, Loss 33376242244864.00, Val loss 1693943332864.00\n","Epoch 592, Loss 33460208831232.00, Val loss 1693320478720.00\n","Epoch 593, Loss 33414849274880.00, Val loss 1692993191936.00\n","Epoch 594, Loss 33426385841408.00, Val loss 1692699066368.00\n","Epoch 595, Loss 33395416064000.00, Val loss 1692467593216.00\n","Epoch 596, Loss 33389772925184.00, Val loss 1692115009536.00\n","Epoch 597, Loss 33389360157952.00, Val loss 1690813988864.00\n","Epoch 598, Loss 33397158439936.00, Val loss 1690338721792.00\n","Epoch 599, Loss 33387878305024.00, Val loss 1689744572416.00\n","Epoch 600, Loss 33382793785344.00, Val loss 1689256722432.00\n","Epoch 601, Loss 33374758310656.00, Val loss 1688804655104.00\n","Epoch 602, Loss 33371446924288.00, Val loss 1688314445824.00\n","Epoch 603, Loss 33388108341248.00, Val loss 1687443472384.00\n","Epoch 604, Loss 33382265198848.00, Val loss 1686380347392.00\n","Epoch 605, Loss 33385571421440.00, Val loss 1685406220288.00\n","Epoch 606, Loss 33425705920000.00, Val loss 1683155320832.00\n","==================================================================\n","Saved best model\n","Epoch 607, Loss 33524226631680.00, Val loss 1681512333312.00\n","Epoch 608, Loss 33579542210816.00, Val loss 1681714053120.00\n","==================================================================\n","Saved best model\n","Epoch 609, Loss 33557325250816.00, Val loss 1681258184704.00\n","==================================================================\n","Saved best model\n","Epoch 610, Loss 33511064377344.00, Val loss 1680424566784.00\n","==================================================================\n","Saved best model\n","Epoch 611, Loss 33507256623360.00, Val loss 1679406923776.00\n","==================================================================\n","Saved best model\n","Epoch 612, Loss 33507454186240.00, Val loss 1677820559360.00\n","==================================================================\n","Saved best model\n","Epoch 613, Loss 33537678142976.00, Val loss 1676337479680.00\n","==================================================================\n","Saved best model\n","Epoch 614, Loss 33568448674816.00, Val loss 1674353311744.00\n","==================================================================\n","Saved best model\n","Epoch 615, Loss 33642352032512.00, Val loss 1671966359552.00\n","Epoch 616, Loss 33921769009408.00, Val loss 1672195473408.00\n","==================================================================\n","Saved best model\n","Epoch 617, Loss 34225951752960.00, Val loss 1671304839168.00\n","==================================================================\n","Saved best model\n","Epoch 618, Loss 34084973032704.00, Val loss 1669305073664.00\n","Epoch 619, Loss 34517046437888.00, Val loss 1671935295488.00\n","Epoch 620, Loss 34326951841024.00, Val loss 1670529548288.00\n","==================================================================\n","Saved best model\n","Epoch 621, Loss 34225464526080.00, Val loss 1668414570496.00\n","==================================================================\n","Saved best model\n","Epoch 622, Loss 34498421154816.00, Val loss 1665379729408.00\n","Epoch 623, Loss 35060170385152.00, Val loss 1667873636352.00\n","Epoch 624, Loss 34896008539648.00, Val loss 1667255894016.00\n","Epoch 625, Loss 34889909671680.00, Val loss 1665863647232.00\n","==================================================================\n","Saved best model\n","Epoch 626, Loss 35063967203584.00, Val loss 1664319488000.00\n","Epoch 627, Loss 35757317099264.00, Val loss 1667441491968.00\n","Epoch 628, Loss 35991579164416.00, Val loss 1668621008896.00\n","Epoch 629, Loss 35073212449792.00, Val loss 1669318705152.00\n","Epoch 630, Loss 35347104367360.00, Val loss 1668205510656.00\n","Epoch 631, Loss 35263846334208.00, Val loss 1667546742784.00\n","Epoch 632, Loss 35345702093312.00, Val loss 1666437611520.00\n","Epoch 633, Loss 35651593346048.00, Val loss 1665036189696.00\n","Epoch 634, Loss 36221128279808.00, Val loss 1668938465280.00\n","Epoch 635, Loss 36538597303296.00, Val loss 1673934798848.00\n","Epoch 636, Loss 35638809533696.00, Val loss 1672550416384.00\n","Epoch 637, Loss 36227303291136.00, Val loss 1674289741824.00\n","Epoch 638, Loss 35789338748672.00, Val loss 1674992287744.00\n","Epoch 639, Loss 36057268488448.00, Val loss 1674378215424.00\n","Epoch 640, Loss 36011548307456.00, Val loss 1674089988096.00\n","Epoch 641, Loss 36229878186240.00, Val loss 1672693284864.00\n","Epoch 642, Loss 33142046254336.00, Val loss 1748872331264.00\n","Epoch 643, Loss 91414445482496.00, Val loss 4485550702592.00\n","Epoch 644, Loss 86052279060736.00, Val loss 4380005498880.00\n","Epoch 645, Loss 82833729978624.00, Val loss 4300855050240.00\n","Epoch 646, Loss 80391184972288.00, Val loss 4234535239680.00\n","Epoch 647, Loss 78398643127296.00, Val loss 4176420274176.00\n","Epoch 648, Loss 76718344949760.00, Val loss 4124270657536.00\n","Epoch 649, Loss 75275229633536.00, Val loss 4076803981312.00\n","Epoch 650, Loss 74022305197056.00, Val loss 4033202880512.00\n","Epoch 651, Loss 666263943302144.00, Val loss 3992629542912.00\n","==================================================================\n","Saved best model\n","Epoch 652, Loss 34696805028864.00, Val loss 1664181600256.00\n","Epoch 653, Loss 62215236563968.00, Val loss 1664595656704.00\n","Epoch 654, Loss 94271736168960.00, Val loss 4631162781696.00\n","Epoch 655, Loss 94273962088960.00, Val loss 4631159635968.00\n","Epoch 656, Loss 94273798287360.00, Val loss 4631156490240.00\n","Epoch 657, Loss 94273631465984.00, Val loss 4631153344512.00\n","Epoch 658, Loss 94273461679104.00, Val loss 4631150198784.00\n","Epoch 659, Loss 94273288649216.00, Val loss 4631147053056.00\n","Epoch 660, Loss 94273112342528.00, Val loss 4631143907328.00\n","Epoch 661, Loss 95155724234752.00, Val loss 4631140761600.00\n","Epoch 662, Loss 94277657836544.00, Val loss 4631138140160.00\n","Epoch 663, Loss 94271173346816.00, Val loss 4631134470144.00\n","Epoch 664, Loss 94270012468736.00, Val loss 4631131324416.00\n","Epoch 665, Loss 94269821527040.00, Val loss 4631128178688.00\n","Epoch 666, Loss 94269627234816.00, Val loss 4631123984384.00\n","Epoch 667, Loss 94269427682816.00, Val loss 4631120838656.00\n","Epoch 668, Loss 94269221976064.00, Val loss 4631117168640.00\n","Epoch 669, Loss 94269012067328.00, Val loss 4631114022912.00\n","Epoch 670, Loss 94268797190656.00, Val loss 4631110352896.00\n","Epoch 671, Loss 94268575427072.00, Val loss 4631107207168.00\n","Epoch 672, Loss 94268347823104.00, Val loss 4631103537152.00\n","Epoch 673, Loss 94268113354752.00, Val loss 4631099867136.00\n","Epoch 674, Loss 94267872815616.00, Val loss 4631096197120.00\n","Epoch 675, Loss 94267623502336.00, Val loss 4631093051392.00\n","Epoch 676, Loss 94267369363456.00, Val loss 4631088857088.00\n","Epoch 677, Loss 94267105818112.00, Val loss 4631085187072.00\n","Epoch 678, Loss 94266832887296.00, Val loss 4631080992768.00\n","Epoch 679, Loss 94266552144384.00, Val loss 4631076798464.00\n","Epoch 680, Loss 94266263095808.00, Val loss 4631073128448.00\n","Epoch 681, Loss 94265963722752.00, Val loss 4631069458432.00\n","Epoch 682, Loss 94265653670912.00, Val loss 4631065264128.00\n","Epoch 683, Loss 94265331741184.00, Val loss 4631061069824.00\n","Epoch 684, Loss 94264999024640.00, Val loss 4631056351232.00\n","Epoch 685, Loss 94264654819328.00, Val loss 4631052156928.00\n","Epoch 686, Loss 94264297463808.00, Val loss 4631047962624.00\n","Epoch 687, Loss 94263926691840.00, Val loss 4631043244032.00\n","Epoch 688, Loss 94263542806016.00, Val loss 4631038001152.00\n","Epoch 689, Loss 94263146033664.00, Val loss 4631033806848.00\n","Epoch 690, Loss 94262732208640.00, Val loss 4631029088256.00\n","Epoch 691, Loss 94262301425664.00, Val loss 4631023845376.00\n","Epoch 692, Loss 94261856064000.00, Val loss 4631019126784.00\n","Epoch 693, Loss 94261393431040.00, Val loss 4631013883904.00\n","Epoch 694, Loss 94260912677376.00, Val loss 4631008641024.00\n","Epoch 695, Loss 94260413849600.00, Val loss 4631002873856.00\n","Epoch 696, Loss 94259896939008.00, Val loss 4630997630976.00\n","Epoch 697, Loss 94259355760128.00, Val loss 4630991863808.00\n","Epoch 698, Loss 94258798201856.00, Val loss 4630986096640.00\n","Epoch 699, Loss 94258217238528.00, Val loss 4630979805184.00\n","Epoch 700, Loss 94257614981120.00, Val loss 4630973513728.00\n","Epoch 701, Loss 94256990194688.00, Val loss 4630966697984.00\n","Epoch 702, Loss 94256341957632.00, Val loss 4630959882240.00\n","Epoch 703, Loss 94255669368320.00, Val loss 4630953590784.00\n","Epoch 704, Loss 94254973302784.00, Val loss 4630946775040.00\n","Epoch 705, Loss 94254253227520.00, Val loss 4630938910720.00\n","Epoch 706, Loss 94253507952128.00, Val loss 4630931570688.00\n","Epoch 707, Loss 94252736433152.00, Val loss 4630923706368.00\n","Epoch 708, Loss 94251941609472.00, Val loss 4630915842048.00\n","Epoch 709, Loss 94251120408576.00, Val loss 4630907977728.00\n","Epoch 710, Loss 94250272087552.00, Val loss 4630899589120.00\n","Epoch 711, Loss 94249398979584.00, Val loss 4630890676224.00\n","Epoch 712, Loss 94248500643328.00, Val loss 4630881239040.00\n","Epoch 713, Loss 94247577481216.00, Val loss 4630872850432.00\n","Epoch 714, Loss 94246630074880.00, Val loss 4630862888960.00\n","Epoch 715, Loss 94245656657920.00, Val loss 4630853451776.00\n","Epoch 716, Loss 94244661869568.00, Val loss 4630842441728.00\n","Epoch 717, Loss 94243642953728.00, Val loss 4630832480256.00\n","Epoch 718, Loss 94242602693632.00, Val loss 4630822518784.00\n","Epoch 719, Loss 94241541386240.00, Val loss 4630810984448.00\n","Epoch 720, Loss 94240461790720.00, Val loss 4630799450112.00\n","Epoch 721, Loss 94239363404800.00, Val loss 4630788440064.00\n","Epoch 722, Loss 94238248932352.00, Val loss 4630776381440.00\n","Epoch 723, Loss 94237117120000.00, Val loss 4630764322816.00\n","Epoch 724, Loss 94235969907712.00, Val loss 4630751739904.00\n","Epoch 725, Loss 94234812560896.00, Val loss 4630739156992.00\n","Epoch 726, Loss 94233642006528.00, Val loss 4630725525504.00\n","Epoch 727, Loss 94232460363264.00, Val loss 4630711894016.00\n","Epoch 728, Loss 94231272843264.00, Val loss 4630698262528.00\n","Epoch 729, Loss 94230078059008.00, Val loss 4630683582464.00\n","Epoch 730, Loss 94228878342656.00, Val loss 4630669426688.00\n","Epoch 731, Loss 94227673247744.00, Val loss 4630654222336.00\n","Epoch 732, Loss 94226465008640.00, Val loss 4630639542272.00\n","Epoch 733, Loss 94225256794624.00, Val loss 4630623289344.00\n","Epoch 734, Loss 94224048944128.00, Val loss 4630607560704.00\n","Epoch 735, Loss 94222841079552.00, Val loss 4630592356352.00\n","Epoch 736, Loss 94221635126272.00, Val loss 4630575579136.00\n","Epoch 737, Loss 94220433037056.00, Val loss 4630558801920.00\n","Epoch 738, Loss 94219233962752.00, Val loss 4630541500416.00\n","Epoch 739, Loss 94218040572928.00, Val loss 4630524198912.00\n","Epoch 740, Loss 94216850283520.00, Val loss 4630506897408.00\n","Epoch 741, Loss 94215667424768.00, Val loss 4630489071616.00\n","Epoch 742, Loss 94214489828352.00, Val loss 4630470197248.00\n","Epoch 743, Loss 94213319343360.00, Val loss 4630451847168.00\n","Epoch 744, Loss 94212155066880.00, Val loss 4630432972800.00\n","Epoch 745, Loss 94210998164736.00, Val loss 4630414098432.00\n","Epoch 746, Loss 94209847504640.00, Val loss 4630394175488.00\n","Epoch 747, Loss 94208705432320.00, Val loss 4630374776832.00\n","Epoch 748, Loss 94207566750976.00, Val loss 4630354853888.00\n","Epoch 749, Loss 94206439843584.00, Val loss 4630334406656.00\n","Epoch 750, Loss 94205315909632.00, Val loss 4630313959424.00\n","Epoch 751, Loss 94204197685760.00, Val loss 4630292987904.00\n","Epoch 752, Loss 94203087178240.00, Val loss 4630272540672.00\n","Epoch 753, Loss 94201981999616.00, Val loss 4630250520576.00\n","Epoch 754, Loss 94200880989952.00, Val loss 4630228500480.00\n","Epoch 755, Loss 94199785861376.00, Val loss 4630207528960.00\n","Epoch 756, Loss 94198692923648.00, Val loss 4630185508864.00\n","Epoch 757, Loss 94197604610560.00, Val loss 4630162964480.00\n","Epoch 758, Loss 94196518637568.00, Val loss 4630140420096.00\n","Epoch 759, Loss 94195434893824.00, Val loss 4630117875712.00\n","Epoch 760, Loss 94194350085632.00, Val loss 4630094807040.00\n","Epoch 761, Loss 94193056424448.00, Val loss 4592544776192.00\n","Epoch 762, Loss 74823842102272.00, Val loss 1749761392640.00\n","Epoch 763, Loss 37972613801216.00, Val loss 1703764951040.00\n","Epoch 764, Loss 38085198752000.00, Val loss 1706732027904.00\n","Epoch 765, Loss 39541378746880.00, Val loss 1723151941632.00\n","Epoch 766, Loss 63757128958464.00, Val loss 1734845136896.00\n","Epoch 767, Loss 94189644307456.00, Val loss 4629993095168.00\n","Epoch 768, Loss 94188451814656.00, Val loss 4629964783616.00\n","Epoch 769, Loss 94187203521024.00, Val loss 4629936996352.00\n","Epoch 770, Loss 94185968773376.00, Val loss 4629908684800.00\n","Epoch 771, Loss 94184744982784.00, Val loss 4629880373248.00\n","Epoch 772, Loss 94183529645568.00, Val loss 4629852061696.00\n","Epoch 773, Loss 94182320050688.00, Val loss 4629823750144.00\n","Epoch 774, Loss 94181112657408.00, Val loss 4629795962880.00\n","Epoch 775, Loss 94179909491712.00, Val loss 4629767127040.00\n","Epoch 776, Loss 94178705716736.00, Val loss 4629738815488.00\n","Epoch 777, Loss 94177501162752.00, Val loss 4629711028224.00\n","Epoch 778, Loss 37464800198912.00, Val loss 2006568140800.00\n","Epoch 779, Loss 90330555801984.00, Val loss 4497975803904.00\n","Epoch 780, Loss 87701515220480.00, Val loss 4444674588672.00\n","Epoch 781, Loss 85900048109312.00, Val loss 4400179838976.00\n","Epoch 782, Loss 84368280039168.00, Val loss 4360107196416.00\n","Epoch 783, Loss 82999550482176.00, Val loss 4322866757632.00\n","Epoch 784, Loss 81748469555200.00, Val loss 4287677595648.00\n","Epoch 785, Loss 80590521127424.00, Val loss 4254097735680.00\n","Epoch 786, Loss 79510759928320.00, Val loss 4221851926528.00\n","Epoch 787, Loss 78499324723712.00, Val loss 4190761385984.00\n","Epoch 788, Loss 77549328812032.00, Val loss 4160702644224.00\n","Epoch 789, Loss 76655714641920.00, Val loss 4131592601600.00\n","Epoch 790, Loss 75814627785728.00, Val loss 4103371227136.00\n","Epoch 791, Loss 75022995294720.00, Val loss 4075995529216.00\n","Epoch 792, Loss 74278291623936.00, Val loss 4049435623424.00\n","Epoch 793, Loss 73578353160192.00, Val loss 4023670538240.00\n","Epoch 794, Loss 72921264916480.00, Val loss 3998684807168.00\n","Epoch 795, Loss 72305281351680.00, Val loss 3974467420160.00\n","Epoch 796, Loss 71728769700864.00, Val loss 3951011561472.00\n","Epoch 797, Loss 71190158462976.00, Val loss 3928311726080.00\n","Epoch 798, Loss 70687919703040.00, Val loss 3906362933248.00\n","Epoch 799, Loss 70220544916480.00, Val loss 3885161250816.00\n","Epoch 800, Loss 69786533265408.00, Val loss 3864703533056.00\n","Epoch 801, Loss 69384379121664.00, Val loss 3844984012800.00\n","Epoch 802, Loss 69012577363968.00, Val loss 3825999282176.00\n","Epoch 803, Loss 68669616805888.00, Val loss 3807741739008.00\n","Epoch 804, Loss 68353989982208.00, Val loss 3790206402560.00\n","Epoch 805, Loss 68064190638080.00, Val loss 3773383311360.00\n","Epoch 806, Loss 67798714863616.00, Val loss 3757264338944.00\n","Epoch 807, Loss 67556093255680.00, Val loss 3741837688832.00\n","Epoch 808, Loss 67334851307520.00, Val loss 3727092088832.00\n","Epoch 809, Loss 67133566650368.00, Val loss 3713013121024.00\n","Epoch 810, Loss 66950844094464.00, Val loss 3699586367488.00\n","Epoch 811, Loss 66785326635008.00, Val loss 3686795837440.00\n","Epoch 812, Loss 66635715663872.00, Val loss 3674624753664.00\n","Epoch 813, Loss 66500762449920.00, Val loss 3663055814656.00\n","Epoch 814, Loss 66379269562368.00, Val loss 3652069621760.00\n","Epoch 815, Loss 66270107133952.00, Val loss 3641647300608.00\n","Epoch 816, Loss 66172207988736.00, Val loss 3631769976832.00\n","Epoch 817, Loss 66084557807616.00, Val loss 3622415368192.00\n","Epoch 818, Loss 66006225645568.00, Val loss 3613565124608.00\n","Epoch 819, Loss 95833577809920.00, Val loss 4630000435200.00\n","Epoch 820, Loss 94188355014656.00, Val loss 4629976317952.00\n","Epoch 821, Loss 94187422889216.00, Val loss 4629954822144.00\n","Epoch 822, Loss 94186535693824.00, Val loss 4629934374912.00\n","Epoch 823, Loss 94185680653568.00, Val loss 4629913403392.00\n","Epoch 824, Loss 94184851123200.00, Val loss 4629894004736.00\n","Epoch 825, Loss 94184036648960.00, Val loss 4629875130368.00\n","Epoch 826, Loss 94183238361344.00, Val loss 4629855731712.00\n","Epoch 827, Loss 94182451514112.00, Val loss 4629836857344.00\n","Epoch 828, Loss 94181670134272.00, Val loss 4629817982976.00\n","Epoch 829, Loss 94180895877632.00, Val loss 4629799632896.00\n","Epoch 830, Loss 94180124942592.00, Val loss 4629780758528.00\n","Epoch 831, Loss 94179350927616.00, Val loss 4629762408448.00\n","Epoch 832, Loss 94178581624320.00, Val loss 4629744058368.00\n","Epoch 833, Loss 94177810363648.00, Val loss 4629725184000.00\n","Epoch 834, Loss 94177034600192.00, Val loss 4629706833920.00\n","Epoch 835, Loss 94176256427008.00, Val loss 4629687959552.00\n","Epoch 836, Loss 94175473995776.00, Val loss 4629669085184.00\n","Epoch 837, Loss 94174689725184.00, Val loss 4629650210816.00\n","Epoch 838, Loss 94173894061312.00, Val loss 4629630812160.00\n","Epoch 839, Loss 94173096330240.00, Val loss 4629611413504.00\n","Epoch 840, Loss 94172289447424.00, Val loss 4629592539136.00\n","Epoch 841, Loss 94171473645056.00, Val loss 4629572091904.00\n","Epoch 842, Loss 94170649678592.00, Val loss 4629552693248.00\n","Epoch 843, Loss 94169817848320.00, Val loss 4629532246016.00\n","Epoch 844, Loss 94168973310720.00, Val loss 4629511798784.00\n","Epoch 845, Loss 94168120966656.00, Val loss 4629490827264.00\n","Epoch 846, Loss 94167255972864.00, Val loss 4629470380032.00\n","Epoch 847, Loss 94166381533952.00, Val loss 4629448359936.00\n","Epoch 848, Loss 94165495098880.00, Val loss 4629427388416.00\n","Epoch 849, Loss 94164594524928.00, Val loss 4629405368320.00\n","Epoch 850, Loss 94163683313408.00, Val loss 4629384396800.00\n","Epoch 851, Loss 94162760640768.00, Val loss 4629362376704.00\n","Epoch 852, Loss 94161822771456.00, Val loss 4629339308032.00\n","Epoch 853, Loss 94160875125760.00, Val loss 4629315715072.00\n","Epoch 854, Loss 94159911652864.00, Val loss 4629292646400.00\n","Epoch 855, Loss 94158936229376.00, Val loss 4629269053440.00\n","Epoch 856, Loss 94157947152896.00, Val loss 4629244936192.00\n","Epoch 857, Loss 94156946555648.00, Val loss 4629220818944.00\n","Epoch 858, Loss 94155930977280.00, Val loss 4629196177408.00\n","Epoch 859, Loss 94154904761344.00, Val loss 4629171011584.00\n","Epoch 860, Loss 94153864627456.00, Val loss 4629145845760.00\n","Epoch 861, Loss 94152812297216.00, Val loss 4629120679936.00\n","Epoch 862, Loss 94151746889728.00, Val loss 4629094989824.00\n","Epoch 863, Loss 94150670980608.00, Val loss 4629069299712.00\n","Epoch 864, Loss 94149582248704.00, Val loss 4629043085312.00\n","Epoch 865, Loss 94148483230720.00, Val loss 4629015822336.00\n","Epoch 866, Loss 94147372735488.00, Val loss 4628989083648.00\n","Epoch 867, Loss 94146251784960.00, Val loss 4628962344960.00\n","Epoch 868, Loss 94145120616192.00, Val loss 4628935081984.00\n","Epoch 869, Loss 94143978281216.00, Val loss 4628907294720.00\n","Epoch 870, Loss 94142826820352.00, Val loss 4628880031744.00\n","Epoch 871, Loss 94141667652352.00, Val loss 4628852244480.00\n","Epoch 872, Loss 94140498547456.00, Val loss 4628823408640.00\n","Epoch 873, Loss 94139319525120.00, Val loss 4628795097088.00\n","Epoch 874, Loss 94138133722112.00, Val loss 4628766261248.00\n","Epoch 875, Loss 94136939791616.00, Val loss 4628737425408.00\n","Epoch 876, Loss 94135738512128.00, Val loss 4628708589568.00\n","Epoch 877, Loss 94134528200704.00, Val loss 4628679229440.00\n","Epoch 878, Loss 94133311314688.00, Val loss 4628649869312.00\n","Epoch 879, Loss 94132086641408.00, Val loss 4628621033472.00\n","Epoch 880, Loss 94130856772352.00, Val loss 4628591149056.00\n","Epoch 881, Loss 94129619068928.00, Val loss 4628560740352.00\n","Epoch 882, Loss 94128374572288.00, Val loss 4628530855936.00\n","Epoch 883, Loss 94127123626752.00, Val loss 4628500971520.00\n","Epoch 884, Loss 94125864262656.00, Val loss 4628470038528.00\n","Epoch 885, Loss 94124600202496.00, Val loss 4628439629824.00\n","Epoch 886, Loss 94123329500160.00, Val loss 4628409221120.00\n","Epoch 887, Loss 94122051935232.00, Val loss 4628378812416.00\n","Epoch 888, Loss 94120766974720.00, Val loss 4628347355136.00\n","Epoch 889, Loss 94119476701440.00, Val loss 4628316422144.00\n","Epoch 890, Loss 94118177150208.00, Val loss 4628284964864.00\n","Epoch 891, Loss 94116871815168.00, Val loss 4628253507584.00\n","Epoch 892, Loss 94115557728768.00, Val loss 4628221526016.00\n","Epoch 893, Loss 94114237636608.00, Val loss 4628189544448.00\n","Epoch 894, Loss 94112908645120.00, Val loss 4628157038592.00\n","Epoch 895, Loss 94111573304064.00, Val loss 4628125057024.00\n","Epoch 896, Loss 94110226364672.00, Val loss 4628093599744.00\n","Epoch 897, Loss 94108871074816.00, Val loss 4628060569600.00\n","Epoch 898, Loss 94107508141824.00, Val loss 4628027539456.00\n","Epoch 899, Loss 94106134788608.00, Val loss 4627994509312.00\n","Epoch 900, Loss 94104752112384.00, Val loss 4627960954880.00\n","Epoch 901, Loss 94103358589696.00, Val loss 4627926876160.00\n","Epoch 902, Loss 94101954112256.00, Val loss 4627893321728.00\n","Epoch 903, Loss 94100537327616.00, Val loss 4627859767296.00\n","Epoch 904, Loss 94099108598528.00, Val loss 4627824640000.00\n","Epoch 905, Loss 94097669068800.00, Val loss 4627789512704.00\n","Epoch 906, Loss 94096214146048.00, Val loss 4627754385408.00\n","Epoch 907, Loss 94094748431104.00, Val loss 4627719258112.00\n","Epoch 908, Loss 94093267267840.00, Val loss 4627683606528.00\n","Epoch 909, Loss 94091770938112.00, Val loss 4627647430656.00\n","Epoch 910, Loss 94090262622976.00, Val loss 4627610730496.00\n","Epoch 911, Loss 94088737674240.00, Val loss 4627574030336.00\n","Epoch 912, Loss 94087195517440.00, Val loss 4627536805888.00\n","Epoch 913, Loss 94085637547008.00, Val loss 4627499057152.00\n","Epoch 914, Loss 94084065761792.00, Val loss 4627460784128.00\n","Epoch 915, Loss 94082476849408.00, Val loss 4627422511104.00\n","Epoch 916, Loss 94080869526528.00, Val loss 4627383189504.00\n","Epoch 917, Loss 94079245273600.00, Val loss 4627344392192.00\n","Epoch 918, Loss 94077605922560.00, Val loss 4627305070592.00\n","Epoch 919, Loss 94075949467648.00, Val loss 4627264700416.00\n","Epoch 920, Loss 94074275324672.00, Val loss 4627224330240.00\n","Epoch 921, Loss 94072585300736.00, Val loss 4627183960064.00\n","Epoch 922, Loss 94070879241728.00, Val loss 4627142017024.00\n","Epoch 923, Loss 94069157491456.00, Val loss 4627100598272.00\n","Epoch 924, Loss 94067420173312.00, Val loss 4627058655232.00\n","Epoch 925, Loss 94065668541184.00, Val loss 4627016187904.00\n","Epoch 926, Loss 94063902861824.00, Val loss 4626973196288.00\n","Epoch 927, Loss 94062121585920.00, Val loss 4626930204672.00\n","Epoch 928, Loss 94060328629760.00, Val loss 4626887213056.00\n","Epoch 929, Loss 94058523712256.00, Val loss 4626843172864.00\n","Epoch 930, Loss 94056706682368.00, Val loss 4626799656960.00\n","Epoch 931, Loss 94054878761728.00, Val loss 4626755616768.00\n","Epoch 932, Loss 94053041768448.00, Val loss 4626711052288.00\n","Epoch 933, Loss 94051195371520.00, Val loss 4626666487808.00\n","Epoch 934, Loss 94049339469824.00, Val loss 4626621399040.00\n","Epoch 935, Loss 94047474178816.00, Val loss 4626576834560.00\n","Epoch 936, Loss 94045604172800.00, Val loss 4626531745792.00\n","Epoch 937, Loss 94043727475712.00, Val loss 4626486657024.00\n","Epoch 938, Loss 94041842634752.00, Val loss 4626441043968.00\n","Epoch 939, Loss 94039952313088.00, Val loss 4626395430912.00\n","Epoch 940, Loss 94038057374464.00, Val loss 4626349293568.00\n","Epoch 941, Loss 94036159215360.00, Val loss 4626303680512.00\n","Epoch 942, Loss 94034256821504.00, Val loss 4626258067456.00\n","Epoch 943, Loss 94032348186112.00, Val loss 4626211930112.00\n","Epoch 944, Loss 94030439138304.00, Val loss 4626166841344.00\n","Epoch 945, Loss 94028524379904.00, Val loss 4626119655424.00\n","Epoch 946, Loss 94026608496128.00, Val loss 4626073518080.00\n","Epoch 947, Loss 94024690622464.00, Val loss 4626027905024.00\n","Epoch 948, Loss 94022769728512.00, Val loss 4625981243392.00\n","Epoch 949, Loss 94020847517696.00, Val loss 4625934581760.00\n","Epoch 950, Loss 94018923738880.00, Val loss 4625888444416.00\n","Epoch 951, Loss 94016997834240.00, Val loss 4625842307072.00\n","Epoch 952, Loss 94015070910464.00, Val loss 4625795645440.00\n","Epoch 953, Loss 94013142328832.00, Val loss 4625749508096.00\n","Epoch 954, Loss 94011215095296.00, Val loss 4625702846464.00\n","Epoch 955, Loss 94009284030720.00, Val loss 4625656184832.00\n","Epoch 956, Loss 94007353394432.00, Val loss 4625608998912.00\n","Epoch 957, Loss 94005422428672.00, Val loss 4625563385856.00\n","Epoch 958, Loss 94003489934080.00, Val loss 4625516199936.00\n","Epoch 959, Loss 94001557802240.00, Val loss 4625470062592.00\n","Epoch 960, Loss 93999624651520.00, Val loss 4625422876672.00\n","Epoch 961, Loss 93997690156032.00, Val loss 4625376739328.00\n","Epoch 962, Loss 93995757365760.00, Val loss 4625330077696.00\n","Epoch 963, Loss 93993822234880.00, Val loss 4625283416064.00\n","Epoch 964, Loss 93991887667200.00, Val loss 4625236754432.00\n","Epoch 965, Loss 93989954096896.00, Val loss 4625190617088.00\n","Epoch 966, Loss 93988019815936.00, Val loss 4625143955456.00\n","Epoch 967, Loss 93986084024576.00, Val loss 4625097293824.00\n","Epoch 968, Loss 93984148447232.00, Val loss 4625050632192.00\n","Epoch 969, Loss 93982215075072.00, Val loss 4625003446272.00\n","Epoch 970, Loss 93980278713344.00, Val loss 4624957308928.00\n","Epoch 971, Loss 93978343830272.00, Val loss 4624911171584.00\n","Epoch 972, Loss 93976408582144.00, Val loss 4624863985664.00\n","Epoch 973, Loss 93974472432384.00, Val loss 4624816799744.00\n","Epoch 974, Loss 93972535845120.00, Val loss 4624770662400.00\n","Epoch 975, Loss 93970602054912.00, Val loss 4624723476480.00\n","Epoch 976, Loss 93968665921280.00, Val loss 4624677339136.00\n","Epoch 977, Loss 93966729772288.00, Val loss 4624630677504.00\n","Epoch 978, Loss 93964795474944.00, Val loss 4624583491584.00\n","Epoch 979, Loss 93962858384640.00, Val loss 4624537354240.00\n","Epoch 980, Loss 93960924117760.00, Val loss 4624490168320.00\n","Epoch 981, Loss 93958989513728.00, Val loss 4624444030976.00\n","Epoch 982, Loss 93957054697984.00, Val loss 4624397369344.00\n","Epoch 983, Loss 93955118522112.00, Val loss 4624350707712.00\n","Epoch 984, Loss 93953183563264.00, Val loss 4624304046080.00\n","Epoch 985, Loss 93951248517888.00, Val loss 4624257384448.00\n","Epoch 986, Loss 93949313717248.00, Val loss 4624210722816.00\n","Epoch 987, Loss 93947379134720.00, Val loss 4624164585472.00\n","Epoch 988, Loss 93945442048256.00, Val loss 4624117399552.00\n","Epoch 989, Loss 93943509145600.00, Val loss 4624070737920.00\n","Epoch 990, Loss 93941573708544.00, Val loss 4624024076288.00\n","Epoch 991, Loss 93939639518720.00, Val loss 4623977414656.00\n","Epoch 992, Loss 93937703450112.00, Val loss 4623930753024.00\n","Epoch 993, Loss 93935770515712.00, Val loss 4623884091392.00\n","Epoch 994, Loss 93933835938560.00, Val loss 4623837429760.00\n","Epoch 995, Loss 93931900327168.00, Val loss 4623791292416.00\n","Epoch 996, Loss 93929965273088.00, Val loss 4623744630784.00\n","Epoch 997, Loss 93928033072896.00, Val loss 4623697969152.00\n","Epoch 998, Loss 93926098193920.00, Val loss 4623650783232.00\n","Epoch 999, Loss 93924163102464.00, Val loss 4623604645888.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"status":"ok","timestamp":1648741057899,"user_tz":300,"elapsed":187,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"4e477f00-c06a-4176-c315-3219a493f422"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5183366496256.0\n","tensor([1498.2845, 1506.0310, 1527.5992, 1539.2390, 1551.4510, 1564.0785,\n","        1586.0848, 1598.1151, 1613.2634, 1634.6437, 1641.2559, 1663.3315,\n","        1693.6451, 1723.6447, 1762.8822], grad_fn=<SelectBackward0>)\n","tensor([292187., 293697., 293697., 293697., 293697., 293697., 295701., 296870.,\n","        297729., 297729., 297729., 298362., 298626., 298808., 298993.])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","    # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_cumulative_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_cumulative_infected_tensor':labeled_output # (52, 15)\n","}\n","\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)"],"metadata":{"id":"s3-8Yge6CAWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_squared_error = criterion(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","mae_function = torch.nn.L1Loss()\n","mean_absolute_error = mae_function(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","print(\"mean squared error: \", mean_squared_error)\n","print(\"mean absolute error: \", mean_absolute_error)\n","\n","# With manual seed = 0 - trial 1\n","# mean squared error:  5183366496256.0\n","# mean absolute error:  1512013.125\n","\n","# With manual seed = 0 - trial 2\n","# mean squared error:  5183366496256.0\n","# mean absolute error:  1512013.125"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Re_cSYo66Jln","executionInfo":{"status":"ok","timestamp":1648741057905,"user_tz":300,"elapsed":14,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"657336cd-3092-4547-efef-c9685816b715"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean squared error:  5183366496256.0\n","mean absolute error:  1512013.125\n"]}]}]}