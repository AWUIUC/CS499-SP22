{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.3_new_data_COVID_Forecaster_full.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOZoJ7fbDwoJlRgd0O0s3Lk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1649261313170,"user_tz":300,"elapsed":18284,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"c12ade39-91e3-41a4-aef2-2ef4811f50ce"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad8e5297-0709-49d9-9c5c-95a42093534f","executionInfo":{"status":"ok","timestamp":1649261375243,"user_tz":300,"elapsed":62080,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 2.6 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.13\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 2.8 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.0\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n","\u001b[K     |████████████████████████████████| 750 kB 2.7 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n","\u001b[K     |████████████████████████████████| 407 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=65ae8c3df678fa282118dec4927d0f8a238a25fee10a16edf1a7141f3bf38c77\n","  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n","Successfully built torch-geometric\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.0.4\n","Collecting torch-geometric-temporal\n","  Downloading torch_geometric_temporal-0.52.0.tar.gz (48 kB)\n","\u001b[K     |████████████████████████████████| 48 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: torch_sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.13)\n","Requirement already satisfied: torch_scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.6.3)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.1.0)\n","Building wheels for collected packages: torch-geometric-temporal\n","  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.52.0-py3-none-any.whl size=86194 sha256=2036d2d3f390a61617d925e2939bcb9721a346677c01e9bc573bb6ae94a72490\n","  Stored in directory: /root/.cache/pip/wheels/eb/f1/e5/dd02c6d1e5f00f907f5e4894c60c5a21b774db1ed7464d0c23\n","Successfully built torch-geometric-temporal\n","Installing collected packages: torch-geometric-temporal\n","Successfully installed torch-geometric-temporal-0.52.0\n","Collecting ogb\n","  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Collecting outdated>=0.2.0\n","  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Collecting littleutils\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=124152c0c519a36bfc25450d3982d078c1f4ae04b55bcfc2564a02fbbedf419b\n","  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n","PyTorch has version 1.10.0+cu111\n","Collecting epiweeks\n","  Downloading epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n","Installing collected packages: epiweeks\n","Successfully installed epiweeks-2.1.4\n","Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 24\n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3.3_to_3.5_new_data.pickle'\n","save_model_relative_path = './saved_models/v3.3_new_data_COVID_Forecaster_full'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3.3_archived_output.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1649261375244,"user_tz":300,"elapsed":17,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"09c18e48-5ee5-4b91-ff96-28ace08ecb0b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["# Try to ensure reproducibility\n","torch.manual_seed(0)"],"metadata":{"id":"9Ijgx4LykuQZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649261375244,"user_tz":300,"elapsed":15,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"764f4ada-92c8-4d9b-f5f9-09b7995c31e0"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f5d286609f0>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_confirmed_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_confirmed_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_confirmed_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class COVID_Forecaster_full(torch.nn.Module):\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","      self.linear2 = Linear(history_window, outputLayer_num_features)\n","      self.linear3 = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x) + self.linear2(data.x[:, 0:history_window])\n","\n","      x = F.elu(x)\n","      x = self.linear3(x)\n","\n","      return x\n","\n","model = COVID_Forecaster_full().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648743668985,"user_tz":300,"elapsed":21,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"4bee9d98-b9ce-4968-c1a2-aa9937b30261"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["COVID_Forecaster_full(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n","  (linear2): Linear(in_features=6, out_features=15, bias=True)\n","  (linear3): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648745246708,"user_tz":300,"elapsed":1577732,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"c7facfeb-81e1-432e-f251-e76f0ae97440"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 93805565402.00, Val loss 32237856768.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 47085821279.00, Val loss 29494781952.00\n","==================================================================\n","Saved best model\n","Epoch 2, Loss 44221056994.00, Val loss 27942463488.00\n","Epoch 3, Loss 42833796073.50, Val loss 47243345920.00\n","Epoch 4, Loss 52687074589.00, Val loss 43452399616.00\n","Epoch 5, Loss 59187599032.00, Val loss 57806032896.00\n","Epoch 6, Loss 110120048002.00, Val loss 35676921856.00\n","Epoch 7, Loss 50627872581.00, Val loss 34854072320.00\n","Epoch 8, Loss 49680058274.00, Val loss 30570014720.00\n","Epoch 9, Loss 47445732286.00, Val loss 32907759616.00\n","Epoch 10, Loss 49833828576.00, Val loss 33465399296.00\n","Epoch 11, Loss 52360711394.00, Val loss 52271767552.00\n","Epoch 12, Loss 66014063812.00, Val loss 64293470208.00\n","Epoch 13, Loss 77897301972.00, Val loss 58954100736.00\n","Epoch 14, Loss 73408043170.00, Val loss 61619994624.00\n","Epoch 15, Loss 76727463360.00, Val loss 56417538048.00\n","Epoch 16, Loss 72488775172.00, Val loss 60576153600.00\n","Epoch 17, Loss 77181844604.00, Val loss 53421424640.00\n","Epoch 18, Loss 71067656758.00, Val loss 59182460928.00\n","Epoch 19, Loss 77641166034.00, Val loss 53058682880.00\n","Epoch 20, Loss 73124217732.00, Val loss 57012506624.00\n","Epoch 21, Loss 78272500810.00, Val loss 51942858752.00\n","Epoch 22, Loss 74062171028.00, Val loss 54192852992.00\n","Epoch 23, Loss 76268479674.00, Val loss 50033926144.00\n","Epoch 24, Loss 72596646810.00, Val loss 49880330240.00\n","Epoch 25, Loss 73332228928.00, Val loss 51492712448.00\n","Epoch 26, Loss 75563269014.00, Val loss 48375762944.00\n","Epoch 27, Loss 75950602028.50, Val loss 40574976000.00\n","Epoch 28, Loss 49604392258.00, Val loss 32216274944.00\n","Epoch 29, Loss 56720050718.00, Val loss 31746144256.00\n","==================================================================\n","Saved best model\n","Epoch 30, Loss 58519592662.00, Val loss 26118477824.00\n","Epoch 31, Loss 56002992334.00, Val loss 28812480512.00\n","Epoch 32, Loss 57104227044.00, Val loss 35763322880.00\n","Epoch 33, Loss 61651827178.00, Val loss 54731616256.00\n","==================================================================\n","Saved best model\n","Epoch 34, Loss 80513976694.00, Val loss 23459379200.00\n","Epoch 35, Loss 58481727198.00, Val loss 51279216640.00\n","Epoch 36, Loss 78068520546.00, Val loss 37968363520.00\n","Epoch 37, Loss 64855489204.00, Val loss 53887901696.00\n","Epoch 38, Loss 81594377728.00, Val loss 31519551488.00\n","Epoch 39, Loss 60111852890.00, Val loss 54879031296.00\n","Epoch 40, Loss 82485322716.00, Val loss 41899319296.00\n","Epoch 41, Loss 69632080768.00, Val loss 49072070656.00\n","Epoch 42, Loss 77677967292.00, Val loss 33387542528.00\n","Epoch 43, Loss 62470838540.00, Val loss 55179853824.00\n","Epoch 44, Loss 84868957690.00, Val loss 29954324480.00\n","Epoch 45, Loss 60273981436.00, Val loss 54232018944.00\n","Epoch 46, Loss 83697083552.00, Val loss 40538054656.00\n","Epoch 47, Loss 70122759052.00, Val loss 46949072896.00\n","Epoch 48, Loss 77093748874.00, Val loss 29425205248.00\n","Epoch 49, Loss 60195957920.00, Val loss 54573150208.00\n","Epoch 50, Loss 84837973350.00, Val loss 29061068800.00\n","Epoch 51, Loss 59707593500.00, Val loss 54640951296.00\n","Epoch 52, Loss 84987358312.00, Val loss 25059176448.00\n","Epoch 53, Loss 56848282814.00, Val loss 53303967744.00\n","Epoch 54, Loss 82557467902.00, Val loss 35337764864.00\n","Epoch 55, Loss 64633328056.00, Val loss 48701194240.00\n","==================================================================\n","Saved best model\n","Epoch 56, Loss 79221694204.00, Val loss 17718538240.00\n","Epoch 57, Loss 55561834124.00, Val loss 53801508864.00\n","Epoch 58, Loss 83601838872.00, Val loss 19409408000.00\n","Epoch 59, Loss 54250307832.00, Val loss 53744521216.00\n","Epoch 60, Loss 82661329566.00, Val loss 22440501248.00\n","Epoch 61, Loss 54187238362.00, Val loss 54411587584.00\n","Epoch 62, Loss 83738649762.00, Val loss 18052587520.00\n","Epoch 63, Loss 53791985765.00, Val loss 54326587392.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 84166594104.00, Val loss 15362685952.00\n","Epoch 65, Loss 55240205248.00, Val loss 52308197376.00\n","Epoch 66, Loss 72670222200.00, Val loss 17689786368.00\n","Epoch 67, Loss 78410160902.00, Val loss 16353968128.00\n","Epoch 68, Loss 51200258259.00, Val loss 19188234240.00\n","==================================================================\n","Saved best model\n","Epoch 69, Loss 49826688216.00, Val loss 14558158848.00\n","Epoch 70, Loss 54887330389.00, Val loss 45089169408.00\n","==================================================================\n","Saved best model\n","Epoch 71, Loss 73713813584.00, Val loss 14029965312.00\n","Epoch 72, Loss 54713086718.00, Val loss 38677598208.00\n","Epoch 73, Loss 65245433970.00, Val loss 16344178688.00\n","Epoch 74, Loss 53424572984.00, Val loss 53560471552.00\n","Epoch 75, Loss 84844564288.00, Val loss 18825623552.00\n","Epoch 76, Loss 50774382231.00, Val loss 24589221888.00\n","Epoch 77, Loss 54900106980.00, Val loss 48951754752.00\n","Epoch 78, Loss 79779895670.00, Val loss 17530253312.00\n","Epoch 79, Loss 51698082663.00, Val loss 25384882176.00\n","Epoch 80, Loss 55986285968.00, Val loss 44311990272.00\n","Epoch 81, Loss 74687127232.00, Val loss 14801337344.00\n","Epoch 82, Loss 53958480596.00, Val loss 26883975168.00\n","Epoch 83, Loss 57263295958.00, Val loss 35368931328.00\n","==================================================================\n","Saved best model\n","Epoch 84, Loss 64241658836.00, Val loss 13929004032.00\n","Epoch 85, Loss 56663911539.00, Val loss 44655702016.00\n","Epoch 86, Loss 76699602012.00, Val loss 19082332160.00\n","Epoch 87, Loss 52223530699.00, Val loss 14443331584.00\n","Epoch 88, Loss 56454287431.00, Val loss 42847469568.00\n","Epoch 89, Loss 75018867072.00, Val loss 19183704064.00\n","==================================================================\n","Saved best model\n","Epoch 90, Loss 52576904840.00, Val loss 12144909312.00\n","Epoch 91, Loss 55541666107.50, Val loss 14504621056.00\n","Epoch 92, Loss 56367054813.00, Val loss 51959402496.00\n","Epoch 93, Loss 82980649998.00, Val loss 12523814912.00\n","Epoch 94, Loss 56556726160.00, Val loss 14367880192.00\n","Epoch 95, Loss 53460785764.00, Val loss 17866387456.00\n","Epoch 96, Loss 56831094123.00, Val loss 14448173056.00\n","Epoch 97, Loss 57902392646.00, Val loss 13314092032.00\n","Epoch 98, Loss 57951450506.00, Val loss 13838879744.00\n","Epoch 99, Loss 56716254684.00, Val loss 17781495808.00\n","==================================================================\n","Saved best model\n","Epoch 100, Loss 59673114083.00, Val loss 7178471936.00\n","Epoch 101, Loss 63574235634.50, Val loss 18002710528.00\n","Epoch 102, Loss 57044861338.00, Val loss 16320161792.00\n","Epoch 103, Loss 57455579642.00, Val loss 11487221760.00\n","Epoch 104, Loss 60157255364.00, Val loss 11535239168.00\n","Epoch 105, Loss 59995140896.00, Val loss 12327007232.00\n","Epoch 106, Loss 59250469890.00, Val loss 13460497408.00\n","Epoch 107, Loss 58516434483.00, Val loss 14488660992.00\n","Epoch 108, Loss 58067056169.00, Val loss 14856353792.00\n","Epoch 109, Loss 57897185388.00, Val loss 14647709696.00\n","Epoch 110, Loss 57858466351.00, Val loss 14224324608.00\n","Epoch 111, Loss 57882183309.00, Val loss 13705884672.00\n","Epoch 112, Loss 57966257115.00, Val loss 13111021568.00\n","Epoch 113, Loss 58133774881.00, Val loss 12435397632.00\n","Epoch 114, Loss 57460056617.00, Val loss 15753007104.00\n","Epoch 115, Loss 57785713424.00, Val loss 15084299264.00\n","==================================================================\n","Saved best model\n","Epoch 116, Loss 59204301174.00, Val loss 3544581376.00\n","Epoch 117, Loss 76536956231.00, Val loss 24961486848.00\n","Epoch 118, Loss 67443306648.00, Val loss 24791687168.00\n","Epoch 119, Loss 65636028960.00, Val loss 23004710912.00\n","Epoch 120, Loss 62585692296.00, Val loss 23000145920.00\n","Epoch 121, Loss 62705182330.00, Val loss 24118372352.00\n","Epoch 122, Loss 63172777054.00, Val loss 23278139392.00\n","Epoch 123, Loss 62005522414.00, Val loss 21894533120.00\n","Epoch 124, Loss 60673621972.00, Val loss 20871696384.00\n","Epoch 125, Loss 59881704294.00, Val loss 20300984320.00\n","Epoch 126, Loss 59384713602.00, Val loss 19975858176.00\n","Epoch 127, Loss 58992309040.00, Val loss 19782410240.00\n","Epoch 128, Loss 58645536028.00, Val loss 19661686784.00\n","Epoch 129, Loss 58323456556.00, Val loss 19581640704.00\n","Epoch 130, Loss 58017207258.00, Val loss 19525814272.00\n","Epoch 131, Loss 57723418082.00, Val loss 19483054080.00\n","Epoch 132, Loss 57439061042.00, Val loss 19450320896.00\n","Epoch 133, Loss 57163149936.00, Val loss 19423815680.00\n","Epoch 134, Loss 56912398464.00, Val loss 19401089024.00\n","Epoch 135, Loss 56678668254.00, Val loss 19382177792.00\n","Epoch 136, Loss 56452778024.00, Val loss 19365666816.00\n","Epoch 137, Loss 56235881216.00, Val loss 19351230464.00\n","Epoch 138, Loss 56027887544.00, Val loss 19339753472.00\n","Epoch 139, Loss 55827833416.00, Val loss 19329828864.00\n","Epoch 140, Loss 55635651844.00, Val loss 19321970688.00\n","Epoch 141, Loss 55452505734.00, Val loss 19316801536.00\n","Epoch 142, Loss 55271091342.00, Val loss 19311601664.00\n","Epoch 143, Loss 55100320123.00, Val loss 19308697600.00\n","Epoch 144, Loss 54935769905.00, Val loss 19308621824.00\n","Epoch 145, Loss 54777918255.00, Val loss 19310780416.00\n","Epoch 146, Loss 54624596505.00, Val loss 19315142656.00\n","Epoch 147, Loss 54477129759.00, Val loss 19320037376.00\n","Epoch 148, Loss 54335896054.00, Val loss 19326386176.00\n","Epoch 149, Loss 54199871982.00, Val loss 19333363712.00\n","Epoch 150, Loss 54069207243.00, Val loss 19341471744.00\n","Epoch 151, Loss 53943930722.00, Val loss 19352651776.00\n","Epoch 152, Loss 53822890532.00, Val loss 19363993600.00\n","Epoch 153, Loss 53708264188.00, Val loss 19376343040.00\n","Epoch 154, Loss 53599679801.00, Val loss 19391840256.00\n","Epoch 155, Loss 53489223426.00, Val loss 19405453312.00\n","Epoch 156, Loss 53385336002.00, Val loss 19419756544.00\n","Epoch 157, Loss 53285640609.00, Val loss 19435323392.00\n","Epoch 158, Loss 53189895183.00, Val loss 19452186624.00\n","Epoch 159, Loss 53097195222.00, Val loss 19469191168.00\n","Epoch 160, Loss 53008384359.00, Val loss 19487279104.00\n","Epoch 161, Loss 52922768529.00, Val loss 19505866752.00\n","Epoch 162, Loss 52840678820.00, Val loss 19524679680.00\n","Epoch 163, Loss 52761487816.00, Val loss 19544377344.00\n","Epoch 164, Loss 52685033501.00, Val loss 19564386304.00\n","Epoch 165, Loss 52611762878.00, Val loss 19583752192.00\n","Epoch 166, Loss 52541718423.00, Val loss 19604105216.00\n","Epoch 167, Loss 52473986006.00, Val loss 19625052160.00\n","Epoch 168, Loss 52408911118.00, Val loss 19646193664.00\n","Epoch 169, Loss 52346519462.00, Val loss 19667015680.00\n","Epoch 170, Loss 52286505759.00, Val loss 19688730624.00\n","Epoch 171, Loss 52228530553.00, Val loss 19710183424.00\n","Epoch 172, Loss 52173310274.00, Val loss 19730946048.00\n","Epoch 173, Loss 52120338378.00, Val loss 19751659520.00\n","Epoch 174, Loss 52069825074.00, Val loss 19772698624.00\n","Epoch 175, Loss 52019067525.00, Val loss 19794503680.00\n","Epoch 176, Loss 51974376276.00, Val loss 19815028736.00\n","Epoch 177, Loss 51929743401.00, Val loss 19835217920.00\n","Epoch 178, Loss 51887370644.00, Val loss 19855509504.00\n","Epoch 179, Loss 51847502296.00, Val loss 19877052416.00\n","Epoch 180, Loss 51807201425.00, Val loss 19896725504.00\n","Epoch 181, Loss 51770384138.00, Val loss 19915902976.00\n","Epoch 182, Loss 51734404608.00, Val loss 19935334400.00\n","Epoch 183, Loss 51700753407.00, Val loss 19954438144.00\n","Epoch 184, Loss 51668765907.00, Val loss 19973599232.00\n","Epoch 185, Loss 51638218947.00, Val loss 19992930304.00\n","Epoch 186, Loss 51609124790.00, Val loss 20011890688.00\n","Epoch 187, Loss 51580756579.00, Val loss 20029360128.00\n","Epoch 188, Loss 51555108462.00, Val loss 20046948352.00\n","Epoch 189, Loss 51529895283.00, Val loss 20064387072.00\n","Epoch 190, Loss 51506555408.00, Val loss 20081508352.00\n","Epoch 191, Loss 51484392446.00, Val loss 20098379776.00\n","Epoch 192, Loss 51463721336.00, Val loss 20114692096.00\n","Epoch 193, Loss 51444507922.00, Val loss 20131463168.00\n","Epoch 194, Loss 51425240096.00, Val loss 20146143232.00\n","Epoch 195, Loss 51408170700.00, Val loss 20161755136.00\n","Epoch 196, Loss 51392098789.00, Val loss 20176349184.00\n","Epoch 197, Loss 51377902471.00, Val loss 20191549440.00\n","Epoch 198, Loss 51363909203.50, Val loss 20205924352.00\n","Epoch 199, Loss 51351427116.50, Val loss 20219895808.00\n","Epoch 200, Loss 51339943878.00, Val loss 20233062400.00\n","Epoch 201, Loss 51328039956.00, Val loss 20246693888.00\n","Epoch 202, Loss 51318243583.50, Val loss 20259121152.00\n","Epoch 203, Loss 51307738408.00, Val loss 20271464448.00\n","Epoch 204, Loss 51301228453.00, Val loss 20282607616.00\n","Epoch 205, Loss 51292740548.50, Val loss 20292626432.00\n","Epoch 206, Loss 51287763439.00, Val loss 20305291264.00\n","Epoch 207, Loss 51282165822.50, Val loss 20315764736.00\n","Epoch 208, Loss 51276124240.50, Val loss 20327141376.00\n","Epoch 209, Loss 51270749492.50, Val loss 20337309696.00\n","Epoch 210, Loss 51266901092.50, Val loss 20346114048.00\n","Epoch 211, Loss 51265187353.00, Val loss 20356421632.00\n","Epoch 212, Loss 51261975540.00, Val loss 20364333056.00\n","Epoch 213, Loss 51261665852.50, Val loss 20372439040.00\n","Epoch 214, Loss 51260637863.00, Val loss 20380264448.00\n","Epoch 215, Loss 51261911022.00, Val loss 20388907008.00\n","Epoch 216, Loss 51261624033.00, Val loss 20396154880.00\n","Epoch 217, Loss 51263888175.50, Val loss 20402542592.00\n","Epoch 218, Loss 51265556465.00, Val loss 20408776704.00\n","Epoch 219, Loss 51268625581.00, Val loss 20415166464.00\n","Epoch 220, Loss 51271375275.00, Val loss 20420276224.00\n","Epoch 221, Loss 51275650515.00, Val loss 20425273344.00\n","Epoch 222, Loss 51279842949.50, Val loss 20430598144.00\n","Epoch 223, Loss 51283763900.00, Val loss 20434780160.00\n","Epoch 224, Loss 51289330359.50, Val loss 20438814720.00\n","Epoch 225, Loss 51294768056.00, Val loss 20442853376.00\n","Epoch 226, Loss 51300477931.00, Val loss 20446357504.00\n","Epoch 227, Loss 51306244961.00, Val loss 20448657408.00\n","Epoch 228, Loss 51313791922.50, Val loss 20451883008.00\n","Epoch 229, Loss 51322052732.50, Val loss 20453199872.00\n","Epoch 230, Loss 51327808253.00, Val loss 20454002688.00\n","Epoch 231, Loss 51334618676.00, Val loss 20454971392.00\n","Epoch 232, Loss 51343414467.00, Val loss 20456740864.00\n","Epoch 233, Loss 51350743771.00, Val loss 20458528768.00\n","Epoch 234, Loss 51358189007.50, Val loss 20460916736.00\n","Epoch 235, Loss 51365631389.00, Val loss 20461725696.00\n","Epoch 236, Loss 51374420205.50, Val loss 20462927872.00\n","Epoch 237, Loss 51382726433.50, Val loss 20461996032.00\n","Epoch 238, Loss 51393067407.50, Val loss 20462233600.00\n","Epoch 239, Loss 51402182508.00, Val loss 20462008320.00\n","Epoch 240, Loss 51412060645.00, Val loss 20460810240.00\n","Epoch 241, Loss 51422648396.00, Val loss 20459147264.00\n","Epoch 242, Loss 51433228114.50, Val loss 20458555392.00\n","Epoch 243, Loss 51443370568.50, Val loss 20456687616.00\n","Epoch 244, Loss 51453610315.00, Val loss 20454289408.00\n","Epoch 245, Loss 51464159010.00, Val loss 20451514368.00\n","Epoch 246, Loss 51474409943.00, Val loss 20448438272.00\n","Epoch 247, Loss 51486578924.00, Val loss 20446154752.00\n","Epoch 248, Loss 51497141306.00, Val loss 20442886144.00\n","Epoch 249, Loss 51508957606.00, Val loss 20439361536.00\n","Epoch 250, Loss 51520636256.00, Val loss 20435574784.00\n","Epoch 251, Loss 51532730218.00, Val loss 20432025600.00\n","Epoch 252, Loss 51544989770.50, Val loss 20427835392.00\n","Epoch 253, Loss 51557796262.50, Val loss 20423456768.00\n","Epoch 254, Loss 51568573834.50, Val loss 20419344384.00\n","Epoch 255, Loss 51581168456.00, Val loss 20414699520.00\n","Epoch 256, Loss 51592755197.00, Val loss 20409507840.00\n","Epoch 257, Loss 51605633750.50, Val loss 20404514816.00\n","Epoch 258, Loss 51617835023.50, Val loss 20398737408.00\n","Epoch 259, Loss 51630918582.50, Val loss 20392892416.00\n","Epoch 260, Loss 51643775647.50, Val loss 20386930688.00\n","Epoch 261, Loss 51653332884.50, Val loss 20382173184.00\n","Epoch 262, Loss 51670075767.00, Val loss 20374439936.00\n","Epoch 263, Loss 51684457872.00, Val loss 20367486976.00\n","Epoch 264, Loss 51697793225.50, Val loss 20359706624.00\n","Epoch 265, Loss 51711678014.00, Val loss 20352540672.00\n","Epoch 266, Loss 51725310427.50, Val loss 20345671680.00\n","Epoch 267, Loss 51738717015.00, Val loss 20338726912.00\n","Epoch 268, Loss 51751820167.00, Val loss 20331327488.00\n","Epoch 269, Loss 51764359770.00, Val loss 20323975168.00\n","Epoch 270, Loss 51777802149.00, Val loss 20315506688.00\n","Epoch 271, Loss 51791877818.00, Val loss 20308254720.00\n","Epoch 272, Loss 51804353907.50, Val loss 20299878400.00\n","Epoch 273, Loss 51818557071.50, Val loss 20291121152.00\n","Epoch 274, Loss 51832431605.00, Val loss 20282548224.00\n","Epoch 275, Loss 51845523726.00, Val loss 20273412096.00\n","Epoch 276, Loss 51859282413.50, Val loss 20263254016.00\n","Epoch 277, Loss 51873876351.00, Val loss 20253517824.00\n","Epoch 278, Loss 51888332052.50, Val loss 20243388416.00\n","Epoch 279, Loss 51902708872.50, Val loss 20233500672.00\n","Epoch 280, Loss 51916743396.00, Val loss 20223080448.00\n","Epoch 281, Loss 51931405742.00, Val loss 20213680128.00\n","Epoch 282, Loss 51944742899.50, Val loss 20202713088.00\n","Epoch 283, Loss 51958618862.50, Val loss 20191100928.00\n","Epoch 284, Loss 51974109683.00, Val loss 20180477952.00\n","Epoch 285, Loss 51988333223.50, Val loss 20169932800.00\n","Epoch 286, Loss 52002190715.00, Val loss 20159307776.00\n","Epoch 287, Loss 52016339662.50, Val loss 20148273152.00\n","Epoch 288, Loss 52030598844.00, Val loss 20137152512.00\n","Epoch 289, Loss 52045134326.50, Val loss 20127166464.00\n","Epoch 290, Loss 52058378997.50, Val loss 20116348928.00\n","Epoch 291, Loss 52073158876.50, Val loss 20105463808.00\n","Epoch 292, Loss 52087431249.00, Val loss 20096987136.00\n","Epoch 293, Loss 52095899189.00, Val loss 20083232768.00\n","Epoch 294, Loss 52111886788.00, Val loss 20069410816.00\n","Epoch 295, Loss 52127125291.50, Val loss 20056215552.00\n","Epoch 296, Loss 52142356521.50, Val loss 20044371968.00\n","Epoch 297, Loss 52155868339.50, Val loss 20032172032.00\n","Epoch 298, Loss 52169767045.50, Val loss 20018571264.00\n","Epoch 299, Loss 52185041697.00, Val loss 20005814272.00\n","Epoch 300, Loss 52199954094.50, Val loss 19994138624.00\n","Epoch 301, Loss 52213269351.00, Val loss 19981721600.00\n","Epoch 302, Loss 52227178253.00, Val loss 19969169408.00\n","Epoch 303, Loss 52241303970.00, Val loss 19955408896.00\n","Epoch 304, Loss 52256093748.50, Val loss 19942498304.00\n","Epoch 305, Loss 52270088196.50, Val loss 19929456640.00\n","Epoch 306, Loss 52284508570.00, Val loss 19915829248.00\n","Epoch 307, Loss 52298723211.50, Val loss 19902373888.00\n","Epoch 308, Loss 52313069476.50, Val loss 19888494592.00\n","Epoch 309, Loss 52328083011.50, Val loss 19875708928.00\n","Epoch 310, Loss 52341605293.00, Val loss 19861999616.00\n","Epoch 311, Loss 52355850642.00, Val loss 19847790592.00\n","Epoch 312, Loss 52370645902.00, Val loss 19834486784.00\n","Epoch 313, Loss 52384710420.50, Val loss 19820851200.00\n","Epoch 314, Loss 52398509562.50, Val loss 19806984192.00\n","Epoch 315, Loss 52413018462.50, Val loss 19792535552.00\n","Epoch 316, Loss 52427735484.00, Val loss 19779098624.00\n","Epoch 317, Loss 52441263002.00, Val loss 19765055488.00\n","Epoch 318, Loss 52455529606.00, Val loss 19749937152.00\n","Epoch 319, Loss 51627384063.00, Val loss 19688665088.00\n","Epoch 320, Loss 52538325666.50, Val loss 19715072000.00\n","Epoch 321, Loss 52513916845.50, Val loss 19714322432.00\n","Epoch 322, Loss 52516313339.00, Val loss 19704907776.00\n","Epoch 323, Loss 52525506796.00, Val loss 19691833344.00\n","Epoch 324, Loss 52538905408.50, Val loss 19677700096.00\n","Epoch 325, Loss 52552711739.00, Val loss 19663302656.00\n","Epoch 326, Loss 52566594187.50, Val loss 19648792576.00\n","Epoch 327, Loss 52580726429.50, Val loss 19634280448.00\n","Epoch 328, Loss 52594385582.50, Val loss 19619463168.00\n","Epoch 329, Loss 52608155424.00, Val loss 19603861504.00\n","Epoch 330, Loss 52623141827.50, Val loss 19589748736.00\n","Epoch 331, Loss 52636559158.00, Val loss 19574388736.00\n","Epoch 332, Loss 52650841520.50, Val loss 19559970816.00\n","Epoch 333, Loss 52664156807.00, Val loss 19544064000.00\n","Epoch 334, Loss 52678861698.00, Val loss 19529052160.00\n","Epoch 335, Loss 52692743935.00, Val loss 19512549376.00\n","Epoch 336, Loss 52707934310.50, Val loss 19497525248.00\n","Epoch 337, Loss 52722078806.50, Val loss 19483283456.00\n","Epoch 338, Loss 52734850743.00, Val loss 19468851200.00\n","Epoch 339, Loss 52747933065.50, Val loss 19452608512.00\n","Epoch 340, Loss 52762791929.00, Val loss 19436578816.00\n","Epoch 341, Loss 52777396843.50, Val loss 19422179328.00\n","Epoch 342, Loss 52790369860.50, Val loss 19406366720.00\n","Epoch 343, Loss 52804716940.00, Val loss 19392215040.00\n","Epoch 344, Loss 52817531475.00, Val loss 19376656384.00\n","Epoch 345, Loss 52831336548.00, Val loss 19361163264.00\n","Epoch 346, Loss 52845006916.00, Val loss 19345872896.00\n","Epoch 347, Loss 52858635973.00, Val loss 19330129920.00\n","Epoch 348, Loss 52872124121.50, Val loss 19314114560.00\n","Epoch 349, Loss 52886674488.50, Val loss 19298580480.00\n","Epoch 350, Loss 52899139912.50, Val loss 19283503104.00\n","Epoch 351, Loss 52912077454.50, Val loss 19267909632.00\n","Epoch 352, Loss 52925969810.50, Val loss 19252463616.00\n","Epoch 353, Loss 52939018973.50, Val loss 19236798464.00\n","Epoch 354, Loss 52952364120.00, Val loss 19222265856.00\n","Epoch 355, Loss 52965396786.00, Val loss 19206463488.00\n","Epoch 356, Loss 52978611930.00, Val loss 19190614016.00\n","Epoch 357, Loss 52992660933.50, Val loss 19175624704.00\n","Epoch 358, Loss 53005864115.50, Val loss 19160508416.00\n","Epoch 359, Loss 53018965836.00, Val loss 19145781248.00\n","Epoch 360, Loss 53031160711.00, Val loss 19129968640.00\n","Epoch 361, Loss 53045043973.00, Val loss 19113664512.00\n","Epoch 362, Loss 53059337647.50, Val loss 19099039744.00\n","Epoch 363, Loss 53071472610.00, Val loss 19084595200.00\n","Epoch 364, Loss 53083874967.50, Val loss 19066654720.00\n","Epoch 365, Loss 53099083555.50, Val loss 19053985792.00\n","Epoch 366, Loss 53109754106.00, Val loss 19038138368.00\n","Epoch 367, Loss 53123853045.50, Val loss 19022000128.00\n","Epoch 368, Loss 53137168715.50, Val loss 19008032768.00\n","Epoch 369, Loss 53149396074.50, Val loss 18992357376.00\n","Epoch 370, Loss 53162242975.00, Val loss 18977552384.00\n","Epoch 371, Loss 53174244056.50, Val loss 18961827840.00\n","Epoch 372, Loss 53188643133.00, Val loss 18946476032.00\n","Epoch 373, Loss 53201087993.00, Val loss 18932375552.00\n","Epoch 374, Loss 53212507366.50, Val loss 18917378048.00\n","Epoch 375, Loss 53226028969.50, Val loss 18901432320.00\n","Epoch 376, Loss 53239281415.50, Val loss 18887825408.00\n","Epoch 377, Loss 53250545294.50, Val loss 18872672256.00\n","Epoch 378, Loss 53262754203.50, Val loss 18856876032.00\n","Epoch 379, Loss 53276491753.00, Val loss 18842191872.00\n","Epoch 380, Loss 53288852192.00, Val loss 18827206656.00\n","Epoch 381, Loss 53301144430.00, Val loss 18812215296.00\n","Epoch 382, Loss 53313486797.00, Val loss 18797258752.00\n","Epoch 383, Loss 53325787044.00, Val loss 18782062592.00\n","Epoch 384, Loss 53338609081.00, Val loss 18766899200.00\n","Epoch 385, Loss 53351541789.00, Val loss 18753204224.00\n","Epoch 386, Loss 53362377545.00, Val loss 18738948096.00\n","Epoch 387, Loss 53373931478.00, Val loss 18722781184.00\n","Epoch 388, Loss 53387605776.00, Val loss 18708533248.00\n","Epoch 389, Loss 53399112623.00, Val loss 18694631424.00\n","Epoch 390, Loss 53410822428.00, Val loss 18679173120.00\n","Epoch 391, Loss 53422859524.00, Val loss 18664595456.00\n","Epoch 392, Loss 53435787249.50, Val loss 18649284608.00\n","Epoch 393, Loss 53449407993.50, Val loss 18632003584.00\n","Epoch 394, Loss 53461298905.00, Val loss 18623043584.00\n","Epoch 395, Loss 53469080787.00, Val loss 18607867904.00\n","Epoch 396, Loss 53484065900.00, Val loss 18588946432.00\n","Epoch 397, Loss 53495970125.00, Val loss 18580535296.00\n","Epoch 398, Loss 53503313537.00, Val loss 18564722688.00\n","Epoch 399, Loss 53517681317.50, Val loss 18549071872.00\n","Epoch 400, Loss 53530106552.00, Val loss 18536390656.00\n","Epoch 401, Loss 53538893233.50, Val loss 18522560512.00\n","Epoch 402, Loss 53549695727.50, Val loss 18508480512.00\n","Epoch 403, Loss 53561224623.50, Val loss 18494545920.00\n","Epoch 404, Loss 53572137540.00, Val loss 18479904768.00\n","Epoch 405, Loss 53584109594.50, Val loss 18466430976.00\n","Epoch 406, Loss 53594889373.00, Val loss 18451888128.00\n","Epoch 407, Loss 53606918188.50, Val loss 18437230592.00\n","Epoch 408, Loss 53618939449.50, Val loss 18424045568.00\n","Epoch 409, Loss 53629412749.50, Val loss 18409867264.00\n","Epoch 410, Loss 53640778486.50, Val loss 18396364800.00\n","Epoch 411, Loss 53651580875.50, Val loss 18381946880.00\n","Epoch 412, Loss 53663584814.50, Val loss 18367551488.00\n","Epoch 413, Loss 53674797518.00, Val loss 18354460672.00\n","Epoch 414, Loss 53684508578.00, Val loss 18340141056.00\n","Epoch 415, Loss 53696044543.50, Val loss 18326394880.00\n","Epoch 416, Loss 53706023542.50, Val loss 18312349696.00\n","Epoch 417, Loss 53716161027.50, Val loss 18297724928.00\n","Epoch 418, Loss 53727834592.00, Val loss 18283300864.00\n","Epoch 419, Loss 53739889725.00, Val loss 18269425664.00\n","Epoch 420, Loss 53750597809.00, Val loss 18256852992.00\n","Epoch 421, Loss 53760481714.50, Val loss 18242336768.00\n","Epoch 422, Loss 53772202608.50, Val loss 18229116928.00\n","Epoch 423, Loss 53782540507.50, Val loss 18215487488.00\n","Epoch 424, Loss 53793647129.00, Val loss 18200977408.00\n","Epoch 425, Loss 53804646225.50, Val loss 18188009472.00\n","Epoch 426, Loss 53814814422.00, Val loss 18173900800.00\n","Epoch 427, Loss 53826097910.00, Val loss 18160584704.00\n","Epoch 428, Loss 53831224014.00, Val loss 18147637248.00\n","Epoch 429, Loss 53853316589.00, Val loss 18128861184.00\n","Epoch 430, Loss 53865962069.00, Val loss 18117459968.00\n","Epoch 431, Loss 53872137265.00, Val loss 18105919488.00\n","Epoch 432, Loss 53881262489.00, Val loss 18092199936.00\n","Epoch 433, Loss 53891612654.50, Val loss 18079174656.00\n","Epoch 434, Loss 53901688600.00, Val loss 18065113088.00\n","Epoch 435, Loss 53912455969.00, Val loss 18051846144.00\n","Epoch 436, Loss 53922940917.00, Val loss 18038376448.00\n","Epoch 437, Loss 53933726292.50, Val loss 18026827776.00\n","Epoch 438, Loss 53943658584.00, Val loss 18013716480.00\n","Epoch 439, Loss 53954016484.00, Val loss 18002186240.00\n","Epoch 440, Loss 53962816728.00, Val loss 17989859328.00\n","Epoch 441, Loss 53972036903.00, Val loss 17977780224.00\n","Epoch 442, Loss 53980813929.00, Val loss 17966401536.00\n","Epoch 443, Loss 53989168404.50, Val loss 17953558528.00\n","Epoch 444, Loss 53999272955.50, Val loss 17941610496.00\n","Epoch 445, Loss 54008381584.00, Val loss 17929691136.00\n","Epoch 446, Loss 54017280219.00, Val loss 17916504064.00\n","Epoch 447, Loss 54027864154.00, Val loss 17903751168.00\n","Epoch 448, Loss 54037805465.00, Val loss 17892163584.00\n","Epoch 449, Loss 54046271560.50, Val loss 17879064576.00\n","Epoch 450, Loss 54057154133.00, Val loss 17866352640.00\n","Epoch 451, Loss 54066985900.00, Val loss 17854742528.00\n","Epoch 452, Loss 54075963397.50, Val loss 17842022400.00\n","Epoch 453, Loss 54085840033.50, Val loss 17830414336.00\n","Epoch 454, Loss 54094319844.50, Val loss 17817581568.00\n","Epoch 455, Loss 54104621796.50, Val loss 17805490176.00\n","Epoch 456, Loss 54113310150.50, Val loss 17794326528.00\n","Epoch 457, Loss 54119971399.50, Val loss 17781676032.00\n","Epoch 458, Loss 54128721970.00, Val loss 17769918464.00\n","Epoch 459, Loss 54136227175.00, Val loss 17757820928.00\n","Epoch 460, Loss 54161128206.50, Val loss 17740429312.00\n","Epoch 461, Loss 54167537637.00, Val loss 17731065856.00\n","Epoch 462, Loss 54173921961.00, Val loss 17720645632.00\n","Epoch 463, Loss 54181604708.50, Val loss 17709789184.00\n","Epoch 464, Loss 54189715654.50, Val loss 17699121152.00\n","Epoch 465, Loss 54197054873.50, Val loss 17686333440.00\n","Epoch 466, Loss 54206545983.00, Val loss 17674600448.00\n","Epoch 467, Loss 54214349480.50, Val loss 17661542400.00\n","Epoch 468, Loss 54223322690.00, Val loss 17650944000.00\n","Epoch 469, Loss 54230824381.00, Val loss 17636413440.00\n","Epoch 470, Loss 54243361617.50, Val loss 17624432640.00\n","Epoch 471, Loss 54252724403.00, Val loss 17612548096.00\n","Epoch 472, Loss 54261413036.00, Val loss 17600479232.00\n","Epoch 473, Loss 54270582926.00, Val loss 17589266432.00\n","Epoch 474, Loss 54279053767.50, Val loss 17577545728.00\n","Epoch 475, Loss 54287746239.00, Val loss 17565667328.00\n","Epoch 476, Loss 54297338604.00, Val loss 17553930240.00\n","Epoch 477, Loss 54305859761.00, Val loss 17542764544.00\n","Epoch 478, Loss 54315005790.50, Val loss 17530306560.00\n","Epoch 479, Loss 54324585554.50, Val loss 17521395712.00\n","Epoch 480, Loss 54330284491.00, Val loss 17509341184.00\n","Epoch 481, Loss 54339752685.50, Val loss 17498531840.00\n","Epoch 482, Loss 54348248255.50, Val loss 17487360000.00\n","Epoch 483, Loss 54356298131.00, Val loss 17476110336.00\n","Epoch 484, Loss 54364747580.00, Val loss 17464862720.00\n","Epoch 485, Loss 54373751488.00, Val loss 17454166016.00\n","Epoch 486, Loss 54381953398.50, Val loss 17442488320.00\n","Epoch 487, Loss 54390721528.00, Val loss 17433622528.00\n","Epoch 488, Loss 54397003569.50, Val loss 17422440448.00\n","Epoch 489, Loss 54405688383.50, Val loss 17411303424.00\n","Epoch 490, Loss 54413664082.00, Val loss 17401120768.00\n","Epoch 491, Loss 54421639121.50, Val loss 17390004224.00\n","Epoch 492, Loss 54430338261.50, Val loss 17379311616.00\n","Epoch 493, Loss 54438145541.00, Val loss 17369913344.00\n","Epoch 494, Loss 54444705297.50, Val loss 17359050752.00\n","Epoch 495, Loss 54453444757.00, Val loss 17348128768.00\n","Epoch 496, Loss 54461396670.00, Val loss 17338640384.00\n","Epoch 497, Loss 54468086427.00, Val loss 17327337472.00\n","Epoch 498, Loss 54477283838.00, Val loss 17316999168.00\n","Epoch 499, Loss 54485087559.50, Val loss 17308045312.00\n","Epoch 500, Loss 54491581384.50, Val loss 17297856512.00\n","Epoch 501, Loss 54499082711.50, Val loss 17288179712.00\n","Epoch 502, Loss 54506067622.00, Val loss 17278201856.00\n","Epoch 503, Loss 54513574716.50, Val loss 17268017152.00\n","Epoch 504, Loss 54520514544.50, Val loss 17257095168.00\n","Epoch 505, Loss 54529847188.00, Val loss 17247211520.00\n","Epoch 506, Loss 54537527673.50, Val loss 17238956032.00\n","Epoch 507, Loss 54543011671.50, Val loss 17229811712.00\n","Epoch 508, Loss 54550227571.50, Val loss 17222010880.00\n","Epoch 509, Loss 54553716472.50, Val loss 17211846656.00\n","Epoch 510, Loss 54562635160.50, Val loss 17201465344.00\n","Epoch 511, Loss 54569917868.00, Val loss 17192409088.00\n","Epoch 512, Loss 54576816293.00, Val loss 17182953472.00\n","Epoch 513, Loss 54583331868.50, Val loss 17174292480.00\n","Epoch 514, Loss 54589287917.50, Val loss 17163844608.00\n","Epoch 515, Loss 54596767417.00, Val loss 17153931264.00\n","Epoch 516, Loss 54604569638.00, Val loss 17144623104.00\n","Epoch 517, Loss 54611534097.50, Val loss 17136736256.00\n","Epoch 518, Loss 54617167332.00, Val loss 17128072192.00\n","Epoch 519, Loss 54623405283.50, Val loss 17120190464.00\n","Epoch 520, Loss 54628942670.00, Val loss 17111744512.00\n","Epoch 521, Loss 54634839323.00, Val loss 17103528960.00\n","Epoch 522, Loss 54640999323.00, Val loss 17095768064.00\n","Epoch 523, Loss 54645532301.50, Val loss 17087630336.00\n","Epoch 524, Loss 54651423132.50, Val loss 17079361536.00\n","Epoch 525, Loss 54656479556.00, Val loss 17070470144.00\n","Epoch 526, Loss 54663245490.00, Val loss 17061667840.00\n","Epoch 527, Loss 54669766941.50, Val loss 17053394944.00\n","Epoch 528, Loss 54675657493.00, Val loss 17044877312.00\n","Epoch 529, Loss 54681534084.00, Val loss 17035951104.00\n","Epoch 530, Loss 54687843785.00, Val loss 17026989056.00\n","Epoch 531, Loss 54694342712.00, Val loss 17018428416.00\n","Epoch 532, Loss 54700384299.00, Val loss 17009102848.00\n","Epoch 533, Loss 54707129846.50, Val loss 17000039424.00\n","Epoch 534, Loss 54713866483.50, Val loss 16990368768.00\n","Epoch 535, Loss 54719704122.50, Val loss 16981241856.00\n","Epoch 536, Loss 54726783885.00, Val loss 16971401216.00\n","Epoch 537, Loss 54734543803.00, Val loss 16962894848.00\n","Epoch 538, Loss 54740269898.50, Val loss 16954473472.00\n","Epoch 539, Loss 54746456341.50, Val loss 16945207296.00\n","Epoch 540, Loss 54753655308.00, Val loss 16937047040.00\n","Epoch 541, Loss 54759573120.00, Val loss 16928271360.00\n","Epoch 542, Loss 54766090437.00, Val loss 16920505344.00\n","Epoch 543, Loss 54771398476.50, Val loss 16912342016.00\n","Epoch 544, Loss 54776749463.50, Val loss 16903754752.00\n","Epoch 545, Loss 54783226278.00, Val loss 16894735360.00\n","Epoch 546, Loss 54789851912.00, Val loss 16886352896.00\n","Epoch 547, Loss 54796253274.00, Val loss 16878662656.00\n","Epoch 548, Loss 54801701949.00, Val loss 16870849536.00\n","Epoch 549, Loss 54807364483.00, Val loss 16862545920.00\n","Epoch 550, Loss 54813445070.00, Val loss 16854922240.00\n","Epoch 551, Loss 54818160983.50, Val loss 16846980096.00\n","Epoch 552, Loss 54824266370.00, Val loss 16838174720.00\n","Epoch 553, Loss 54830715790.00, Val loss 16831453184.00\n","Epoch 554, Loss 54834828592.00, Val loss 16822969344.00\n","Epoch 555, Loss 54841378137.50, Val loss 16815139840.00\n","Epoch 556, Loss 54846540299.50, Val loss 16807606272.00\n","Epoch 557, Loss 54852007857.50, Val loss 16799323136.00\n","Epoch 558, Loss 54858111295.50, Val loss 16792011776.00\n","Epoch 559, Loss 54863064729.50, Val loss 16784811008.00\n","Epoch 560, Loss 54867885348.50, Val loss 16776591360.00\n","Epoch 561, Loss 54874128760.00, Val loss 16770264064.00\n","Epoch 562, Loss 54877654603.50, Val loss 16763095040.00\n","Epoch 563, Loss 54882551485.50, Val loss 16754654208.00\n","Epoch 564, Loss 54888819336.00, Val loss 16747520000.00\n","Epoch 565, Loss 54893799563.50, Val loss 16740111360.00\n","Epoch 566, Loss 54898987646.50, Val loss 16732912640.00\n","Epoch 567, Loss 54904006797.00, Val loss 16725500928.00\n","Epoch 568, Loss 54908410774.00, Val loss 16717592576.00\n","Epoch 569, Loss 54914232401.00, Val loss 16709328896.00\n","Epoch 570, Loss 54920682800.00, Val loss 16701816832.00\n","Epoch 571, Loss 54926223635.00, Val loss 16694477824.00\n","Epoch 572, Loss 54931157731.00, Val loss 16687420416.00\n","Epoch 573, Loss 54935982380.00, Val loss 16680082432.00\n","Epoch 574, Loss 54941369852.00, Val loss 16672560128.00\n","Epoch 575, Loss 54946639667.00, Val loss 16665808896.00\n","Epoch 576, Loss 54951063080.50, Val loss 16657820672.00\n","Epoch 577, Loss 54957196150.50, Val loss 16650754048.00\n","Epoch 578, Loss 54962701332.00, Val loss 16644060160.00\n","Epoch 579, Loss 54966575653.00, Val loss 16637479936.00\n","Epoch 580, Loss 54970707275.50, Val loss 16629283840.00\n","Epoch 581, Loss 54977419740.00, Val loss 16622736384.00\n","Epoch 582, Loss 54981395233.50, Val loss 16615286784.00\n","Epoch 583, Loss 54986812792.50, Val loss 16608037888.00\n","Epoch 584, Loss 54991750515.50, Val loss 16601019392.00\n","Epoch 585, Loss 54996920456.00, Val loss 16593436672.00\n","Epoch 586, Loss 55002589002.00, Val loss 16587014144.00\n","Epoch 587, Loss 55006747437.50, Val loss 16579309568.00\n","Epoch 588, Loss 55012428862.00, Val loss 16572368896.00\n","Epoch 589, Loss 55017335912.00, Val loss 16565121024.00\n","Epoch 590, Loss 55022587114.50, Val loss 16558724096.00\n","Epoch 591, Loss 55026383114.50, Val loss 16551049216.00\n","Epoch 592, Loss 55032722573.50, Val loss 16543797248.00\n","Epoch 593, Loss 55037740681.00, Val loss 16537218048.00\n","Epoch 594, Loss 55042316754.00, Val loss 16530484224.00\n","Epoch 595, Loss 55047155952.50, Val loss 16524067840.00\n","Epoch 596, Loss 55051492007.00, Val loss 16517280768.00\n","Epoch 597, Loss 55056344206.00, Val loss 16511513600.00\n","Epoch 598, Loss 55059648036.50, Val loss 16505625600.00\n","Epoch 599, Loss 55063546598.50, Val loss 16498694144.00\n","Epoch 600, Loss 55067948258.50, Val loss 16492262400.00\n","Epoch 601, Loss 55072600815.00, Val loss 16485843968.00\n","Epoch 602, Loss 55077107351.00, Val loss 16479784960.00\n","Epoch 603, Loss 55080894842.50, Val loss 16473368576.00\n","Epoch 604, Loss 55085279187.00, Val loss 16466868224.00\n","Epoch 605, Loss 55089958049.50, Val loss 16460981248.00\n","Epoch 606, Loss 55093876994.50, Val loss 16455354368.00\n","Epoch 607, Loss 55097100957.00, Val loss 16449088512.00\n","Epoch 608, Loss 55101161599.50, Val loss 16443363328.00\n","Epoch 609, Loss 55104835922.00, Val loss 16436261888.00\n","Epoch 610, Loss 55110108694.50, Val loss 16430795776.00\n","Epoch 611, Loss 55113294143.00, Val loss 16424168448.00\n","Epoch 612, Loss 55117986003.00, Val loss 16418999296.00\n","Epoch 613, Loss 55121161075.50, Val loss 16412553216.00\n","Epoch 614, Loss 55125496777.50, Val loss 16406530048.00\n","Epoch 615, Loss 55129503020.00, Val loss 16400429056.00\n","Epoch 616, Loss 55133593310.00, Val loss 16394823680.00\n","Epoch 617, Loss 55137444053.50, Val loss 16388675584.00\n","Epoch 618, Loss 55141181596.50, Val loss 16382926848.00\n","Epoch 619, Loss 55144938125.50, Val loss 16376450048.00\n","Epoch 620, Loss 55149499049.50, Val loss 16371016704.00\n","Epoch 621, Loss 55153042001.50, Val loss 16364476416.00\n","Epoch 622, Loss 55157775196.50, Val loss 16358862848.00\n","Epoch 623, Loss 55160864942.00, Val loss 16352572416.00\n","Epoch 624, Loss 55165772951.00, Val loss 16347119616.00\n","Epoch 625, Loss 55169195864.00, Val loss 16342194176.00\n","Epoch 626, Loss 55171886733.50, Val loss 16336410624.00\n","Epoch 627, Loss 55175366442.00, Val loss 16330781696.00\n","Epoch 628, Loss 55179440212.00, Val loss 16324922368.00\n","Epoch 629, Loss 55183198452.00, Val loss 16319818752.00\n","Epoch 630, Loss 55185975739.50, Val loss 16314460160.00\n","Epoch 631, Loss 55189404565.50, Val loss 16307938304.00\n","Epoch 632, Loss 55194200293.50, Val loss 16302589952.00\n","Epoch 633, Loss 55197492120.50, Val loss 16296516608.00\n","Epoch 634, Loss 55201723276.50, Val loss 16291695616.00\n","Epoch 635, Loss 55204235861.50, Val loss 16285382656.00\n","Epoch 636, Loss 55208636154.50, Val loss 16280263680.00\n","Epoch 637, Loss 55211653818.00, Val loss 16274149376.00\n","Epoch 638, Loss 55216001586.50, Val loss 16269163520.00\n","Epoch 639, Loss 55218801096.50, Val loss 16263370752.00\n","Epoch 640, Loss 55222641081.50, Val loss 16257215488.00\n","Epoch 641, Loss 55227176669.50, Val loss 16252072960.00\n","Epoch 642, Loss 55230863958.00, Val loss 16247772160.00\n","Epoch 643, Loss 55232073544.00, Val loss 16242778112.00\n","Epoch 644, Loss 55235594103.00, Val loss 16237651968.00\n","Epoch 645, Loss 55238620585.00, Val loss 16232636416.00\n","Epoch 646, Loss 55241429249.00, Val loss 16227786752.00\n","Epoch 647, Loss 55243495912.00, Val loss 16222274560.00\n","Epoch 648, Loss 55247010408.50, Val loss 16217074688.00\n","Epoch 649, Loss 55250414200.50, Val loss 16211990528.00\n","Epoch 650, Loss 55253827076.50, Val loss 16206748672.00\n","Epoch 651, Loss 55256402795.00, Val loss 16201468928.00\n","Epoch 652, Loss 55260652028.00, Val loss 16197507072.00\n","Epoch 653, Loss 55262320839.50, Val loss 16192646144.00\n","Epoch 654, Loss 55265181319.00, Val loss 16188286976.00\n","Epoch 655, Loss 55266691137.00, Val loss 16182003712.00\n","Epoch 656, Loss 55272264249.00, Val loss 16178045952.00\n","Epoch 657, Loss 55273441842.50, Val loss 16172922880.00\n","Epoch 658, Loss 55277078807.50, Val loss 16168219648.00\n","Epoch 659, Loss 55279374595.00, Val loss 16163476480.00\n","Epoch 660, Loss 55282692740.50, Val loss 16157891584.00\n","Epoch 661, Loss 55286135244.50, Val loss 16153794560.00\n","Epoch 662, Loss 55288626277.50, Val loss 16149277696.00\n","Epoch 663, Loss 55290424535.50, Val loss 16143782912.00\n","Epoch 664, Loss 55294704738.00, Val loss 16138638336.00\n","Epoch 665, Loss 55296151798.00, Val loss 16131356672.00\n","Epoch 666, Loss 55303811812.00, Val loss 16126764032.00\n","Epoch 667, Loss 55306676650.50, Val loss 16122943488.00\n","Epoch 668, Loss 55308607022.50, Val loss 16119526400.00\n","Epoch 669, Loss 55309425968.50, Val loss 16114362368.00\n","Epoch 670, Loss 55312885007.00, Val loss 16109643776.00\n","Epoch 671, Loss 55316038320.00, Val loss 16105310208.00\n","Epoch 672, Loss 55318462685.00, Val loss 16100898816.00\n","Epoch 673, Loss 55320632785.50, Val loss 16096265216.00\n","Epoch 674, Loss 55323120751.50, Val loss 16091495424.00\n","Epoch 675, Loss 55326465172.50, Val loss 16085855232.00\n","Epoch 676, Loss 55330356072.50, Val loss 16081433600.00\n","Epoch 677, Loss 55333322514.50, Val loss 16077097984.00\n","Epoch 678, Loss 55335477811.00, Val loss 16072881152.00\n","Epoch 679, Loss 55337760565.00, Val loss 16067630080.00\n","Epoch 680, Loss 55341317869.50, Val loss 16062856192.00\n","Epoch 681, Loss 55344380330.50, Val loss 16058021888.00\n","Epoch 682, Loss 55347028883.00, Val loss 16053811200.00\n","Epoch 683, Loss 55349742836.50, Val loss 16048617472.00\n","Epoch 684, Loss 55353392552.50, Val loss 16044626944.00\n","Epoch 685, Loss 55355154044.50, Val loss 16040242176.00\n","Epoch 686, Loss 55357338855.50, Val loss 16035158016.00\n","Epoch 687, Loss 55361011074.00, Val loss 16030761984.00\n","Epoch 688, Loss 55363794248.00, Val loss 16025714688.00\n","Epoch 689, Loss 55366774424.00, Val loss 16021313536.00\n","Epoch 690, Loss 55369702911.50, Val loss 16016260096.00\n","Epoch 691, Loss 55372746236.00, Val loss 16012478464.00\n","Epoch 692, Loss 55374570771.00, Val loss 16008340480.00\n","Epoch 693, Loss 55376924427.50, Val loss 16003817472.00\n","Epoch 694, Loss 55379579204.50, Val loss 15999427584.00\n","Epoch 695, Loss 55382200154.50, Val loss 15994866688.00\n","Epoch 696, Loss 55385027010.50, Val loss 15990664192.00\n","Epoch 697, Loss 55387209088.00, Val loss 15986356224.00\n","Epoch 698, Loss 55389643616.50, Val loss 15981793280.00\n","Epoch 699, Loss 55392351835.50, Val loss 15977251840.00\n","Epoch 700, Loss 55395328698.50, Val loss 15973409792.00\n","Epoch 701, Loss 55397298870.50, Val loss 15968317440.00\n","Epoch 702, Loss 55400477694.50, Val loss 15964302336.00\n","Epoch 703, Loss 55402453533.00, Val loss 15959249920.00\n","Epoch 704, Loss 55406148183.00, Val loss 15954430976.00\n","Epoch 705, Loss 55409355207.00, Val loss 15950761984.00\n","Epoch 706, Loss 55411073748.50, Val loss 15946365952.00\n","Epoch 707, Loss 55413365134.00, Val loss 15941630976.00\n","Epoch 708, Loss 55416900116.00, Val loss 15936922624.00\n","Epoch 709, Loss 55420007783.50, Val loss 15933873152.00\n","Epoch 710, Loss 55421155253.50, Val loss 15929463808.00\n","Epoch 711, Loss 55423719392.50, Val loss 15925548032.00\n","Epoch 712, Loss 55425580170.00, Val loss 15921263616.00\n","Epoch 713, Loss 55428267510.00, Val loss 15916743680.00\n","Epoch 714, Loss 55430867438.50, Val loss 15912677376.00\n","Epoch 715, Loss 55433731579.50, Val loss 15908153344.00\n","Epoch 716, Loss 55436724166.00, Val loss 15905263616.00\n","Epoch 717, Loss 55437276109.00, Val loss 15901314048.00\n","Epoch 718, Loss 55439527457.00, Val loss 15897096192.00\n","Epoch 719, Loss 55441483078.00, Val loss 15892857856.00\n","Epoch 720, Loss 55444037378.00, Val loss 15887761408.00\n","Epoch 721, Loss 55448154840.00, Val loss 15884165120.00\n","Epoch 722, Loss 55450086708.50, Val loss 15880309760.00\n","Epoch 723, Loss 55451897262.00, Val loss 15876681728.00\n","Epoch 724, Loss 55453570967.50, Val loss 15872300032.00\n","Epoch 725, Loss 55456260984.50, Val loss 15868119040.00\n","Epoch 726, Loss 55459231741.50, Val loss 15864270848.00\n","Epoch 727, Loss 55461236054.00, Val loss 15860658176.00\n","Epoch 728, Loss 55462952278.50, Val loss 15857017856.00\n","Epoch 729, Loss 55464873767.50, Val loss 15852457984.00\n","Epoch 730, Loss 55467631977.00, Val loss 15848217600.00\n","Epoch 731, Loss 55470248876.50, Val loss 15844663296.00\n","Epoch 732, Loss 55471814305.00, Val loss 15840466944.00\n","Epoch 733, Loss 55474177556.00, Val loss 15835942912.00\n","Epoch 734, Loss 55477588900.50, Val loss 15831723008.00\n","Epoch 735, Loss 55479556159.50, Val loss 15827476480.00\n","Epoch 736, Loss 55481969795.50, Val loss 15821714432.00\n","Epoch 737, Loss 55487397158.00, Val loss 15817446400.00\n","Epoch 738, Loss 55490243099.50, Val loss 15813084160.00\n","Epoch 739, Loss 55492662957.50, Val loss 15809028096.00\n","Epoch 740, Loss 55494895936.50, Val loss 15804120064.00\n","Epoch 741, Loss 55498946632.00, Val loss 15799636992.00\n","Epoch 742, Loss 55501783179.00, Val loss 15796953088.00\n","Epoch 743, Loss 55503074370.00, Val loss 15792567296.00\n","Epoch 744, Loss 55506063451.00, Val loss 15789982720.00\n","Epoch 745, Loss 55506218920.00, Val loss 15786306560.00\n","Epoch 746, Loss 55507875468.50, Val loss 15782045696.00\n","Epoch 747, Loss 55510380020.50, Val loss 15777510400.00\n","Epoch 748, Loss 55513511314.00, Val loss 15774460928.00\n","Epoch 749, Loss 55514800031.50, Val loss 15770227712.00\n","Epoch 750, Loss 55517370575.50, Val loss 15766625280.00\n","Epoch 751, Loss 55519393541.50, Val loss 15762563072.00\n","Epoch 752, Loss 55521979457.00, Val loss 15759207424.00\n","Epoch 753, Loss 55523647576.50, Val loss 15756061696.00\n","Epoch 754, Loss 55524896168.50, Val loss 15752724480.00\n","Epoch 755, Loss 55526366418.50, Val loss 15749804032.00\n","Epoch 756, Loss 55527188643.50, Val loss 15745542144.00\n","Epoch 757, Loss 55530115175.00, Val loss 15742341120.00\n","Epoch 758, Loss 55531349072.50, Val loss 15738819584.00\n","Epoch 759, Loss 55533347830.50, Val loss 15735270400.00\n","Epoch 760, Loss 55535278332.50, Val loss 15731473408.00\n","Epoch 761, Loss 55537603960.50, Val loss 15729000448.00\n","Epoch 762, Loss 55537952312.00, Val loss 15725102080.00\n","Epoch 763, Loss 55540116628.50, Val loss 15721837568.00\n","Epoch 764, Loss 55541828399.50, Val loss 15717840896.00\n","Epoch 765, Loss 55544384408.50, Val loss 15714853888.00\n","Epoch 766, Loss 55545265362.50, Val loss 15711411200.00\n","Epoch 767, Loss 55546965555.00, Val loss 15708128256.00\n","Epoch 768, Loss 55548653815.00, Val loss 15703907328.00\n","Epoch 769, Loss 55551745317.50, Val loss 15700960256.00\n","Epoch 770, Loss 55552653196.50, Val loss 15698093056.00\n","Epoch 771, Loss 55553747106.50, Val loss 15694171136.00\n","Epoch 772, Loss 55555805976.50, Val loss 15690827776.00\n","Epoch 773, Loss 55557280940.50, Val loss 15686994944.00\n","Epoch 774, Loss 55559938986.50, Val loss 15683182592.00\n","Epoch 775, Loss 55562409732.00, Val loss 15680211968.00\n","Epoch 776, Loss 55563473614.50, Val loss 15676666880.00\n","Epoch 777, Loss 55565538054.00, Val loss 15673507840.00\n","Epoch 778, Loss 55566744659.50, Val loss 15670641664.00\n","Epoch 779, Loss 55568163824.50, Val loss 15667152896.00\n","Epoch 780, Loss 55568372038.50, Val loss 15662257152.00\n","Epoch 781, Loss 55573173751.00, Val loss 15657761792.00\n","Epoch 782, Loss 55576657010.00, Val loss 15655772160.00\n","Epoch 783, Loss 55576437950.50, Val loss 15652404224.00\n","Epoch 784, Loss 55578293638.00, Val loss 15648962560.00\n","Epoch 785, Loss 55580045607.00, Val loss 15645806592.00\n","Epoch 786, Loss 55581770251.00, Val loss 15642484736.00\n","Epoch 787, Loss 55583287328.00, Val loss 15640379392.00\n","Epoch 788, Loss 55583130175.00, Val loss 15636628480.00\n","Epoch 789, Loss 55585548967.00, Val loss 15633012736.00\n","Epoch 790, Loss 55587486102.00, Val loss 15629528064.00\n","Epoch 791, Loss 55589792850.00, Val loss 15626424320.00\n","Epoch 792, Loss 55591007364.00, Val loss 15623576576.00\n","Epoch 793, Loss 55591745314.50, Val loss 15619875840.00\n","Epoch 794, Loss 55593993645.50, Val loss 15616251904.00\n","Epoch 795, Loss 55596138947.50, Val loss 15612747776.00\n","Epoch 796, Loss 55597862937.00, Val loss 15609283584.00\n","Epoch 797, Loss 55599417994.00, Val loss 15606182912.00\n","Epoch 798, Loss 55600568495.00, Val loss 15602562048.00\n","Epoch 799, Loss 55602750107.00, Val loss 15599560704.00\n","Epoch 800, Loss 55604199491.00, Val loss 15596086272.00\n","Epoch 801, Loss 55605969045.50, Val loss 15593384960.00\n","Epoch 802, Loss 55606941853.50, Val loss 15590017024.00\n","Epoch 803, Loss 55608616958.00, Val loss 15587193856.00\n","Epoch 804, Loss 55609400019.50, Val loss 15583873024.00\n","Epoch 805, Loss 55611470272.50, Val loss 15580172288.00\n","Epoch 806, Loss 55613880177.00, Val loss 15576812544.00\n","Epoch 807, Loss 55615461833.50, Val loss 15574288384.00\n","Epoch 808, Loss 55616083824.50, Val loss 15570790400.00\n","Epoch 809, Loss 55618228691.50, Val loss 15567116288.00\n","Epoch 810, Loss 55619983311.50, Val loss 15564605440.00\n","Epoch 811, Loss 55620901245.50, Val loss 15560514560.00\n","Epoch 812, Loss 55623841738.00, Val loss 15557209088.00\n","Epoch 813, Loss 55625313483.50, Val loss 15554038784.00\n","Epoch 814, Loss 55627123203.00, Val loss 15550901248.00\n","Epoch 815, Loss 55628358369.50, Val loss 15548193792.00\n","Epoch 816, Loss 55629374237.50, Val loss 15544314880.00\n","Epoch 817, Loss 55632466911.00, Val loss 15541980160.00\n","Epoch 818, Loss 55632875316.50, Val loss 15538014208.00\n","Epoch 819, Loss 55635459154.00, Val loss 15536040960.00\n","Epoch 820, Loss 55635516049.00, Val loss 15532233728.00\n","Epoch 821, Loss 55637397163.50, Val loss 15529418752.00\n","Epoch 822, Loss 55638656355.00, Val loss 15525780480.00\n","Epoch 823, Loss 55640803211.50, Val loss 15522566144.00\n","Epoch 824, Loss 55642741594.50, Val loss 15519698944.00\n","Epoch 825, Loss 55643829783.50, Val loss 15516372992.00\n","Epoch 826, Loss 55645537150.00, Val loss 15513410560.00\n","Epoch 827, Loss 55647034583.50, Val loss 15510175744.00\n","Epoch 828, Loss 55648756573.00, Val loss 15507006464.00\n","Epoch 829, Loss 55650277308.00, Val loss 15503315968.00\n","Epoch 830, Loss 55652605656.50, Val loss 15500690432.00\n","Epoch 831, Loss 55653332839.00, Val loss 15497114624.00\n","Epoch 832, Loss 55655585451.00, Val loss 15494141952.00\n","Epoch 833, Loss 55657264780.50, Val loss 15490792448.00\n","Epoch 834, Loss 55658865751.00, Val loss 15488315392.00\n","Epoch 835, Loss 55659455437.50, Val loss 15484921856.00\n","Epoch 836, Loss 55661413747.50, Val loss 15481112576.00\n","Epoch 837, Loss 55663980941.50, Val loss 15478868992.00\n","Epoch 838, Loss 55664435123.00, Val loss 15475511296.00\n","Epoch 839, Loss 55666224298.00, Val loss 15471477760.00\n","Epoch 840, Loss 55664738086.00, Val loss 15469742080.00\n","Epoch 841, Loss 55664937768.50, Val loss 15465783296.00\n","Epoch 842, Loss 55667192700.00, Val loss 15462852608.00\n","Epoch 843, Loss 55668901009.50, Val loss 15459863552.00\n","Epoch 844, Loss 55670448948.00, Val loss 15456976896.00\n","Epoch 845, Loss 55671779429.50, Val loss 15453845504.00\n","Epoch 846, Loss 55673310968.50, Val loss 15450436608.00\n","Epoch 847, Loss 55675676677.00, Val loss 15447557120.00\n","Epoch 848, Loss 55676842250.50, Val loss 15445180416.00\n","Epoch 849, Loss 55677337088.00, Val loss 15441141760.00\n","Epoch 850, Loss 55680090828.00, Val loss 15438447616.00\n","Epoch 851, Loss 55681088859.00, Val loss 15435390976.00\n","Epoch 852, Loss 55682713350.00, Val loss 15432249344.00\n","Epoch 853, Loss 55683791740.50, Val loss 15427574784.00\n","Epoch 854, Loss 55682887134.00, Val loss 15426232320.00\n","Epoch 855, Loss 55687261176.50, Val loss 15423286272.00\n","Epoch 856, Loss 55689001873.50, Val loss 15421331456.00\n","Epoch 857, Loss 55689500822.00, Val loss 15418210304.00\n","Epoch 858, Loss 55690804962.50, Val loss 15415648256.00\n","Epoch 859, Loss 55691245471.50, Val loss 15411192832.00\n","Epoch 860, Loss 55694817622.00, Val loss 15408586752.00\n","Epoch 861, Loss 55695899873.50, Val loss 15405459456.00\n","Epoch 862, Loss 55697549165.50, Val loss 15402051584.00\n","Epoch 863, Loss 55699624800.50, Val loss 15398961152.00\n","Epoch 864, Loss 55701887867.00, Val loss 15396739072.00\n","Epoch 865, Loss 55701789427.50, Val loss 15393851392.00\n","Epoch 866, Loss 55703425502.50, Val loss 15390675968.00\n","Epoch 867, Loss 55704804102.50, Val loss 15387869184.00\n","Epoch 868, Loss 55706377722.50, Val loss 15384767488.00\n","Epoch 869, Loss 55707615263.50, Val loss 15381824512.00\n","Epoch 870, Loss 55709379249.50, Val loss 15378296832.00\n","Epoch 871, Loss 55711373373.00, Val loss 15375331328.00\n","Epoch 872, Loss 55713106596.00, Val loss 15372093440.00\n","Epoch 873, Loss 55714389653.50, Val loss 15369476096.00\n","Epoch 874, Loss 55715854426.00, Val loss 15366220800.00\n","Epoch 875, Loss 55717136069.00, Val loss 15363542016.00\n","Epoch 876, Loss 55718956970.00, Val loss 15359770624.00\n","Epoch 877, Loss 55720836410.50, Val loss 15357638656.00\n","Epoch 878, Loss 55721954129.00, Val loss 15353924608.00\n","Epoch 879, Loss 55723871352.00, Val loss 15350981632.00\n","Epoch 880, Loss 55725821661.50, Val loss 15348407296.00\n","Epoch 881, Loss 55726014049.50, Val loss 15345266688.00\n","Epoch 882, Loss 55728524861.50, Val loss 15342897152.00\n","Epoch 883, Loss 55728731032.00, Val loss 15339388928.00\n","Epoch 884, Loss 55731094091.00, Val loss 15336257536.00\n","Epoch 885, Loss 55732401466.50, Val loss 15333184512.00\n","Epoch 886, Loss 55734454693.50, Val loss 15329896448.00\n","Epoch 887, Loss 55735737954.00, Val loss 15326852096.00\n","Epoch 888, Loss 55738135503.50, Val loss 15323725824.00\n","Epoch 889, Loss 55739451425.00, Val loss 15321187328.00\n","Epoch 890, Loss 55741136740.00, Val loss 15318043648.00\n","Epoch 891, Loss 55741712825.50, Val loss 15314839552.00\n","Epoch 892, Loss 55744389876.00, Val loss 15310577664.00\n","Epoch 893, Loss 55746744174.50, Val loss 15308247040.00\n","Epoch 894, Loss 55747990469.00, Val loss 15304206336.00\n","Epoch 895, Loss 55750757951.00, Val loss 15301438464.00\n","Epoch 896, Loss 55752498957.00, Val loss 15298409472.00\n","Epoch 897, Loss 55754077883.00, Val loss 15295811584.00\n","Epoch 898, Loss 55755314489.00, Val loss 15292204032.00\n","Epoch 899, Loss 55757525902.00, Val loss 15289785344.00\n","Epoch 900, Loss 55759141311.00, Val loss 15286755328.00\n","Epoch 901, Loss 55760473572.00, Val loss 15284322304.00\n","Epoch 902, Loss 55761464251.00, Val loss 15280774144.00\n","Epoch 903, Loss 55764139734.00, Val loss 15278069760.00\n","Epoch 904, Loss 55764973026.00, Val loss 15274429440.00\n","Epoch 905, Loss 55767188035.00, Val loss 15271512064.00\n","Epoch 906, Loss 55768506150.00, Val loss 15268070400.00\n","Epoch 907, Loss 55770877539.00, Val loss 15265710080.00\n","Epoch 908, Loss 55771710623.00, Val loss 15262854144.00\n","Epoch 909, Loss 55773387423.00, Val loss 15260447744.00\n","Epoch 910, Loss 55774620776.00, Val loss 15258315776.00\n","Epoch 911, Loss 55774796727.00, Val loss 15255355392.00\n","Epoch 912, Loss 55776339842.00, Val loss 15252306944.00\n","Epoch 913, Loss 55777916712.00, Val loss 15249615872.00\n","Epoch 914, Loss 55778910182.00, Val loss 15246235648.00\n","Epoch 915, Loss 55781143126.00, Val loss 15243764736.00\n","Epoch 916, Loss 55782165953.00, Val loss 15240462336.00\n","Epoch 917, Loss 55784149899.00, Val loss 15237842944.00\n","Epoch 918, Loss 55785073532.00, Val loss 15235011584.00\n","Epoch 919, Loss 55787056044.00, Val loss 15232190464.00\n","Epoch 920, Loss 55788505623.00, Val loss 15230424064.00\n","Epoch 921, Loss 55788298799.00, Val loss 15226960896.00\n","Epoch 922, Loss 55790990663.00, Val loss 15224682496.00\n","Epoch 923, Loss 636654503987.00, Val loss 20250732544.00\n","Epoch 924, Loss 50345160896.00, Val loss 17375057920.00\n","Epoch 925, Loss 53035556512.00, Val loss 16253171712.00\n","Epoch 926, Loss 54362193114.00, Val loss 15729648640.00\n","Epoch 927, Loss 55047032190.00, Val loss 15478291456.00\n","Epoch 928, Loss 55385226596.00, Val loss 15353798656.00\n","Epoch 929, Loss 55555254121.00, Val loss 15287792640.00\n","Epoch 930, Loss 55645133096.00, Val loss 15252913152.00\n","Epoch 931, Loss 55691523430.00, Val loss 15233235968.00\n","Epoch 932, Loss 55717014101.00, Val loss 15223359488.00\n","Epoch 933, Loss 55728912318.00, Val loss 15216540672.00\n","Epoch 934, Loss 55736534785.00, Val loss 15212191744.00\n","Epoch 935, Loss 55740792386.00, Val loss 15209594880.00\n","Epoch 936, Loss 55742415168.00, Val loss 15206731776.00\n","Epoch 937, Loss 55744684266.00, Val loss 15203910656.00\n","Epoch 938, Loss 55747080320.00, Val loss 15201590272.00\n","Epoch 939, Loss 55748719858.00, Val loss 15199594496.00\n","Epoch 940, Loss 55750042810.00, Val loss 15198658560.00\n","Epoch 941, Loss 55749761193.00, Val loss 15196085248.00\n","Epoch 942, Loss 55751507870.00, Val loss 15194217472.00\n","Epoch 943, Loss 55752509640.00, Val loss 15191692288.00\n","Epoch 944, Loss 55754014564.00, Val loss 15189590016.00\n","Epoch 945, Loss 55755363706.00, Val loss 15187025920.00\n","Epoch 946, Loss 55757495702.00, Val loss 15184676864.00\n","Epoch 947, Loss 55757446313.00, Val loss 15182195712.00\n","Epoch 948, Loss 55756192865.00, Val loss 15179844608.00\n","Epoch 949, Loss 55758131208.00, Val loss 15177846784.00\n","Epoch 950, Loss 55759561544.00, Val loss 15175087104.00\n","Epoch 951, Loss 55759196211.00, Val loss 15172464640.00\n","Epoch 952, Loss 55760892203.00, Val loss 15170278400.00\n","Epoch 953, Loss 55762513622.00, Val loss 15168391168.00\n","Epoch 954, Loss 55763370163.00, Val loss 15165966336.00\n","Epoch 955, Loss 55764804676.00, Val loss 15164321792.00\n","Epoch 956, Loss 55765544466.00, Val loss 15161668608.00\n","Epoch 957, Loss 55767487772.00, Val loss 15158849536.00\n","Epoch 958, Loss 55770070466.00, Val loss 15156606976.00\n","Epoch 959, Loss 55771381926.00, Val loss 15154911232.00\n","Epoch 960, Loss 55772099595.00, Val loss 15152454656.00\n","Epoch 961, Loss 55773567240.00, Val loss 15149655040.00\n","Epoch 962, Loss 55776147591.00, Val loss 15147714560.00\n","Epoch 963, Loss 55777323253.00, Val loss 15146113024.00\n","Epoch 964, Loss 55777830092.00, Val loss 15144283136.00\n","Epoch 965, Loss 55778578482.00, Val loss 15141640192.00\n","Epoch 966, Loss 55780406329.00, Val loss 15138864128.00\n","Epoch 967, Loss 55782826304.00, Val loss 15136640000.00\n","Epoch 968, Loss 55783933865.00, Val loss 15135033344.00\n","Epoch 969, Loss 55784787397.00, Val loss 15132830720.00\n","Epoch 970, Loss 55785912352.00, Val loss 15130719232.00\n","Epoch 971, Loss 55786548582.00, Val loss 15127976960.00\n","Epoch 972, Loss 55788505239.00, Val loss 15124871168.00\n","Epoch 973, Loss 55791135603.00, Val loss 15122227200.00\n","Epoch 974, Loss 55793179374.00, Val loss 15119785984.00\n","Epoch 975, Loss 55794641363.00, Val loss 15117747200.00\n","Epoch 976, Loss 55795306575.00, Val loss 15115219968.00\n","Epoch 977, Loss 55797103341.00, Val loss 15112714240.00\n","Epoch 978, Loss 55799362117.00, Val loss 15110468608.00\n","Epoch 979, Loss 55800746496.00, Val loss 15108556800.00\n","Epoch 980, Loss 55801484339.00, Val loss 15106593792.00\n","Epoch 981, Loss 55801615244.00, Val loss 15103413248.00\n","Epoch 982, Loss 55804462352.00, Val loss 15100688384.00\n","Epoch 983, Loss 55806214995.00, Val loss 15099160576.00\n","Epoch 984, Loss 55806425766.00, Val loss 15095515136.00\n","Epoch 985, Loss 55809249035.00, Val loss 15093878784.00\n","Epoch 986, Loss 55809489642.00, Val loss 15090969600.00\n","Epoch 987, Loss 55810275924.00, Val loss 15087841280.00\n","Epoch 988, Loss 55813134120.00, Val loss 15085682688.00\n","Epoch 989, Loss 55808295631.00, Val loss 15082052608.00\n","Epoch 990, Loss 55835657884.00, Val loss 15080077312.00\n","Epoch 991, Loss 55832872152.00, Val loss 15073850368.00\n","Epoch 992, Loss 55850566499.00, Val loss 15076493312.00\n","Epoch 993, Loss 55830262140.00, Val loss 15078203392.00\n","Epoch 994, Loss 55830011251.00, Val loss 15069568000.00\n","Epoch 995, Loss 55839553809.00, Val loss 15066008576.00\n","Epoch 996, Loss 55841587431.00, Val loss 15060830208.00\n","Epoch 997, Loss 55848552327.00, Val loss 15058492416.00\n","Epoch 998, Loss 55847914112.00, Val loss 15055541248.00\n","Epoch 999, Loss 55851681678.00, Val loss 15053179904.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"status":"ok","timestamp":1648745246891,"user_tz":300,"elapsed":200,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"98a28165-6fbb-4ae7-b9af-7957c4fa04ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["11941960704.0\n","tensor([304631.8125, 305597.7188, 306572.0625, 307513.5312, 308292.1562,\n","        308696.3125, 308879.8750, 309931.6562, 311487.1875, 312880.6250,\n","        313507.8750, 313894.5312, 314994.3750, 316399.2500, 317805.2812],\n","       grad_fn=<SelectBackward0>)\n","tensor([292187., 293697., 293697., 293697., 293697., 293697., 295701., 296870.,\n","        297729., 297729., 297729., 298362., 298626., 298808., 298993.])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","    # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_cumulative_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_cumulative_infected_tensor':labeled_output # (52, 15)\n","}\n","\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)"],"metadata":{"id":"s3-8Yge6CAWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save archived_output as a CSV file\n","archived_output = None\n","with open(save_predictions_relative_path, 'rb') as handle:\n","  archived_output = pickle.load(handle)\n","model_predictions = archived_output['model_predictions_cumulative_infected_tensor']\n","\n","state_list = None\n","state_name_to_abbrev = None\n","with open('data/preprocessed_data_v3.3_to_3.5_new_data.pickle', 'rb') as handle:\n","  preprocessed_data = pickle.load(handle)\n","  state_list = preprocessed_data['state_list']\n","  state_name_to_abbrev = preprocessed_data['state_name_to_abbrev']\n","\n","date_list = ['2022-02-17', '2022-02-18', '2022-02-19', '2022-02-20', '2022-02-21', \n","             '2022-02-22', '2022-02-23', '2022-02-24', '2022-02-25', '2022-02-26', \n","             '2022-02-27', '2022-02-28', '2022-03-01', '2022-03-02', '2022-03-03']\n","\n","import csv\n","with open('data/saved_output_v3.3_new_data_COVID_Forecaster_full.csv', 'w') as handle:\n","  csv_writer = csv.writer(handle)\n","  csv_writer.writerow(['cumulative_cases', 'date_today', 'state'])\n","  for i, state_name in enumerate(state_list):\n","    state_predictions = model_predictions[i]\n","    curr_state_abbrev = state_name_to_abbrev[state_name]\n","    for j in range(len(state_predictions)):\n","      curr_day_prediction = state_predictions[j].item()\n","      curr_day = date_list[j]\n","      csv_writer.writerow([curr_day_prediction, curr_day, curr_state_abbrev])"],"metadata":{"id":"AjcqRpezos4h","executionInfo":{"status":"ok","timestamp":1649263005768,"user_tz":300,"elapsed":470,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["mean_squared_error = criterion(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","mae_function = torch.nn.L1Loss()\n","mean_absolute_error = mae_function(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","print(\"mean squared error: \", mean_squared_error)\n","print(\"mean absolute error: \", mean_absolute_error)\n","# Without seed: \n","# mean squared error:  38665711616.0\n","# mean absolute error:  128769.2890625\n","\n","# With seed = 0 - trial 1\n","# mean squared error:  11941960704.0\n","# mean absolute error:  70670.78125\n","\n","# With seed = 0 - trial 2\n","# mean squared error:  11941960704.0\n","# mean absolute error:  70670.78125"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddc7iLV-7OJb","executionInfo":{"status":"ok","timestamp":1648745246897,"user_tz":300,"elapsed":14,"user":{"displayName":"Andrew Wang","userId":"15265478926702059943"}},"outputId":"bf4a9caa-813e-4767-ac96-9eb6c85cd1d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean squared error:  11941960704.0\n","mean absolute error:  70670.78125\n"]}]}]}