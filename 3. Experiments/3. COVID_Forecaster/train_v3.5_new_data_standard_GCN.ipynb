{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.5_new_data_standard_GCN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN5xAxuHnMEWUPSdADN6gZH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1646782413408,"user_tz":360,"elapsed":297829,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"563145fd-3c9e-49bf-c4d9-87f22a851ed2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97a30a92-742d-4016-c4b4-5b3e0dbbba9c","executionInfo":{"status":"ok","timestamp":1646782466690,"user_tz":360,"elapsed":53295,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.12\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.5.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (747 kB)\n","\u001b[K     |████████████████████████████████| 747 kB 3.9 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.3.tar.gz (370 kB)\n","\u001b[K     |████████████████████████████████| 370 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Collecting rdflib\n","  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n","\u001b[K     |████████████████████████████████| 482 kB 42.9 MB/s \n","\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (4.11.2)\n","Collecting isodate\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 495 kB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.3-py3-none-any.whl size=581968 sha256=79c1af23afc3824673d000a05c8b8976e7c83bfdc99d4d960773e8d6f55b7708\n","  Stored in directory: /root/.cache/pip/wheels/c3/2a/58/87ce0508964d4def1aafb92750c4f3ac77038efd1b9a89dcf5\n","Successfully built torch-geometric\n","Installing collected packages: isodate, yacs, rdflib, torch-geometric\n","Successfully installed isodate-0.6.1 rdflib-6.1.1 torch-geometric-2.0.3 yacs-0.1.8\n","Collecting torch-geometric-temporal\n","  Downloading torch_geometric_temporal-0.51.0.tar.gz (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: torch_sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.12)\n","Requirement already satisfied: torch_scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (6.1.1)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.1.8)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.13)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.6.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (4.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (57.4.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (0.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch_geometric->torch-geometric-temporal) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.1.0)\n","Building wheels for collected packages: torch-geometric-temporal\n","  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.51.0-py3-none-any.whl size=83569 sha256=7e65009f410eb6a488c4c44bd490b4fa957a5abde734c2fd942985c7c9f4c5b0\n","  Stored in directory: /root/.cache/pip/wheels/a5/26/64/465700aa43b21fccca9ae446b407de2389f0ba16114e84db8d\n","Successfully built torch-geometric-temporal\n","Installing collected packages: torch-geometric-temporal\n","Successfully installed torch-geometric-temporal-0.51.0\n","Collecting ogb\n","  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Collecting outdated>=0.2.0\n","  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Collecting littleutils\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=e107f012cd1db39e1272c41256ca1e4e70d538ef06c24ae892f57baab41d94f2\n","  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n","PyTorch has version 1.10.0+cu111\n","Collecting epiweeks\n","  Downloading epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n","Installing collected packages: epiweeks\n","Successfully installed epiweeks-2.1.4\n","Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 24\n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3.3_to_3.5_new_data.pickle'\n","save_model_relative_path = './saved_models/v3.5_new_data_standard_GCN'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3.5_archived_output.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1646782466692,"user_tz":360,"elapsed":47,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"ea51ace4-bd50-43c7-f1fc-45de2c93c94e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_confirmed_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_confirmed_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_confirmed_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU","executionInfo":{"status":"ok","timestamp":1646782467428,"user_tz":360,"elapsed":771,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3","executionInfo":{"status":"ok","timestamp":1646782467429,"user_tz":360,"elapsed":7,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class GCN_standard(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = GCNConv(inputLayer_num_features, hiddenLayer1_num_features)\n","        self.conv2 = GCNConv(hiddenLayer1_num_features, outputLayer_num_features)\n","        self.linear = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      \n","      x = self.linear(x)\n","\n","      return x\n","\n","model = GCN_standard().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj","executionInfo":{"status":"ok","timestamp":1646782486160,"user_tz":360,"elapsed":272,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646782491974,"user_tz":360,"elapsed":284,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"a7d71ad6-c6f6-4ffa-eb6e-a5a8fee9aa49"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GCN_standard(\n","  (conv1): GCNConv(24, 24)\n","  (conv2): GCNConv(24, 15)\n","  (linear): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646782775161,"user_tz":360,"elapsed":282935,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"0edb4b48-b0d6-46e1-c8dc-8af22766c9ed"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 36975566115072.00, Val loss 1823316115456.00\n","Epoch 1, Loss 36952790368768.00, Val loss 1828599234560.00\n","Epoch 2, Loss 36948353301632.00, Val loss 1830303694848.00\n","Epoch 3, Loss 36947837728768.00, Val loss 1844455145472.00\n","Epoch 4, Loss 36957961972864.00, Val loss 1848978309120.00\n","Epoch 5, Loss 36964977509248.00, Val loss 1827649224704.00\n","Epoch 6, Loss 36947566153088.00, Val loss 1827352215552.00\n","Epoch 7, Loss 36947938542592.00, Val loss 1827854614528.00\n","Epoch 8, Loss 36948360700800.00, Val loss 1828423729152.00\n","Epoch 9, Loss 36948947098496.00, Val loss 1828770938880.00\n","Epoch 10, Loss 36949507744512.00, Val loss 1829200461824.00\n","Epoch 11, Loss 36950382446464.00, Val loss 1829233229824.00\n","Epoch 12, Loss 36950961886208.00, Val loss 1829257347072.00\n","Epoch 13, Loss 36951376472064.00, Val loss 1828891394048.00\n","Epoch 14, Loss 36949995907072.00, Val loss 1831727661056.00\n","Epoch 15, Loss 36950847991040.00, Val loss 1828844077056.00\n","Epoch 16, Loss 36946943870208.00, Val loss 1828298031104.00\n","Epoch 17, Loss 36951111288064.00, Val loss 1830350880768.00\n","Epoch 18, Loss 36954592019328.00, Val loss 1830525075456.00\n","Epoch 19, Loss 36955619514112.00, Val loss 1828222402560.00\n","Epoch 20, Loss 36955118038016.00, Val loss 1827563241472.00\n","Epoch 21, Loss 36954797595264.00, Val loss 1826517811200.00\n","Epoch 22, Loss 36953846847232.00, Val loss 1826930294784.00\n","Epoch 23, Loss 36952596948992.00, Val loss 1825676328960.00\n","Epoch 24, Loss 36950813561984.00, Val loss 1827303325696.00\n","Epoch 25, Loss 36950242001664.00, Val loss 1826654126080.00\n","Epoch 26, Loss 36948400226176.00, Val loss 1826509553664.00\n","Epoch 27, Loss 36952161654528.00, Val loss 1825637400576.00\n","Epoch 28, Loss 36951046307712.00, Val loss 1825277345792.00\n","Epoch 29, Loss 36950790932224.00, Val loss 1824872071168.00\n","Epoch 30, Loss 36950239463552.00, Val loss 1824421445632.00\n","Epoch 31, Loss 36949164818560.00, Val loss 1823974752256.00\n","Epoch 32, Loss 36947554637312.00, Val loss 1823571181568.00\n","==================================================================\n","Saved best model\n","Epoch 33, Loss 36945503397120.00, Val loss 1823225282560.00\n","==================================================================\n","Saved best model\n","Epoch 34, Loss 36943136103936.00, Val loss 1822929059840.00\n","==================================================================\n","Saved best model\n","Epoch 35, Loss 36940550485504.00, Val loss 1822659706880.00\n","==================================================================\n","Saved best model\n","Epoch 36, Loss 36937790211072.00, Val loss 1822386814976.00\n","==================================================================\n","Saved best model\n","Epoch 37, Loss 36934847965440.00, Val loss 1822077353984.00\n","==================================================================\n","Saved best model\n","Epoch 38, Loss 36931701563776.00, Val loss 1821714284544.00\n","==================================================================\n","Saved best model\n","Epoch 39, Loss 36928374779008.00, Val loss 1821320806400.00\n","==================================================================\n","Saved best model\n","Epoch 40, Loss 36924978388992.00, Val loss 1820966125568.00\n","==================================================================\n","Saved best model\n","Epoch 41, Loss 36921689035264.00, Val loss 1820709748736.00\n","==================================================================\n","Saved best model\n","Epoch 42, Loss 36918572543360.00, Val loss 1820506849280.00\n","==================================================================\n","Saved best model\n","Epoch 43, Loss 36915411518848.00, Val loss 1820217442304.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 36911841487232.00, Val loss 1819769044992.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 36907774235136.00, Val loss 1819219984384.00\n","==================================================================\n","Saved best model\n","Epoch 46, Loss 36903385110272.00, Val loss 1818648772608.00\n","==================================================================\n","Saved best model\n","Epoch 47, Loss 36898813454848.00, Val loss 1818094731264.00\n","==================================================================\n","Saved best model\n","Epoch 48, Loss 36894107218944.00, Val loss 1817568608256.00\n","==================================================================\n","Saved best model\n","Epoch 49, Loss 36889275555840.00, Val loss 1817073025024.00\n","==================================================================\n","Saved best model\n","Epoch 50, Loss 36884314510208.00, Val loss 1816607719424.00\n","==================================================================\n","Saved best model\n","Epoch 51, Loss 36876031168128.00, Val loss 1816166531072.00\n","==================================================================\n","Saved best model\n","Epoch 52, Loss 36874086053760.00, Val loss 1815778164736.00\n","==================================================================\n","Saved best model\n","Epoch 53, Loss 36868170839808.00, Val loss 1815403298816.00\n","==================================================================\n","Saved best model\n","Epoch 54, Loss 36861470360448.00, Val loss 1815025156096.00\n","==================================================================\n","Saved best model\n","Epoch 55, Loss 36853500125824.00, Val loss 1814643474432.00\n","==================================================================\n","Saved best model\n","Epoch 56, Loss 36844280449280.00, Val loss 1814348824576.00\n","==================================================================\n","Saved best model\n","Epoch 57, Loss 36835008111360.00, Val loss 1814333751296.00\n","Epoch 58, Loss 36825063988096.00, Val loss 1814636134400.00\n","Epoch 59, Loss 36815333482112.00, Val loss 1814655664128.00\n","==================================================================\n","Saved best model\n","Epoch 60, Loss 36805357693952.00, Val loss 1813911699456.00\n","==================================================================\n","Saved best model\n","Epoch 61, Loss 36795512537728.00, Val loss 1813334982656.00\n","Epoch 62, Loss 36798654696064.00, Val loss 1813418475520.00\n","==================================================================\n","Saved best model\n","Epoch 63, Loss 36785445832192.00, Val loss 1812890517504.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 36773109054848.00, Val loss 1811965542400.00\n","Epoch 65, Loss 36758464252928.00, Val loss 1812100546560.00\n","==================================================================\n","Saved best model\n","Epoch 66, Loss 36738641387392.00, Val loss 1811831324672.00\n","==================================================================\n","Saved best model\n","Epoch 67, Loss 36738479140352.00, Val loss 1811351863296.00\n","Epoch 68, Loss 36714355298304.00, Val loss 1812732968960.00\n","==================================================================\n","Saved best model\n","Epoch 69, Loss 36725149950848.00, Val loss 1810153209856.00\n","==================================================================\n","Saved best model\n","Epoch 70, Loss 36687219850240.00, Val loss 1809713332224.00\n","Epoch 71, Loss 36677725427584.00, Val loss 1812333330432.00\n","Epoch 72, Loss 36763828535424.00, Val loss 1810985517056.00\n","Epoch 73, Loss 36733205301632.00, Val loss 1811264438272.00\n","Epoch 74, Loss 36728350569728.00, Val loss 1811802750976.00\n","Epoch 75, Loss 36726423330048.00, Val loss 1812180238336.00\n","Epoch 76, Loss 36725381799680.00, Val loss 1812289683456.00\n","Epoch 77, Loss 36724159547136.00, Val loss 1812201340928.00\n","Epoch 78, Loss 36795431546368.00, Val loss 1810159501312.00\n","==================================================================\n","Saved best model\n","Epoch 79, Loss 36731581161856.00, Val loss 1809394827264.00\n","==================================================================\n","Saved best model\n","Epoch 80, Loss 36729070361216.00, Val loss 1809031233536.00\n","==================================================================\n","Saved best model\n","Epoch 81, Loss 36723896554752.00, Val loss 1808731734016.00\n","==================================================================\n","Saved best model\n","Epoch 82, Loss 36726068957312.00, Val loss 1808503406592.00\n","==================================================================\n","Saved best model\n","Epoch 83, Loss 36709419319552.00, Val loss 1808109535232.00\n","==================================================================\n","Saved best model\n","Epoch 84, Loss 36693011445120.00, Val loss 1807756689408.00\n","==================================================================\n","Saved best model\n","Epoch 85, Loss 36688144122752.00, Val loss 1807475933184.00\n","==================================================================\n","Saved best model\n","Epoch 86, Loss 36686519412352.00, Val loss 1807209857024.00\n","==================================================================\n","Saved best model\n","Epoch 87, Loss 36666141057792.00, Val loss 1806775222272.00\n","==================================================================\n","Saved best model\n","Epoch 88, Loss 36650761684352.00, Val loss 1806465630208.00\n","Epoch 89, Loss 36758651801984.00, Val loss 1812515389440.00\n","Epoch 90, Loss 36721809859968.00, Val loss 1811984809984.00\n","Epoch 91, Loss 36714548320512.00, Val loss 1811281608704.00\n","Epoch 92, Loss 36708104363264.00, Val loss 1810216517632.00\n","Epoch 93, Loss 36702925609600.00, Val loss 1809559060480.00\n","Epoch 94, Loss 36697647752576.00, Val loss 1809170694144.00\n","Epoch 95, Loss 36690514001024.00, Val loss 1808856776704.00\n","Epoch 96, Loss 36682014089344.00, Val loss 1808553738240.00\n","Epoch 97, Loss 36672892555392.00, Val loss 1808244932608.00\n","Epoch 98, Loss 36663378650112.00, Val loss 1807925903360.00\n","Epoch 99, Loss 36653547402624.00, Val loss 1807593373696.00\n","Epoch 100, Loss 36643391407872.00, Val loss 1807247867904.00\n","Epoch 101, Loss 36632755128960.00, Val loss 1806890696704.00\n","Epoch 102, Loss 36621545135872.00, Val loss 1806506131456.00\n","==================================================================\n","Saved best model\n","Epoch 103, Loss 36612283066368.00, Val loss 1806111473664.00\n","==================================================================\n","Saved best model\n","Epoch 104, Loss 36601167865216.00, Val loss 1805694271488.00\n","==================================================================\n","Saved best model\n","Epoch 105, Loss 36595080303872.00, Val loss 1805361086464.00\n","==================================================================\n","Saved best model\n","Epoch 106, Loss 36578035555456.00, Val loss 1804927631360.00\n","==================================================================\n","Saved best model\n","Epoch 107, Loss 36571064288384.00, Val loss 1804550799360.00\n","==================================================================\n","Saved best model\n","Epoch 108, Loss 36555239557248.00, Val loss 1804139757568.00\n","==================================================================\n","Saved best model\n","Epoch 109, Loss 36545752068480.00, Val loss 1803725438976.00\n","==================================================================\n","Saved best model\n","Epoch 110, Loss 36528377041920.00, Val loss 1803296964608.00\n","==================================================================\n","Saved best model\n","Epoch 111, Loss 36532426594176.00, Val loss 1802801119232.00\n","==================================================================\n","Saved best model\n","Epoch 112, Loss 36501126030848.00, Val loss 1802363731968.00\n","==================================================================\n","Saved best model\n","Epoch 113, Loss 36490847501056.00, Val loss 1801885843456.00\n","==================================================================\n","Saved best model\n","Epoch 114, Loss 36471801712256.00, Val loss 1801417129984.00\n","==================================================================\n","Saved best model\n","Epoch 115, Loss 36461082042752.00, Val loss 1800916959232.00\n","==================================================================\n","Saved best model\n","Epoch 116, Loss 36443863419008.00, Val loss 1800416919552.00\n","==================================================================\n","Saved best model\n","Epoch 117, Loss 36428732724736.00, Val loss 1799902461952.00\n","==================================================================\n","Saved best model\n","Epoch 118, Loss 36412868887040.00, Val loss 1799398621184.00\n","==================================================================\n","Saved best model\n","Epoch 119, Loss 36397853837568.00, Val loss 1798846021632.00\n","==================================================================\n","Saved best model\n","Epoch 120, Loss 36381141431552.00, Val loss 1798361710592.00\n","==================================================================\n","Saved best model\n","Epoch 121, Loss 36366558572288.00, Val loss 1797779750912.00\n","==================================================================\n","Saved best model\n","Epoch 122, Loss 36347283796736.00, Val loss 1797289410560.00\n","==================================================================\n","Saved best model\n","Epoch 123, Loss 36334093553408.00, Val loss 1796625793024.00\n","==================================================================\n","Saved best model\n","Epoch 124, Loss 36325784946432.00, Val loss 1796007657472.00\n","==================================================================\n","Saved best model\n","Epoch 125, Loss 36282205391360.00, Val loss 1795466985472.00\n","==================================================================\n","Saved best model\n","Epoch 126, Loss 36293190656000.00, Val loss 1794837184512.00\n","==================================================================\n","Saved best model\n","Epoch 127, Loss 36240853761024.00, Val loss 1794203713536.00\n","==================================================================\n","Saved best model\n","Epoch 128, Loss 36274263708928.00, Val loss 1793523843072.00\n","==================================================================\n","Saved best model\n","Epoch 129, Loss 36203684099328.00, Val loss 1792758906880.00\n","Epoch 130, Loss 36570008598016.00, Val loss 1804015894528.00\n","Epoch 131, Loss 36227116554496.00, Val loss 1798490030080.00\n","Epoch 132, Loss 36211491749376.00, Val loss 1796371513344.00\n","Epoch 133, Loss 36201801674496.00, Val loss 1794825519104.00\n","Epoch 134, Loss 36226475648896.00, Val loss 1794249981952.00\n","Epoch 135, Loss 36192808639488.00, Val loss 1793648361472.00\n","Epoch 136, Loss 36176859957248.00, Val loss 1793232076800.00\n","==================================================================\n","Saved best model\n","Epoch 137, Loss 36166682449664.00, Val loss 1792659423232.00\n","==================================================================\n","Saved best model\n","Epoch 138, Loss 36142076536832.00, Val loss 1792059899904.00\n","==================================================================\n","Saved best model\n","Epoch 139, Loss 36119268630272.00, Val loss 1791696830464.00\n","==================================================================\n","Saved best model\n","Epoch 140, Loss 36108128986880.00, Val loss 1791116836864.00\n","==================================================================\n","Saved best model\n","Epoch 141, Loss 36084296816128.00, Val loss 1790768316416.00\n","Epoch 142, Loss 36086842452480.00, Val loss 1790851940352.00\n","Epoch 143, Loss 36323098679808.00, Val loss 1797809766400.00\n","Epoch 144, Loss 36106236793984.00, Val loss 1795832152064.00\n","Epoch 145, Loss 36087256275072.00, Val loss 1793453457408.00\n","Epoch 146, Loss 36090085843840.00, Val loss 1792428081152.00\n","Epoch 147, Loss 36112840538880.00, Val loss 1792293339136.00\n","Epoch 148, Loss 36090738695680.00, Val loss 1791754895360.00\n","Epoch 149, Loss 36064605886208.00, Val loss 1791322488832.00\n","Epoch 150, Loss 36034355533824.00, Val loss 1790986420224.00\n","==================================================================\n","Saved best model\n","Epoch 151, Loss 36026181926656.00, Val loss 1790527275008.00\n","==================================================================\n","Saved best model\n","Epoch 152, Loss 35993040394496.00, Val loss 1790262247424.00\n","==================================================================\n","Saved best model\n","Epoch 153, Loss 35987191756544.00, Val loss 1789712924672.00\n","==================================================================\n","Saved best model\n","Epoch 154, Loss 35957180321280.00, Val loss 1789045374976.00\n","==================================================================\n","Saved best model\n","Epoch 155, Loss 35934195718656.00, Val loss 1788433661952.00\n","==================================================================\n","Saved best model\n","Epoch 156, Loss 35937089953536.00, Val loss 1788157100032.00\n","==================================================================\n","Saved best model\n","Epoch 157, Loss 35853904124928.00, Val loss 1787662565376.00\n","==================================================================\n","Saved best model\n","Epoch 158, Loss 35859719811328.00, Val loss 1786969718784.00\n","==================================================================\n","Saved best model\n","Epoch 159, Loss 35831285151232.00, Val loss 1786198228992.00\n","==================================================================\n","Saved best model\n","Epoch 160, Loss 35811158285056.00, Val loss 1785391218688.00\n","==================================================================\n","Saved best model\n","Epoch 161, Loss 35773056821248.00, Val loss 1785011503104.00\n","==================================================================\n","Saved best model\n","Epoch 162, Loss 35771920154624.00, Val loss 1784087314432.00\n","==================================================================\n","Saved best model\n","Epoch 163, Loss 35729007690240.00, Val loss 1783717167104.00\n","==================================================================\n","Saved best model\n","Epoch 164, Loss 35729523195648.00, Val loss 1782595846144.00\n","==================================================================\n","Saved best model\n","Epoch 165, Loss 35679955760896.00, Val loss 1782299623424.00\n","==================================================================\n","Saved best model\n","Epoch 166, Loss 35681028428032.00, Val loss 1781331263488.00\n","==================================================================\n","Saved best model\n","Epoch 167, Loss 35643427783936.00, Val loss 1780691763200.00\n","==================================================================\n","Saved best model\n","Epoch 168, Loss 35628176404736.00, Val loss 1779675561984.00\n","==================================================================\n","Saved best model\n","Epoch 169, Loss 35583859106304.00, Val loss 1779175391232.00\n","==================================================================\n","Saved best model\n","Epoch 170, Loss 35584046979584.00, Val loss 1778038210560.00\n","==================================================================\n","Saved best model\n","Epoch 171, Loss 35520839211776.00, Val loss 1777777246208.00\n","==================================================================\n","Saved best model\n","Epoch 172, Loss 35543002575104.00, Val loss 1776462462976.00\n","==================================================================\n","Saved best model\n","Epoch 173, Loss 35474516525056.00, Val loss 1776113549312.00\n","==================================================================\n","Saved best model\n","Epoch 174, Loss 35494212331520.00, Val loss 1774849228800.00\n","==================================================================\n","Saved best model\n","Epoch 175, Loss 35425086031360.00, Val loss 1774378549248.00\n","==================================================================\n","Saved best model\n","Epoch 176, Loss 35439601423360.00, Val loss 1773315817472.00\n","==================================================================\n","Saved best model\n","Epoch 177, Loss 35384452134144.00, Val loss 1772617334784.00\n","==================================================================\n","Saved best model\n","Epoch 178, Loss 35378502314496.00, Val loss 1771897880576.00\n","==================================================================\n","Saved best model\n","Epoch 179, Loss 35347166074368.00, Val loss 1771021139968.00\n","==================================================================\n","Saved best model\n","Epoch 180, Loss 35319091898368.00, Val loss 1770345463808.00\n","==================================================================\n","Saved best model\n","Epoch 181, Loss 35298085912832.00, Val loss 1769467281408.00\n","==================================================================\n","Saved best model\n","Epoch 182, Loss 35267226313472.00, Val loss 1768656863232.00\n","==================================================================\n","Saved best model\n","Epoch 183, Loss 35241355471104.00, Val loss 1768019853312.00\n","==================================================================\n","Saved best model\n","Epoch 184, Loss 35219040047360.00, Val loss 1767280738304.00\n","==================================================================\n","Saved best model\n","Epoch 185, Loss 35196672368640.00, Val loss 1766507282432.00\n","==================================================================\n","Saved best model\n","Epoch 186, Loss 35169785528320.00, Val loss 1765700272128.00\n","==================================================================\n","Saved best model\n","Epoch 187, Loss 35148339370752.00, Val loss 1764879368192.00\n","==================================================================\n","Saved best model\n","Epoch 188, Loss 35118274882048.00, Val loss 1764144185344.00\n","==================================================================\n","Saved best model\n","Epoch 189, Loss 35099222999808.00, Val loss 1763335995392.00\n","==================================================================\n","Saved best model\n","Epoch 190, Loss 35070082106880.00, Val loss 1762628730880.00\n","==================================================================\n","Saved best model\n","Epoch 191, Loss 35050847566848.00, Val loss 1761850818560.00\n","==================================================================\n","Saved best model\n","Epoch 192, Loss 35028919416320.00, Val loss 1761083260928.00\n","==================================================================\n","Saved best model\n","Epoch 193, Loss 35004912041728.00, Val loss 1760234045440.00\n","==================================================================\n","Saved best model\n","Epoch 194, Loss 34979189001728.00, Val loss 1759464390656.00\n","==================================================================\n","Saved best model\n","Epoch 195, Loss 34959321523712.00, Val loss 1758608621568.00\n","==================================================================\n","Saved best model\n","Epoch 196, Loss 34930505366528.00, Val loss 1757798596608.00\n","==================================================================\n","Saved best model\n","Epoch 197, Loss 34908885002240.00, Val loss 1756824338432.00\n","==================================================================\n","Saved best model\n","Epoch 198, Loss 34878324353024.00, Val loss 1755823865856.00\n","==================================================================\n","Saved best model\n","Epoch 199, Loss 34849733231104.00, Val loss 1754878181376.00\n","==================================================================\n","Saved best model\n","Epoch 200, Loss 34826193706752.00, Val loss 1753912836096.00\n","==================================================================\n","Saved best model\n","Epoch 201, Loss 34791113181952.00, Val loss 1753054576640.00\n","==================================================================\n","Saved best model\n","Epoch 202, Loss 34773417590528.00, Val loss 1752321490944.00\n","==================================================================\n","Saved best model\n","Epoch 203, Loss 34761586483456.00, Val loss 1751330717696.00\n","==================================================================\n","Saved best model\n","Epoch 204, Loss 34722931668480.00, Val loss 1750375202816.00\n","==================================================================\n","Saved best model\n","Epoch 205, Loss 34698900064256.00, Val loss 1749591916544.00\n","==================================================================\n","Saved best model\n","Epoch 206, Loss 34683045483008.00, Val loss 1748657242112.00\n","==================================================================\n","Saved best model\n","Epoch 207, Loss 34653360925696.00, Val loss 1747690323968.00\n","==================================================================\n","Saved best model\n","Epoch 208, Loss 34618277341184.00, Val loss 1746721701888.00\n","==================================================================\n","Saved best model\n","Epoch 209, Loss 34594632147456.00, Val loss 1745806426112.00\n","==================================================================\n","Saved best model\n","Epoch 210, Loss 34573307075328.00, Val loss 1744899932160.00\n","==================================================================\n","Saved best model\n","Epoch 211, Loss 34546720689152.00, Val loss 1743993307136.00\n","==================================================================\n","Saved best model\n","Epoch 212, Loss 34524860381952.00, Val loss 1743132950528.00\n","==================================================================\n","Saved best model\n","Epoch 213, Loss 34495797811968.00, Val loss 1742258438144.00\n","==================================================================\n","Saved best model\n","Epoch 214, Loss 34468029582336.00, Val loss 1741340147712.00\n","==================================================================\n","Saved best model\n","Epoch 215, Loss 34446092596224.00, Val loss 1740495519744.00\n","==================================================================\n","Saved best model\n","Epoch 216, Loss 34419310294016.00, Val loss 1739680251904.00\n","==================================================================\n","Saved best model\n","Epoch 217, Loss 34398222077696.00, Val loss 1738995400704.00\n","==================================================================\n","Saved best model\n","Epoch 218, Loss 34352130500608.00, Val loss 1737921527808.00\n","==================================================================\n","Saved best model\n","Epoch 219, Loss 34347683020544.00, Val loss 1737159606272.00\n","==================================================================\n","Saved best model\n","Epoch 220, Loss 34321539595008.00, Val loss 1736456798208.00\n","==================================================================\n","Saved best model\n","Epoch 221, Loss 34285338401792.00, Val loss 1735507836928.00\n","==================================================================\n","Saved best model\n","Epoch 222, Loss 34270923152896.00, Val loss 1734816432128.00\n","==================================================================\n","Saved best model\n","Epoch 223, Loss 34241308032000.00, Val loss 1734046253056.00\n","==================================================================\n","Saved best model\n","Epoch 224, Loss 34214451451392.00, Val loss 1733207785472.00\n","==================================================================\n","Saved best model\n","Epoch 225, Loss 34191081000192.00, Val loss 1732152786944.00\n","==================================================================\n","Saved best model\n","Epoch 226, Loss 34178690867456.00, Val loss 1731465838592.00\n","==================================================================\n","Saved best model\n","Epoch 227, Loss 34153227600384.00, Val loss 1730931195904.00\n","==================================================================\n","Saved best model\n","Epoch 228, Loss 34126991238400.00, Val loss 1730429583360.00\n","==================================================================\n","Saved best model\n","Epoch 229, Loss 34085617213440.00, Val loss 1729247313920.00\n","==================================================================\n","Saved best model\n","Epoch 230, Loss 34081583697664.00, Val loss 1728489848832.00\n","==================================================================\n","Saved best model\n","Epoch 231, Loss 34057608649216.00, Val loss 1727689129984.00\n","==================================================================\n","Saved best model\n","Epoch 232, Loss 34038608989440.00, Val loss 1726893785088.00\n","==================================================================\n","Saved best model\n","Epoch 233, Loss 34016526277632.00, Val loss 1726108008448.00\n","==================================================================\n","Saved best model\n","Epoch 234, Loss 33992719969536.00, Val loss 1724939108352.00\n","==================================================================\n","Saved best model\n","Epoch 235, Loss 33984978053376.00, Val loss 1724295806976.00\n","==================================================================\n","Saved best model\n","Epoch 236, Loss 33955242257408.00, Val loss 1723286290432.00\n","==================================================================\n","Saved best model\n","Epoch 237, Loss 33940070431232.00, Val loss 1722927284224.00\n","==================================================================\n","Saved best model\n","Epoch 238, Loss 33910473082368.00, Val loss 1722092617728.00\n","==================================================================\n","Saved best model\n","Epoch 239, Loss 33893847334400.00, Val loss 1721429917696.00\n","==================================================================\n","Saved best model\n","Epoch 240, Loss 33869494917632.00, Val loss 1720605212672.00\n","==================================================================\n","Saved best model\n","Epoch 241, Loss 33849529068032.00, Val loss 1719822057472.00\n","==================================================================\n","Saved best model\n","Epoch 242, Loss 33826217536768.00, Val loss 1718136471552.00\n","==================================================================\n","Saved best model\n","Epoch 243, Loss 33834642177536.00, Val loss 1717545336832.00\n","==================================================================\n","Saved best model\n","Epoch 244, Loss 33808573682176.00, Val loss 1716838465536.00\n","==================================================================\n","Saved best model\n","Epoch 245, Loss 33784235580928.00, Val loss 1715976011776.00\n","==================================================================\n","Saved best model\n","Epoch 246, Loss 33767240359936.00, Val loss 1715260882944.00\n","==================================================================\n","Saved best model\n","Epoch 247, Loss 33744183275520.00, Val loss 1714175868928.00\n","==================================================================\n","Saved best model\n","Epoch 248, Loss 33732072047616.00, Val loss 1713403854848.00\n","==================================================================\n","Saved best model\n","Epoch 249, Loss 33713992612864.00, Val loss 1712728702976.00\n","==================================================================\n","Saved best model\n","Epoch 250, Loss 33689126138880.00, Val loss 1711909240832.00\n","==================================================================\n","Saved best model\n","Epoch 251, Loss 33672623391744.00, Val loss 1711020310528.00\n","==================================================================\n","Saved best model\n","Epoch 252, Loss 33651683845120.00, Val loss 1710155628544.00\n","==================================================================\n","Saved best model\n","Epoch 253, Loss 33642232517632.00, Val loss 1709344817152.00\n","==================================================================\n","Saved best model\n","Epoch 254, Loss 33623713465856.00, Val loss 1708536233984.00\n","==================================================================\n","Saved best model\n","Epoch 255, Loss 33600884142336.00, Val loss 1707986255872.00\n","==================================================================\n","Saved best model\n","Epoch 256, Loss 33584340886784.00, Val loss 1707465768960.00\n","==================================================================\n","Saved best model\n","Epoch 257, Loss 33543588121600.00, Val loss 1706698473472.00\n","==================================================================\n","Saved best model\n","Epoch 258, Loss 33534605107968.00, Val loss 1706045210624.00\n","==================================================================\n","Saved best model\n","Epoch 259, Loss 33516104285696.00, Val loss 1705643868160.00\n","==================================================================\n","Saved best model\n","Epoch 260, Loss 33477146017024.00, Val loss 1705013542912.00\n","==================================================================\n","Saved best model\n","Epoch 261, Loss 33472476919296.00, Val loss 1704306409472.00\n","==================================================================\n","Saved best model\n","Epoch 262, Loss 33455015258880.00, Val loss 1704103641088.00\n","==================================================================\n","Saved best model\n","Epoch 263, Loss 33413406403328.00, Val loss 1702762905600.00\n","==================================================================\n","Saved best model\n","Epoch 264, Loss 33400825634048.00, Val loss 1702089719808.00\n","==================================================================\n","Saved best model\n","Epoch 265, Loss 33399156296704.00, Val loss 1701698338816.00\n","==================================================================\n","Saved best model\n","Epoch 266, Loss 33375965927936.00, Val loss 1701245616128.00\n","==================================================================\n","Saved best model\n","Epoch 267, Loss 33337923396608.00, Val loss 1700312907776.00\n","==================================================================\n","Saved best model\n","Epoch 268, Loss 33324904593152.00, Val loss 1699867131904.00\n","==================================================================\n","Saved best model\n","Epoch 269, Loss 33312009481728.00, Val loss 1699538403328.00\n","==================================================================\n","Saved best model\n","Epoch 270, Loss 33286250788352.00, Val loss 1698808594432.00\n","==================================================================\n","Saved best model\n","Epoch 271, Loss 33268336216576.00, Val loss 1698623389696.00\n","==================================================================\n","Saved best model\n","Epoch 272, Loss 33240944914944.00, Val loss 1697702871040.00\n","==================================================================\n","Saved best model\n","Epoch 273, Loss 33237898112768.00, Val loss 1696462929920.00\n","==================================================================\n","Saved best model\n","Epoch 274, Loss 33225549953536.00, Val loss 1694002184192.00\n","Epoch 275, Loss 33210562388480.00, Val loss 1695635341312.00\n","==================================================================\n","Saved best model\n","Epoch 276, Loss 33198659363328.00, Val loss 1692409528320.00\n","Epoch 277, Loss 33160459386880.00, Val loss 1693930618880.00\n","==================================================================\n","Saved best model\n","Epoch 278, Loss 33161124295680.00, Val loss 1690675052544.00\n","==================================================================\n","Saved best model\n","Epoch 279, Loss 33142125010432.00, Val loss 1690491420672.00\n","==================================================================\n","Saved best model\n","Epoch 280, Loss 33125266108928.00, Val loss 1688347738112.00\n","==================================================================\n","Saved best model\n","Epoch 281, Loss 33113276054528.00, Val loss 1687207542784.00\n","==================================================================\n","Saved best model\n","Epoch 282, Loss 33101809691392.00, Val loss 1685736914944.00\n","==================================================================\n","Saved best model\n","Epoch 283, Loss 33115126257920.00, Val loss 1685700870144.00\n","==================================================================\n","Saved best model\n","Epoch 284, Loss 33091409135872.00, Val loss 1684202979328.00\n","==================================================================\n","Saved best model\n","Epoch 285, Loss 33083352103424.00, Val loss 1682459197440.00\n","Epoch 286, Loss 33130760042240.00, Val loss 1683702022144.00\n","==================================================================\n","Saved best model\n","Epoch 287, Loss 33066775472128.00, Val loss 1681720999936.00\n","==================================================================\n","Saved best model\n","Epoch 288, Loss 33059747578368.00, Val loss 1680149184512.00\n","==================================================================\n","Saved best model\n","Epoch 289, Loss 33048623807232.00, Val loss 1678262796288.00\n","==================================================================\n","Saved best model\n","Epoch 290, Loss 33108150482176.00, Val loss 1677449756672.00\n","Epoch 291, Loss 36238388864512.00, Val loss 1701829804032.00\n","==================================================================\n","Saved best model\n","Epoch 292, Loss 33321982662912.00, Val loss 1676998213632.00\n","==================================================================\n","Saved best model\n","Epoch 293, Loss 32908078609152.00, Val loss 1675596136448.00\n","Epoch 294, Loss 32854432342784.00, Val loss 1676962430976.00\n","Epoch 295, Loss 32767857103872.00, Val loss 1680886726656.00\n","Epoch 296, Loss 32690545468416.00, Val loss 1681806852096.00\n","Epoch 297, Loss 32617865816832.00, Val loss 1686817210368.00\n","Epoch 298, Loss 32611914881536.00, Val loss 1686656778240.00\n","Epoch 299, Loss 32719517622272.00, Val loss 1678917894144.00\n","Epoch 300, Loss 32754004127488.00, Val loss 1683245498368.00\n","Epoch 301, Loss 32672825941760.00, Val loss 1678854717440.00\n","==================================================================\n","Saved best model\n","Epoch 302, Loss 32690319708416.00, Val loss 1674649272320.00\n","==================================================================\n","Saved best model\n","Epoch 303, Loss 32745065727488.00, Val loss 1673179561984.00\n","==================================================================\n","Saved best model\n","Epoch 304, Loss 32755890886912.00, Val loss 1671059865600.00\n","==================================================================\n","Saved best model\n","Epoch 305, Loss 32763889005824.00, Val loss 1669989400576.00\n","==================================================================\n","Saved best model\n","Epoch 306, Loss 32815326657024.00, Val loss 1668107075584.00\n","==================================================================\n","Saved best model\n","Epoch 307, Loss 32873989481472.00, Val loss 1667150774272.00\n","==================================================================\n","Saved best model\n","Epoch 308, Loss 32846546598656.00, Val loss 1666607480832.00\n","Epoch 309, Loss 32854687175936.00, Val loss 1666952069120.00\n","Epoch 310, Loss 32813121801216.00, Val loss 1666690580480.00\n","==================================================================\n","Saved best model\n","Epoch 311, Loss 32801111040768.00, Val loss 1666070085632.00\n","==================================================================\n","Saved best model\n","Epoch 312, Loss 32808586274048.00, Val loss 1665916993536.00\n","==================================================================\n","Saved best model\n","Epoch 313, Loss 32762718735616.00, Val loss 1663025152000.00\n","Epoch 314, Loss 32850709116672.00, Val loss 1663999541248.00\n","==================================================================\n","Saved best model\n","Epoch 315, Loss 32797098539264.00, Val loss 1662504402944.00\n","==================================================================\n","Saved best model\n","Epoch 316, Loss 32746847626496.00, Val loss 1661868048384.00\n","Epoch 317, Loss 32849404666112.00, Val loss 1662778998784.00\n","==================================================================\n","Saved best model\n","Epoch 318, Loss 32728143882496.00, Val loss 1661020274688.00\n","Epoch 319, Loss 32792665423616.00, Val loss 1663037472768.00\n","Epoch 320, Loss 32652670535680.00, Val loss 1666694381568.00\n","Epoch 321, Loss 32581485109248.00, Val loss 1668669767680.00\n","Epoch 322, Loss 32493699991808.00, Val loss 1669846925312.00\n","Epoch 323, Loss 32522591192064.00, Val loss 1665746337792.00\n","Epoch 324, Loss 32516668584960.00, Val loss 1665545797632.00\n","Epoch 325, Loss 32516009702656.00, Val loss 1665803878400.00\n","Epoch 326, Loss 32490278550272.00, Val loss 1665581973504.00\n","Epoch 327, Loss 32473569697792.00, Val loss 1664896860160.00\n","Epoch 328, Loss 32458836496896.00, Val loss 1664165216256.00\n","Epoch 329, Loss 32461737656576.00, Val loss 1666420441088.00\n","Epoch 330, Loss 32384557323520.00, Val loss 1677095993344.00\n","Epoch 331, Loss 32290765156352.00, Val loss 1682037407744.00\n","Epoch 332, Loss 32213067414016.00, Val loss 1678771879936.00\n","Epoch 333, Loss 32234096626688.00, Val loss 1679654387712.00\n","Epoch 334, Loss 32188556722432.00, Val loss 1676109545472.00\n","Epoch 335, Loss 32220166256128.00, Val loss 1676871729152.00\n","Epoch 336, Loss 32204382423040.00, Val loss 1674899226624.00\n","Epoch 337, Loss 32161715374336.00, Val loss 1675048910848.00\n","Epoch 338, Loss 32173534717184.00, Val loss 1673038397440.00\n","Epoch 339, Loss 32160847626240.00, Val loss 1674906828800.00\n","Epoch 340, Loss 32131839269120.00, Val loss 1674669981696.00\n","Epoch 341, Loss 32113990602752.00, Val loss 1672944418816.00\n","Epoch 342, Loss 32106484148992.00, Val loss 1666002845696.00\n","Epoch 343, Loss 32213208093952.00, Val loss 1662983471104.00\n","Epoch 344, Loss 32251746329600.00, Val loss 1667362717696.00\n","Epoch 345, Loss 32166970961920.00, Val loss 1675130961920.00\n","==================================================================\n","Saved best model\n","Epoch 346, Loss 31991603574016.00, Val loss 1659737473024.00\n","Epoch 347, Loss 32191938663680.00, Val loss 1660888678400.00\n","Epoch 348, Loss 32242007674880.00, Val loss 1669511905280.00\n","==================================================================\n","Saved best model\n","Epoch 349, Loss 32042760896256.00, Val loss 1655149297664.00\n","Epoch 350, Loss 32239305262848.00, Val loss 1667428253696.00\n","Epoch 351, Loss 32077410892032.00, Val loss 1656682053632.00\n","Epoch 352, Loss 32150758151424.00, Val loss 1657073172480.00\n","Epoch 353, Loss 32192376978944.00, Val loss 1666910126080.00\n","Epoch 354, Loss 32052596430592.00, Val loss 1658896384000.00\n","Epoch 355, Loss 32070951406592.00, Val loss 1664345964544.00\n","Epoch 356, Loss 32077995876352.00, Val loss 1660289679360.00\n","==================================================================\n","Saved best model\n","Epoch 357, Loss 32039504393216.00, Val loss 1650664144896.00\n","Epoch 358, Loss 32144984494848.00, Val loss 1653515616256.00\n","Epoch 359, Loss 32088104363264.00, Val loss 1651247284224.00\n","Epoch 360, Loss 32257913850368.00, Val loss 1656197349376.00\n","Epoch 361, Loss 31954639176192.00, Val loss 1661998202880.00\n","Epoch 362, Loss 32059355698688.00, Val loss 1659919794176.00\n","Epoch 363, Loss 32006564125440.00, Val loss 1654985588736.00\n","Epoch 364, Loss 31923694930432.00, Val loss 1651890192384.00\n","Epoch 365, Loss 32036772054272.00, Val loss 1652006060032.00\n","==================================================================\n","Saved best model\n","Epoch 366, Loss 31936440514048.00, Val loss 1642599284736.00\n","Epoch 367, Loss 32153831552256.00, Val loss 1650960367616.00\n","Epoch 368, Loss 31919195599616.00, Val loss 1650319818752.00\n","Epoch 369, Loss 32026365442048.00, Val loss 1654181068800.00\n","Epoch 370, Loss 31929722488832.00, Val loss 1648359112704.00\n","Epoch 371, Loss 31929513282816.00, Val loss 1646315175936.00\n","Epoch 372, Loss 31925584324096.00, Val loss 1647225470976.00\n","Epoch 373, Loss 31928900249344.00, Val loss 1653609332736.00\n","==================================================================\n","Saved best model\n","Epoch 374, Loss 31775861529600.00, Val loss 1642229661696.00\n","Epoch 375, Loss 31997958929664.00, Val loss 1656489246720.00\n","==================================================================\n","Saved best model\n","Epoch 376, Loss 31777651723264.00, Val loss 1641786245120.00\n","Epoch 377, Loss 31945273921024.00, Val loss 1643648647168.00\n","Epoch 378, Loss 31917339799296.00, Val loss 1657939689472.00\n","Epoch 379, Loss 31750409268992.00, Val loss 1645682491392.00\n","Epoch 380, Loss 31820266081792.00, Val loss 1643992973312.00\n","Epoch 381, Loss 31897432742400.00, Val loss 1647618162688.00\n","Epoch 382, Loss 31836763040768.00, Val loss 1644594593792.00\n","==================================================================\n","Saved best model\n","Epoch 383, Loss 32180954933248.00, Val loss 1634966568960.00\n","Epoch 384, Loss 31985278020608.00, Val loss 1648121085952.00\n","Epoch 385, Loss 31715483408896.00, Val loss 1649166909440.00\n","Epoch 386, Loss 31754806903296.00, Val loss 1643243765760.00\n","==================================================================\n","Saved best model\n","Epoch 387, Loss 31680277339904.00, Val loss 1631353569280.00\n","Epoch 388, Loss 32006927758848.00, Val loss 1640883027968.00\n","Epoch 389, Loss 31812299439104.00, Val loss 1643970691072.00\n","Epoch 390, Loss 31793523069696.00, Val loss 1645460193280.00\n","==================================================================\n","Saved best model\n","Epoch 391, Loss 31835541050112.00, Val loss 1626395246592.00\n","Epoch 392, Loss 32120375276288.00, Val loss 1641235349504.00\n","Epoch 393, Loss 31825165596928.00, Val loss 1628945383424.00\n","Epoch 394, Loss 32057684275456.00, Val loss 1630698733568.00\n","Epoch 395, Loss 31960924599040.00, Val loss 1636004003840.00\n","Epoch 396, Loss 31834580968192.00, Val loss 1635618390016.00\n","Epoch 397, Loss 31908279588096.00, Val loss 1634110799872.00\n","Epoch 398, Loss 31899747774464.00, Val loss 1631107153920.00\n","Epoch 399, Loss 32005922426880.00, Val loss 1631752159232.00\n","Epoch 400, Loss 32012449583616.00, Val loss 1632934821888.00\n","Epoch 401, Loss 31979126877440.00, Val loss 1630250467328.00\n","Epoch 402, Loss 32077827390720.00, Val loss 1650929958912.00\n","Epoch 403, Loss 31734463155968.00, Val loss 1626972618752.00\n","Epoch 404, Loss 32077724118784.00, Val loss 1638469992448.00\n","Epoch 405, Loss 31846945924352.00, Val loss 1629027565568.00\n","==================================================================\n","Saved best model\n","Epoch 406, Loss 33153923773440.00, Val loss 1612969410560.00\n","Epoch 407, Loss 37429619208448.00, Val loss 1680415260672.00\n","Epoch 408, Loss 30929714466816.00, Val loss 1625278119936.00\n","==================================================================\n","Saved best model\n","Epoch 409, Loss 34876040547328.00, Val loss 1609162162176.00\n","Epoch 410, Loss 34272535717888.00, Val loss 1643691376640.00\n","==================================================================\n","Saved best model\n","Epoch 411, Loss 36639063095552.00, Val loss 1599025971200.00\n","Epoch 412, Loss 87556493120512.00, Val loss 4376247926784.00\n","Epoch 413, Loss 1371719799305216.00, Val loss 4269139558400.00\n","Epoch 414, Loss 4816874899453696.00, Val loss 4190164484096.00\n","Epoch 415, Loss 76911474368512.00, Val loss 4125331554304.00\n","Epoch 416, Loss 75177349423104.00, Val loss 4069579292672.00\n","Epoch 417, Loss 73756107630592.00, Val loss 4020410515456.00\n","Epoch 418, Loss 72567678996480.00, Val loss 3976366391296.00\n","Epoch 419, Loss 71561635885056.00, Val loss 3936507658240.00\n","Epoch 420, Loss 70703367348224.00, Val loss 3900183150592.00\n","Epoch 421, Loss 69967628976128.00, Val loss 3866918649856.00\n","Epoch 422, Loss 69335126573056.00, Val loss 3836354232320.00\n","Epoch 423, Loss 68790572163072.00, Val loss 3808202850304.00\n","Epoch 424, Loss 68321538932736.00, Val loss 3782233817088.00\n","Epoch 425, Loss 67917673758720.00, Val loss 3758254456832.00\n","Epoch 426, Loss 67570226569216.00, Val loss 3736098045952.00\n","Epoch 427, Loss 67271712391168.00, Val loss 3715621978112.00\n","Epoch 428, Loss 67015658725376.00, Val loss 3696698064896.00\n","Epoch 429, Loss 66796448849920.00, Val loss 3679211749376.00\n","Epoch 430, Loss 66609170530304.00, Val loss 3663058436096.00\n","Epoch 431, Loss 38848826609549312.00, Val loss 3647623921664.00\n","Epoch 432, Loss 66309261582336.00, Val loss 3633890197504.00\n","Epoch 433, Loss 66194750586880.00, Val loss 3621223137280.00\n","Epoch 434, Loss 66097888632832.00, Val loss 3609545670656.00\n","Epoch 435, Loss 66016166518784.00, Val loss 3598785708032.00\n","Epoch 436, Loss 65947401945088.00, Val loss 3588877451264.00\n","Epoch 437, Loss 65889702141952.00, Val loss 3579757199360.00\n","Epoch 438, Loss 65841416822784.00, Val loss 3571366756352.00\n","Epoch 439, Loss 65801138192384.00, Val loss 3563651334144.00\n","Epoch 440, Loss 65767632781312.00, Val loss 3556559552512.00\n","Epoch 441, Loss 65739856830464.00, Val loss 3550043701248.00\n","Epoch 442, Loss 65716903190528.00, Val loss 3544058953728.00\n","Epoch 443, Loss 65698014224384.00, Val loss 3538564415488.00\n","Epoch 444, Loss 65682519080960.00, Val loss 3533521289216.00\n","Epoch 445, Loss 65669869543424.00, Val loss 3528893399040.00\n","Epoch 446, Loss 65659591532544.00, Val loss 3524647976960.00\n","Epoch 447, Loss 65651284910080.00, Val loss 3520753565696.00\n","Epoch 448, Loss 65644613451776.00, Val loss 3517183164416.00\n","Epoch 449, Loss 65639291871232.00, Val loss 3513908985856.00\n","Epoch 450, Loss 65635085025280.00, Val loss 3510907174912.00\n","Epoch 451, Loss 65631788515328.00, Val loss 3508156497920.00\n","Epoch 452, Loss 65629235519488.00, Val loss 3505634672640.00\n","Epoch 453, Loss 65627292073984.00, Val loss 3503322824704.00\n","Epoch 454, Loss 65625842712576.00, Val loss 3501204701184.00\n","Epoch 455, Loss 65624788115456.00, Val loss 3499263262720.00\n","Epoch 456, Loss 65624051490816.00, Val loss 3497484353536.00\n","Epoch 457, Loss 65623568072704.00, Val loss 3495854866432.00\n","Epoch 458, Loss 65623281754112.00, Val loss 3494361432064.00\n","Epoch 459, Loss 65623146356736.00, Val loss 3492992253952.00\n","Epoch 460, Loss 65623132225536.00, Val loss 3491737894912.00\n","Epoch 461, Loss 65623203028992.00, Val loss 3490588393472.00\n","Epoch 462, Loss 65623339212800.00, Val loss 3489535098880.00\n","Epoch 463, Loss 65623524179968.00, Val loss 3488568573952.00\n","Epoch 464, Loss 65623736852480.00, Val loss 3487683837952.00\n","Epoch 465, Loss 65623969972224.00, Val loss 3486872240128.00\n","Epoch 466, Loss 65624212365312.00, Val loss 3486129061888.00\n","Epoch 467, Loss 65624457232384.00, Val loss 3485447225344.00\n","Epoch 468, Loss 65624697208832.00, Val loss 3484822274048.00\n","Epoch 469, Loss 65624929599488.00, Val loss 3484249227264.00\n","Epoch 470, Loss 65625149874176.00, Val loss 3483723628544.00\n","Epoch 471, Loss 65625357189120.00, Val loss 3483242332160.00\n","Epoch 472, Loss 65625551118336.00, Val loss 3482800357376.00\n","Epoch 473, Loss 65625725190144.00, Val loss 3482394558464.00\n","Epoch 474, Loss 65625887293440.00, Val loss 3482022838272.00\n","Epoch 475, Loss 65626029719552.00, Val loss 3481681526784.00\n","Epoch 476, Loss 65626160832512.00, Val loss 3481368788992.00\n","Epoch 477, Loss 65626267516928.00, Val loss 3481080692736.00\n","Epoch 478, Loss 65626366894080.00, Val loss 3480816975872.00\n","Epoch 479, Loss 65626452844544.00, Val loss 3480574754816.00\n","Epoch 480, Loss 65626519519232.00, Val loss 3480352456704.00\n","Epoch 481, Loss 65626575200256.00, Val loss 3480147722240.00\n","Epoch 482, Loss 65626620461056.00, Val loss 3479960813568.00\n","Epoch 483, Loss 65626653130752.00, Val loss 3479788060672.00\n","Epoch 484, Loss 65626673971200.00, Val loss 3479629463552.00\n","Epoch 485, Loss 65626686210048.00, Val loss 3479483449344.00\n","Epoch 486, Loss 65626693246976.00, Val loss 3479349493760.00\n","Epoch 487, Loss 65626685505536.00, Val loss 3479226286080.00\n","Epoch 488, Loss 66007745798144.00, Val loss 3479101505536.00\n","Epoch 489, Loss 65626667237376.00, Val loss 3478997958656.00\n","Epoch 490, Loss 65626642915328.00, Val loss 3478902538240.00\n","Epoch 491, Loss 65626613055488.00, Val loss 3478814720000.00\n","Epoch 492, Loss 65626574659584.00, Val loss 3478733455360.00\n","Epoch 493, Loss 65626538459136.00, Val loss 3478659268608.00\n","Epoch 494, Loss 65626489929728.00, Val loss 3478589800448.00\n","Epoch 495, Loss 65626443997184.00, Val loss 3478526361600.00\n","Epoch 496, Loss 65626390986752.00, Val loss 3478467641344.00\n","Epoch 497, Loss 65626339483648.00, Val loss 3478413115392.00\n","Epoch 498, Loss 65626282688512.00, Val loss 3478362521600.00\n","Epoch 499, Loss 65626224631808.00, Val loss 3478316122112.00\n","Epoch 500, Loss 65626167926784.00, Val loss 3478272606208.00\n","Epoch 501, Loss 65626108059648.00, Val loss 3478233022464.00\n","Epoch 502, Loss 65626042449920.00, Val loss 3478196060160.00\n","Epoch 503, Loss 65625983066112.00, Val loss 3478161195008.00\n","Epoch 504, Loss 65625916850176.00, Val loss 3478129213440.00\n","Epoch 505, Loss 65625852420096.00, Val loss 3478099329024.00\n","Epoch 506, Loss 65625789308928.00, Val loss 3478071803904.00\n","Epoch 507, Loss 65625723371520.00, Val loss 3478045589504.00\n","Epoch 508, Loss 65625660203008.00, Val loss 3478021210112.00\n","Epoch 509, Loss 65625593921536.00, Val loss 3477998665728.00\n","Epoch 510, Loss 65625527885824.00, Val loss 3477977694208.00\n","Epoch 511, Loss 65625461309440.00, Val loss 3477958295552.00\n","Epoch 512, Loss 65625392775168.00, Val loss 3477939159040.00\n","Epoch 513, Loss 65625325223936.00, Val loss 3477922119680.00\n","Epoch 514, Loss 65625261408256.00, Val loss 3477905604608.00\n","Epoch 515, Loss 65625197150208.00, Val loss 3477890400256.00\n","Epoch 516, Loss 65625130270720.00, Val loss 3477875458048.00\n","Epoch 517, Loss 65625060843520.00, Val loss 3477862612992.00\n","Epoch 518, Loss 65624993718272.00, Val loss 3477849767936.00\n","Epoch 519, Loss 65624927084544.00, Val loss 3477837709312.00\n","Epoch 520, Loss 65624859779072.00, Val loss 3477826437120.00\n","Epoch 521, Loss 65624791343104.00, Val loss 3477815689216.00\n","Epoch 522, Loss 65624724193280.00, Val loss 3477805727744.00\n","Epoch 523, Loss 65624656076800.00, Val loss 3477796028416.00\n","Epoch 524, Loss 65624587231232.00, Val loss 3477786853376.00\n","Epoch 525, Loss 65624519606272.00, Val loss 3477778202624.00\n","Epoch 526, Loss 65624453062656.00, Val loss 3477770338304.00\n","Epoch 527, Loss 65624383733760.00, Val loss 3477762473984.00\n","Epoch 528, Loss 65624316428288.00, Val loss 3477754871808.00\n","Epoch 529, Loss 65624248369152.00, Val loss 3477747531776.00\n","Epoch 530, Loss 65624179269632.00, Val loss 3477740716032.00\n","Epoch 531, Loss 65624112521216.00, Val loss 3477734162432.00\n","Epoch 532, Loss 65624045109248.00, Val loss 3477727870976.00\n","Epoch 533, Loss 65623978541056.00, Val loss 3477722103808.00\n","Epoch 534, Loss 65623909965824.00, Val loss 3477715812352.00\n","Epoch 535, Loss 65623845183488.00, Val loss 3477710831616.00\n","Epoch 536, Loss 65623779205120.00, Val loss 3477704802304.00\n","Epoch 537, Loss 65623715848192.00, Val loss 3477700345856.00\n","Epoch 538, Loss 65623649402880.00, Val loss 3477695627264.00\n","Epoch 539, Loss 65623584473088.00, Val loss 3477690646528.00\n","Epoch 540, Loss 65623518380032.00, Val loss 3477686190080.00\n","Epoch 541, Loss 65623454310400.00, Val loss 3477682257920.00\n","Epoch 542, Loss 65623390248960.00, Val loss 3477677801472.00\n","Epoch 543, Loss 65623327342592.00, Val loss 3477673607168.00\n","Epoch 544, Loss 65623264886784.00, Val loss 3477669675008.00\n","Epoch 545, Loss 65623201218560.00, Val loss 3477666004992.00\n","Epoch 546, Loss 65623139672064.00, Val loss 3477661810688.00\n","Epoch 547, Loss 65623077634048.00, Val loss 3477658140672.00\n","Epoch 548, Loss 65623018577920.00, Val loss 3477654994944.00\n","Epoch 549, Loss 65622957400064.00, Val loss 3477651587072.00\n","Epoch 550, Loss 65622896091136.00, Val loss 3477648179200.00\n","Epoch 551, Loss 65622835544064.00, Val loss 3477645033472.00\n","Epoch 552, Loss 65622778355712.00, Val loss 3477641887744.00\n","Epoch 553, Loss 65622717775872.00, Val loss 3477638742016.00\n","Epoch 554, Loss 65622660055040.00, Val loss 3477635596288.00\n","Epoch 555, Loss 65622601334784.00, Val loss 3477633236992.00\n","Epoch 556, Loss 65622543581184.00, Val loss 3477630091264.00\n","Epoch 557, Loss 65622486933504.00, Val loss 3477627469824.00\n","Epoch 558, Loss 65622428876800.00, Val loss 3477624586240.00\n","Epoch 559, Loss 65622371590144.00, Val loss 3477621964800.00\n","Epoch 560, Loss 65622316105728.00, Val loss 3477619081216.00\n","Epoch 561, Loss 65622258933760.00, Val loss 3477617246208.00\n","Epoch 562, Loss 65622202564608.00, Val loss 3477614362624.00\n","Epoch 563, Loss 65622146875392.00, Val loss 3477611741184.00\n","Epoch 564, Loss 65622092881920.00, Val loss 3477609119744.00\n","Epoch 565, Loss 65622038913024.00, Val loss 3477606760448.00\n","Epoch 566, Loss 65621985714176.00, Val loss 3477604401152.00\n","Epoch 567, Loss 65621931737088.00, Val loss 3477602041856.00\n","Epoch 568, Loss 65621878300672.00, Val loss 3477599944704.00\n","Epoch 569, Loss 65621825388544.00, Val loss 3477597585408.00\n","Epoch 570, Loss 65621772902400.00, Val loss 3477595488256.00\n","Epoch 571, Loss 65621720104960.00, Val loss 3477593391104.00\n","Epoch 572, Loss 65621667282944.00, Val loss 3477591293952.00\n","Epoch 573, Loss 65621614862336.00, Val loss 3477588934656.00\n","Epoch 574, Loss 65621564784640.00, Val loss 3477586837504.00\n","Epoch 575, Loss 65621512445952.00, Val loss 3477584740352.00\n","Epoch 576, Loss 65621460787200.00, Val loss 3477582643200.00\n","Epoch 577, Loss 65621409644544.00, Val loss 3477580546048.00\n","Epoch 578, Loss 65621359837184.00, Val loss 3477578448896.00\n","Epoch 579, Loss 65621309022208.00, Val loss 3477576613888.00\n","Epoch 580, Loss 65621259042816.00, Val loss 3477574516736.00\n","Epoch 581, Loss 65621210169344.00, Val loss 3477572419584.00\n","Epoch 582, Loss 65621159788544.00, Val loss 3477570322432.00\n","Epoch 583, Loss 65621112471552.00, Val loss 3477568487424.00\n","Epoch 584, Loss 65621063032832.00, Val loss 3477566652416.00\n","Epoch 585, Loss 65621013839872.00, Val loss 3477564817408.00\n","Epoch 586, Loss 65620966187008.00, Val loss 3477562720256.00\n","Epoch 587, Loss 65620917190656.00, Val loss 3477561147392.00\n","Epoch 588, Loss 65620868194304.00, Val loss 3477559312384.00\n","Epoch 589, Loss 65620820590592.00, Val loss 3477557477376.00\n","Epoch 590, Loss 65620771536896.00, Val loss 3477555642368.00\n","Epoch 591, Loss 65620724899840.00, Val loss 3477553807360.00\n","Epoch 592, Loss 65620678074368.00, Val loss 3477551710208.00\n","Epoch 593, Loss 65620631003136.00, Val loss 3477550137344.00\n","Epoch 594, Loss 65620584849408.00, Val loss 3477547515904.00\n","Epoch 595, Loss 65620538933248.00, Val loss 3477546467328.00\n","Epoch 596, Loss 65620491149312.00, Val loss 3477544632320.00\n","Epoch 597, Loss 65620445872128.00, Val loss 3477542797312.00\n","Epoch 598, Loss 65620399497216.00, Val loss 3477541748736.00\n","Epoch 599, Loss 65620353056768.00, Val loss 3477539389440.00\n","Epoch 600, Loss 65620307722240.00, Val loss 3477538340864.00\n","Epoch 601, Loss 65620262174720.00, Val loss 3477536243712.00\n","Epoch 602, Loss 65620217085952.00, Val loss 3477534932992.00\n","Epoch 603, Loss 65620172939264.00, Val loss 3477533097984.00\n","Epoch 604, Loss 65620127326208.00, Val loss 3477531787264.00\n","Epoch 605, Loss 65620082376704.00, Val loss 3477530738688.00\n","Epoch 606, Loss 65620036763648.00, Val loss 3477528641536.00\n","Epoch 607, Loss 65619992338432.00, Val loss 3477527330816.00\n","Epoch 608, Loss 65619948101632.00, Val loss 3477525495808.00\n","Epoch 609, Loss 65619903275008.00, Val loss 3477524185088.00\n","Epoch 610, Loss 65619859210240.00, Val loss 3477522612224.00\n","Epoch 611, Loss 65619814113280.00, Val loss 3477521039360.00\n","Epoch 612, Loss 65619769122816.00, Val loss 3477519990784.00\n","Epoch 613, Loss 65619726524416.00, Val loss 3477518417920.00\n","Epoch 614, Loss 65619681976320.00, Val loss 3477517107200.00\n","Epoch 615, Loss 65619638460416.00, Val loss 3477515272192.00\n","Epoch 616, Loss 65619594166272.00, Val loss 3477513961472.00\n","Epoch 617, Loss 65619551354880.00, Val loss 3477512388608.00\n","Epoch 618, Loss 65619508060160.00, Val loss 3477510815744.00\n","Epoch 619, Loss 65619464757248.00, Val loss 3477509767168.00\n","Epoch 620, Loss 65619422060544.00, Val loss 3477507670016.00\n","Epoch 621, Loss 65619379085312.00, Val loss 3477506359296.00\n","Epoch 622, Loss 65619336085504.00, Val loss 3477504524288.00\n","Epoch 623, Loss 65619293028352.00, Val loss 3477503213568.00\n","Epoch 624, Loss 65619250003968.00, Val loss 3477501902848.00\n","Epoch 625, Loss 65619208347648.00, Val loss 3477500329984.00\n","Epoch 626, Loss 65619165233152.00, Val loss 3477499019264.00\n","Epoch 627, Loss 65619123216384.00, Val loss 3477497446400.00\n","Epoch 628, Loss 65619080880128.00, Val loss 3477496397824.00\n","Epoch 629, Loss 65619038355456.00, Val loss 3477494562816.00\n","Epoch 630, Loss 65618996109312.00, Val loss 3477493252096.00\n","Epoch 631, Loss 65618953428992.00, Val loss 3477491941376.00\n","Epoch 632, Loss 65618912018432.00, Val loss 3477490106368.00\n","Epoch 633, Loss 65618870435840.00, Val loss 3477489057792.00\n","Epoch 634, Loss 65618828148736.00, Val loss 3477487484928.00\n","Epoch 635, Loss 65618785132544.00, Val loss 3477486436352.00\n","Epoch 636, Loss 65618744426496.00, Val loss 3477484601344.00\n","Epoch 637, Loss 65618701008896.00, Val loss 3477483028480.00\n","Epoch 638, Loss 65618660179968.00, Val loss 3477481455616.00\n","Epoch 639, Loss 65618619006976.00, Val loss 3477480407040.00\n","Epoch 640, Loss 65618576908288.00, Val loss 3477478834176.00\n","Epoch 641, Loss 65618535776256.00, Val loss 3477477523456.00\n","Epoch 642, Loss 65618493546496.00, Val loss 3477475688448.00\n","Epoch 643, Loss 65618452865024.00, Val loss 3477474639872.00\n","Epoch 644, Loss 65618411118592.00, Val loss 3477473329152.00\n","Epoch 645, Loss 65618370207744.00, Val loss 3477472018432.00\n","Epoch 646, Loss 65618329796608.00, Val loss 3477470445568.00\n","Epoch 647, Loss 25636443477614592.00, Val loss 3476863844352.00\n","Epoch 648, Loss 65619056238592.00, Val loss 3476913651712.00\n","Epoch 649, Loss 65618942156800.00, Val loss 3476959002624.00\n","Epoch 650, Loss 65618833866752.00, Val loss 3477001469952.00\n","Epoch 651, Loss 65618730655744.00, Val loss 3477038956544.00\n","Epoch 652, Loss 65618633064448.00, Val loss 3477074083840.00\n","Epoch 653, Loss 39954720699244544.00, Val loss 3476486356992.00\n","Epoch 654, Loss 65619303751680.00, Val loss 3476567359488.00\n","Epoch 655, Loss 65619143196672.00, Val loss 3476641546240.00\n","Epoch 656, Loss 65618992840704.00, Val loss 3476709703680.00\n","Epoch 657, Loss 65618852536320.00, Val loss 3476771307520.00\n","Epoch 658, Loss 65618726191104.00, Val loss 3476827668480.00\n","Epoch 659, Loss 65618604523520.00, Val loss 3476879835136.00\n","Epoch 660, Loss 65618489294848.00, Val loss 3476927545344.00\n","Epoch 661, Loss 65618381561856.00, Val loss 3476970799104.00\n","Epoch 662, Loss 65618279055360.00, Val loss 3477010120704.00\n","Epoch 663, Loss 65618179973120.00, Val loss 3477046558720.00\n","Epoch 664, Loss 65618093047808.00, Val loss 3477079588864.00\n","Epoch 665, Loss 65618004197376.00, Val loss 3477109473280.00\n","Epoch 666, Loss 65617921605632.00, Val loss 3477136736256.00\n","Epoch 667, Loss 65617845526528.00, Val loss 3477161902080.00\n","Epoch 668, Loss 65617772240896.00, Val loss 3477184970752.00\n","Epoch 669, Loss 65617696899072.00, Val loss 3477205417984.00\n","Epoch 670, Loss 65617629511680.00, Val loss 3477224816640.00\n","Epoch 671, Loss 65617566343168.00, Val loss 3477242118144.00\n","Epoch 672, Loss 65617498472448.00, Val loss 3477257846784.00\n","Epoch 673, Loss 65617438638080.00, Val loss 3477272002560.00\n","Epoch 674, Loss 65617378353152.00, Val loss 3477285371904.00\n","Epoch 675, Loss 65617320075264.00, Val loss 3477296644096.00\n","Epoch 676, Loss 65617262559232.00, Val loss 3477307654144.00\n","Epoch 677, Loss 65617209409536.00, Val loss 3477317091328.00\n","Epoch 678, Loss 65617155399680.00, Val loss 3477326004224.00\n","Epoch 679, Loss 65617101512704.00, Val loss 3477334130688.00\n","Epoch 680, Loss 65617051549696.00, Val loss 3477341208576.00\n","Epoch 681, Loss 65617000800256.00, Val loss 3477347237888.00\n","Epoch 682, Loss 65616950386688.00, Val loss 3477353529344.00\n","Epoch 683, Loss 65616904929280.00, Val loss 3477359034368.00\n","Epoch 684, Loss 65616854507520.00, Val loss 3477363490816.00\n","Epoch 685, Loss 65616806731776.00, Val loss 3477367947264.00\n","Epoch 686, Loss 65616759562240.00, Val loss 3477371355136.00\n","Epoch 687, Loss 65616714899456.00, Val loss 3477374763008.00\n","Epoch 688, Loss 65616670023680.00, Val loss 3477377908736.00\n","Epoch 689, Loss 65616624730112.00, Val loss 3477380268032.00\n","Epoch 690, Loss 65616581165056.00, Val loss 3477383151616.00\n","Epoch 691, Loss 65616537550848.00, Val loss 3477385248768.00\n","Epoch 692, Loss 65616493887488.00, Val loss 3477387345920.00\n","Epoch 693, Loss 65616452255744.00, Val loss 3477388918784.00\n","Epoch 694, Loss 65616409313280.00, Val loss 3477389967360.00\n","Epoch 695, Loss 65616366116864.00, Val loss 3477391278080.00\n","Epoch 696, Loss 65616323264512.00, Val loss 3477392326656.00\n","Epoch 697, Loss 65616281010176.00, Val loss 3477393113088.00\n","Epoch 698, Loss 65616237953024.00, Val loss 3477393899520.00\n","Epoch 699, Loss 65616196214784.00, Val loss 3477394161664.00\n","Epoch 700, Loss 65616155443200.00, Val loss 3477394685952.00\n","Epoch 701, Loss 65616114786304.00, Val loss 3477394685952.00\n","Epoch 702, Loss 65616073629696.00, Val loss 3477394685952.00\n","Epoch 703, Loss 65616033480704.00, Val loss 3477394685952.00\n","Epoch 704, Loss 65615991439360.00, Val loss 3477394685952.00\n","Epoch 705, Loss 65615952044032.00, Val loss 3477394685952.00\n","Epoch 706, Loss 65615910887424.00, Val loss 3477394161664.00\n","Epoch 707, Loss 65615871025152.00, Val loss 3477394161664.00\n","Epoch 708, Loss 65615830835200.00, Val loss 3477393375232.00\n","Epoch 709, Loss 65615789506560.00, Val loss 3477393375232.00\n","Epoch 710, Loss 66011538481152.00, Val loss 3476763181056.00\n","Epoch 711, Loss 65616030113792.00, Val loss 3476819279872.00\n","Epoch 712, Loss 65615909027840.00, Val loss 3476870922240.00\n","Epoch 713, Loss 65615796019200.00, Val loss 3476917846016.00\n","Epoch 714, Loss 65615689957376.00, Val loss 3476961624064.00\n","Epoch 715, Loss 65615587500032.00, Val loss 3477000421376.00\n","Epoch 716, Loss 65615493341184.00, Val loss 3477036072960.00\n","Epoch 717, Loss 65615402041344.00, Val loss 3477069365248.00\n","Epoch 718, Loss 65615317639168.00, Val loss 3477099249664.00\n","Epoch 719, Loss 65615238742016.00, Val loss 3477126774784.00\n","Epoch 720, Loss 65615160369152.00, Val loss 3477151416320.00\n","Epoch 721, Loss 65615089278976.00, Val loss 3477174484992.00\n","Epoch 722, Loss 65615018057728.00, Val loss 3477195194368.00\n","Epoch 723, Loss 65614950645760.00, Val loss 3477214068736.00\n","Epoch 724, Loss 65614885814272.00, Val loss 3477231632384.00\n","Epoch 725, Loss 65614823137280.00, Val loss 3477247623168.00\n","Epoch 726, Loss 65614763237376.00, Val loss 3477261516800.00\n","Epoch 727, Loss 65614703575040.00, Val loss 3477274361856.00\n","Epoch 728, Loss 65614648418304.00, Val loss 3477286682624.00\n","Epoch 729, Loss 65614592745472.00, Val loss 3477297168384.00\n","Epoch 730, Loss 65614540464128.00, Val loss 3477306605568.00\n","Epoch 731, Loss 65614490927104.00, Val loss 3477315780608.00\n","Epoch 732, Loss 65614440538112.00, Val loss 3477323907072.00\n","Epoch 733, Loss 65614393229312.00, Val loss 3477330722816.00\n","Epoch 734, Loss 65614343708672.00, Val loss 3477337276416.00\n","Epoch 735, Loss 65614298464256.00, Val loss 3477343305728.00\n","Epoch 736, Loss 65614254333952.00, Val loss 3477348810752.00\n","Epoch 737, Loss 65614208589824.00, Val loss 3477353529344.00\n","Epoch 738, Loss 65614163173376.00, Val loss 3477357723648.00\n","Epoch 739, Loss 65614119747584.00, Val loss 3477361393664.00\n","Epoch 740, Loss 65614078238720.00, Val loss 3477364539392.00\n","Epoch 741, Loss 65614036393984.00, Val loss 3477367160832.00\n","Epoch 742, Loss 65613995933696.00, Val loss 3477370044416.00\n","Epoch 743, Loss 65613955702784.00, Val loss 3477372141568.00\n","Epoch 744, Loss 65613916209152.00, Val loss 3477373714432.00\n","Epoch 745, Loss 65613878345728.00, Val loss 3477375549440.00\n","Epoch 746, Loss 65613840351232.00, Val loss 3477376598016.00\n","Epoch 747, Loss 65613800275968.00, Val loss 3477377646592.00\n","Epoch 748, Loss 65613764313088.00, Val loss 3477378170880.00\n","Epoch 749, Loss 65613727195136.00, Val loss 3477378957312.00\n","Epoch 750, Loss 65613691838464.00, Val loss 3477379219456.00\n","Epoch 751, Loss 65613656530944.00, Val loss 3477379219456.00\n","Epoch 752, Loss 65613621370880.00, Val loss 3477379219456.00\n","Epoch 753, Loss 65613587496960.00, Val loss 3477379219456.00\n","Epoch 754, Loss 65613553303552.00, Val loss 3477378695168.00\n","Epoch 755, Loss 65613521182720.00, Val loss 3477378170880.00\n","Epoch 756, Loss 65613489029120.00, Val loss 3477378170880.00\n","Epoch 757, Loss 65613456064512.00, Val loss 3477377646592.00\n","Epoch 758, Loss 65613423927296.00, Val loss 3477376860160.00\n","Epoch 759, Loss 65613394583552.00, Val loss 3477375549440.00\n","Epoch 760, Loss 65613363200000.00, Val loss 3477374500864.00\n","Epoch 761, Loss 65613332955136.00, Val loss 3477373714432.00\n","Epoch 762, Loss 65613303070720.00, Val loss 3477373190144.00\n","Epoch 763, Loss 65613273899008.00, Val loss 3477371355136.00\n","Epoch 764, Loss 65613245579264.00, Val loss 3477370306560.00\n","Epoch 765, Loss 65613217300480.00, Val loss 3477368995840.00\n","Epoch 766, Loss 65613190004736.00, Val loss 3477367160832.00\n","Epoch 767, Loss 65613162553344.00, Val loss 3477366636544.00\n","Epoch 768, Loss 65613134946304.00, Val loss 3477364801536.00\n","Epoch 769, Loss 65613109182464.00, Val loss 3477363228672.00\n","Epoch 770, Loss 65613082255360.00, Val loss 3477361393664.00\n","Epoch 771, Loss 65613056573440.00, Val loss 3477359296512.00\n","Epoch 772, Loss 65613031366656.00, Val loss 3477357723648.00\n","Epoch 773, Loss 65613005529088.00, Val loss 3477355626496.00\n","Epoch 774, Loss 65612980682752.00, Val loss 3477353791488.00\n","Epoch 775, Loss 65612955492352.00, Val loss 3477352218624.00\n","Epoch 776, Loss 65612931694592.00, Val loss 3477350645760.00\n","Epoch 777, Loss 65612906004480.00, Val loss 3477348286464.00\n","Epoch 778, Loss 65612880158720.00, Val loss 3477346189312.00\n","Epoch 779, Loss 65612855836672.00, Val loss 3477344092160.00\n","Epoch 780, Loss 65612828966912.00, Val loss 3477342519296.00\n","Epoch 781, Loss 65612801875968.00, Val loss 3477340422144.00\n","Epoch 782, Loss 65612775907328.00, Val loss 3477338324992.00\n","Epoch 783, Loss 65612748333056.00, Val loss 3477337014272.00\n","Epoch 784, Loss 65612721684480.00, Val loss 3477334917120.00\n","Epoch 785, Loss 65612692996096.00, Val loss 3477332557824.00\n","Epoch 786, Loss 65612665159680.00, Val loss 3477330722816.00\n","Epoch 787, Loss 65612635750400.00, Val loss 3477329412096.00\n","Epoch 788, Loss 65612605620224.00, Val loss 3477327314944.00\n","Epoch 789, Loss 65612575227904.00, Val loss 3477326004224.00\n","Epoch 790, Loss 65612543860736.00, Val loss 3477324169216.00\n","Epoch 791, Loss 65612512804864.00, Val loss 3477322072064.00\n","Epoch 792, Loss 65612480339968.00, Val loss 3477321285632.00\n","Epoch 793, Loss 65612447227904.00, Val loss 3477319450624.00\n","Epoch 794, Loss 65612414345216.00, Val loss 3477317615616.00\n","Epoch 795, Loss 65612379291648.00, Val loss 3477316304896.00\n","Epoch 796, Loss 65612345933824.00, Val loss 3477314732032.00\n","Epoch 797, Loss 65612311527424.00, Val loss 3477313159168.00\n","Epoch 798, Loss 65612276301824.00, Val loss 3477311848448.00\n","Epoch 799, Loss 65612241469440.00, Val loss 3477310013440.00\n","Epoch 800, Loss 65612205514752.00, Val loss 3477309227008.00\n","Epoch 801, Loss 65612169740288.00, Val loss 3477307392000.00\n","Epoch 802, Loss 65612133416960.00, Val loss 3477306343424.00\n","Epoch 803, Loss 65612097208320.00, Val loss 3477304508416.00\n","Epoch 804, Loss 65612060950528.00, Val loss 3477303721984.00\n","Epoch 805, Loss 65612023701504.00, Val loss 3477302149120.00\n","Epoch 806, Loss 65611986886656.00, Val loss 3477301624832.00\n","Epoch 807, Loss 65611949916160.00, Val loss 3477299789824.00\n","Epoch 808, Loss 65611912421376.00, Val loss 3477299003392.00\n","Epoch 809, Loss 65611874263040.00, Val loss 3477297692672.00\n","Epoch 810, Loss 65611836817408.00, Val loss 3477296644096.00\n","Epoch 811, Loss 65611798749184.00, Val loss 3477295595520.00\n","Epoch 812, Loss 65611761721344.00, Val loss 3477294284800.00\n","Epoch 813, Loss 65611723300864.00, Val loss 3477292974080.00\n","Epoch 814, Loss 65611684724736.00, Val loss 3477292187648.00\n","Epoch 815, Loss 65611647533056.00, Val loss 3477290876928.00\n","Epoch 816, Loss 65611609726976.00, Val loss 3477290090496.00\n","Epoch 817, Loss 65611572011008.00, Val loss 3477288779776.00\n","Epoch 818, Loss 65611533008896.00, Val loss 3477287993344.00\n","Epoch 819, Loss 65611495260160.00, Val loss 3477286682624.00\n","Epoch 820, Loss 65611457347584.00, Val loss 3477285371904.00\n","Epoch 821, Loss 65611417731072.00, Val loss 3477284585472.00\n","Epoch 822, Loss 65611379736576.00, Val loss 3477283536896.00\n","Epoch 823, Loss 65611341684736.00, Val loss 3477282488320.00\n","Epoch 824, Loss 65611303976960.00, Val loss 3477281701888.00\n","Epoch 825, Loss 65611265277952.00, Val loss 3477280653312.00\n","Epoch 826, Loss 65611226308608.00, Val loss 3477279080448.00\n","Epoch 827, Loss 65611187904512.00, Val loss 3477278556160.00\n","Epoch 828, Loss 65611149647872.00, Val loss 3477276983296.00\n","Epoch 829, Loss 65611111899136.00, Val loss 3477276459008.00\n","Epoch 830, Loss 65611072446464.00, Val loss 3477275148288.00\n","Epoch 831, Loss 65611034501120.00, Val loss 3477274361856.00\n","Epoch 832, Loss 65610996244480.00, Val loss 3477273051136.00\n","Epoch 833, Loss 65610957578240.00, Val loss 3477272002560.00\n","Epoch 834, Loss 65610919452672.00, Val loss 3477271216128.00\n","Epoch 835, Loss 65610880475136.00, Val loss 3477269905408.00\n","Epoch 836, Loss 65610842521600.00, Val loss 3477269118976.00\n","Epoch 837, Loss 65610804363264.00, Val loss 3477268070400.00\n","Epoch 838, Loss 65610765434880.00, Val loss 3477267546112.00\n","Epoch 839, Loss 65610727424000.00, Val loss 3477266497536.00\n","Epoch 840, Loss 65610688512000.00, Val loss 3477264662528.00\n","Epoch 841, Loss 65610651279360.00, Val loss 3477264138240.00\n","Epoch 842, Loss 65610611695616.00, Val loss 3477263089664.00\n","Epoch 843, Loss 65610573791232.00, Val loss 3477262041088.00\n","Epoch 844, Loss 65610535297024.00, Val loss 3477260468224.00\n","Epoch 845, Loss 65610496966656.00, Val loss 3477259943936.00\n","Epoch 846, Loss 65610458677248.00, Val loss 3477259157504.00\n","Epoch 847, Loss 65610420076544.00, Val loss 3477257846784.00\n","Epoch 848, Loss 65610381533184.00, Val loss 3477256798208.00\n","Epoch 849, Loss 65610343833600.00, Val loss 3477255487488.00\n","Epoch 850, Loss 65610305961984.00, Val loss 3477254438912.00\n","Epoch 851, Loss 65610266820608.00, Val loss 3477253652480.00\n","Epoch 852, Loss 65610229161984.00, Val loss 3477252603904.00\n","Epoch 853, Loss 65610189742080.00, Val loss 3477251293184.00\n","Epoch 854, Loss 65610152026112.00, Val loss 3477250244608.00\n","Epoch 855, Loss 65610114506752.00, Val loss 3477249458176.00\n","Epoch 856, Loss 65610075209728.00, Val loss 3477248147456.00\n","Epoch 857, Loss 65610035978240.00, Val loss 3477247623168.00\n","Epoch 858, Loss 65609998876672.00, Val loss 3477246836736.00\n","Epoch 859, Loss 65609959432192.00, Val loss 3477245001728.00\n","Epoch 860, Loss 65609921183744.00, Val loss 3477244477440.00\n","Epoch 861, Loss 65609883877376.00, Val loss 3477242904576.00\n","Epoch 862, Loss 65609843884032.00, Val loss 3477242118144.00\n","Epoch 863, Loss 65609806487552.00, Val loss 3477241069568.00\n","Epoch 864, Loss 65609768591360.00, Val loss 3477240283136.00\n","Epoch 865, Loss 65609728966656.00, Val loss 3477238448128.00\n","Epoch 866, Loss 65609691201536.00, Val loss 3477237923840.00\n","Epoch 867, Loss 65609653067776.00, Val loss 3477236875264.00\n","Epoch 868, Loss 65609614811136.00, Val loss 3477235826688.00\n","Epoch 869, Loss 65609577177088.00, Val loss 3477234778112.00\n","Epoch 870, Loss 65609538658304.00, Val loss 3477233729536.00\n","Epoch 871, Loss 65609499721728.00, Val loss 3477232418816.00\n","Epoch 872, Loss 65609461194752.00, Val loss 3477231108096.00\n","Epoch 873, Loss 65609423036416.00, Val loss 3477230321664.00\n","Epoch 874, Loss 65609385025536.00, Val loss 3477229010944.00\n","Epoch 875, Loss 65609345966080.00, Val loss 3477227962368.00\n","Epoch 876, Loss 65609308372992.00, Val loss 3477227175936.00\n","Epoch 877, Loss 65609270001664.00, Val loss 3477225865216.00\n","Epoch 878, Loss 65609231523840.00, Val loss 3477225078784.00\n","Epoch 879, Loss 65609194119168.00, Val loss 3477223768064.00\n","Epoch 880, Loss 65609155534848.00, Val loss 3477222719488.00\n","Epoch 881, Loss 65609117581312.00, Val loss 3477221408768.00\n","Epoch 882, Loss 65609078366208.00, Val loss 3477220622336.00\n","Epoch 883, Loss 65609040461824.00, Val loss 3477219573760.00\n","Epoch 884, Loss 65609001467904.00, Val loss 3477218263040.00\n","Epoch 885, Loss 65608963506176.00, Val loss 3477217476608.00\n","Epoch 886, Loss 65608926019584.00, Val loss 3477216428032.00\n","Epoch 887, Loss 65608887386112.00, Val loss 3477215117312.00\n","Epoch 888, Loss 65608849342464.00, Val loss 3477214593024.00\n","Epoch 889, Loss 65608810659840.00, Val loss 3477213544448.00\n","Epoch 890, Loss 65608772173824.00, Val loss 3477212495872.00\n","Epoch 891, Loss 65608733614080.00, Val loss 3477210923008.00\n","Epoch 892, Loss 65608696283136.00, Val loss 3477210398720.00\n","Epoch 893, Loss 65608658198528.00, Val loss 3477209350144.00\n","Epoch 894, Loss 65608619098112.00, Val loss 3477208039424.00\n","Epoch 895, Loss 65608580284416.00, Val loss 3477206990848.00\n","Epoch 896, Loss 65608542838784.00, Val loss 3477206204416.00\n","Epoch 897, Loss 65608503992320.00, Val loss 3477204893696.00\n","Epoch 898, Loss 65608465965056.00, Val loss 3477203845120.00\n","Epoch 899, Loss 65608427560960.00, Val loss 3477202796544.00\n","Epoch 900, Loss 65608390524928.00, Val loss 3477201485824.00\n","Epoch 901, Loss 65608351399936.00, Val loss 3477200699392.00\n","Epoch 902, Loss 65608314019840.00, Val loss 3477199388672.00\n","Epoch 903, Loss 65608275607552.00, Val loss 3477198340096.00\n","Epoch 904, Loss 65608237350912.00, Val loss 3477197291520.00\n","Epoch 905, Loss 65608199282688.00, Val loss 3477196242944.00\n","Epoch 906, Loss 65608160821248.00, Val loss 3477195194368.00\n","Epoch 907, Loss 65608123113472.00, Val loss 3477193883648.00\n","Epoch 908, Loss 65608084496384.00, Val loss 3477193097216.00\n","Epoch 909, Loss 65608046592000.00, Val loss 3477191786496.00\n","Epoch 910, Loss 65608008957952.00, Val loss 3477190737920.00\n","Epoch 911, Loss 65607969914880.00, Val loss 3477189427200.00\n","Epoch 912, Loss 65607931920384.00, Val loss 3477188378624.00\n","Epoch 913, Loss 65607892377600.00, Val loss 3477187067904.00\n","Epoch 914, Loss 65607855742976.00, Val loss 3477186281472.00\n","Epoch 915, Loss 65607817027584.00, Val loss 3477184970752.00\n","Epoch 916, Loss 65607779262464.00, Val loss 3477183922176.00\n","Epoch 917, Loss 65607740866560.00, Val loss 3477182873600.00\n","Epoch 918, Loss 65607703191552.00, Val loss 3477181562880.00\n","Epoch 919, Loss 65607665778688.00, Val loss 3477180776448.00\n","Epoch 920, Loss 65607626579968.00, Val loss 3477178941440.00\n","Epoch 921, Loss 65607588790272.00, Val loss 3477178417152.00\n","Epoch 922, Loss 65607551328256.00, Val loss 3477177368576.00\n","Epoch 923, Loss 65607511990272.00, Val loss 3477175795712.00\n","Epoch 924, Loss 65607474397184.00, Val loss 3477175009280.00\n","Epoch 925, Loss 65607436566528.00, Val loss 3477174222848.00\n","Epoch 926, Loss 65607398219776.00, Val loss 3477173174272.00\n","Epoch 927, Loss 65607359307776.00, Val loss 3477171863552.00\n","Epoch 928, Loss 65607320862720.00, Val loss 3477170814976.00\n","Epoch 929, Loss 65607282573312.00, Val loss 3477169504256.00\n","Epoch 930, Loss 65607244259328.00, Val loss 3477168717824.00\n","Epoch 931, Loss 65607205675008.00, Val loss 3477167407104.00\n","Epoch 932, Loss 65607167442944.00, Val loss 3477166620672.00\n","Epoch 933, Loss 65607129079808.00, Val loss 3477165309952.00\n","Epoch 934, Loss 65607091183616.00, Val loss 3477164523520.00\n","Epoch 935, Loss 65607052107776.00, Val loss 3477163474944.00\n","Epoch 936, Loss 65607014277120.00, Val loss 3477162164224.00\n","Epoch 937, Loss 65606975561728.00, Val loss 3477161377792.00\n","Epoch 938, Loss 65606935781376.00, Val loss 3477160067072.00\n","Epoch 939, Loss 65606898286592.00, Val loss 3477159018496.00\n","Epoch 940, Loss 65606859882496.00, Val loss 3477157969920.00\n","Epoch 941, Loss 65606821355520.00, Val loss 3477157445632.00\n","Epoch 942, Loss 65606782500864.00, Val loss 3477156659200.00\n","Epoch 943, Loss 65606744358912.00, Val loss 3477154824192.00\n","Epoch 944, Loss 65606704709632.00, Val loss 3477154299904.00\n","Epoch 945, Loss 65606666633216.00, Val loss 3477152989184.00\n","Epoch 946, Loss 65606627950592.00, Val loss 3477151940608.00\n","Epoch 947, Loss 65606589440000.00, Val loss 3477151154176.00\n","Epoch 948, Loss 65606551658496.00, Val loss 3477149843456.00\n","Epoch 949, Loss 65606512877568.00, Val loss 3477149057024.00\n","Epoch 950, Loss 65606474334208.00, Val loss 3477147746304.00\n","Epoch 951, Loss 65606434562048.00, Val loss 3477146697728.00\n","Epoch 952, Loss 65606397059072.00, Val loss 3477145649152.00\n","Epoch 953, Loss 65606358319104.00, Val loss 3477144862720.00\n","Epoch 954, Loss 65606318940160.00, Val loss 3477143552000.00\n","Epoch 955, Loss 65606280249344.00, Val loss 3477142765568.00\n","Epoch 956, Loss 65606241492992.00, Val loss 3477141454848.00\n","Epoch 957, Loss 65606203138048.00, Val loss 3477140930560.00\n","Epoch 958, Loss 65606164316160.00, Val loss 3477139357696.00\n","Epoch 959, Loss 65606124797952.00, Val loss 3477138833408.00\n","Epoch 960, Loss 65606086836224.00, Val loss 3477137784832.00\n","Epoch 961, Loss 65606048595968.00, Val loss 3477136474112.00\n","Epoch 962, Loss 65606009716736.00, Val loss 3477135687680.00\n","Epoch 963, Loss 65605971386368.00, Val loss 3477133852672.00\n","Epoch 964, Loss 65605931720704.00, Val loss 3477133328384.00\n","Epoch 965, Loss 65605892931584.00, Val loss 3477132541952.00\n","Epoch 966, Loss 65605854879744.00, Val loss 3477131231232.00\n","Epoch 967, Loss 65605816016896.00, Val loss 3477130444800.00\n","Epoch 968, Loss 65605777571840.00, Val loss 3477129134080.00\n","Epoch 969, Loss 65605738848256.00, Val loss 3477128347648.00\n","Epoch 970, Loss 65605700558848.00, Val loss 3477127299072.00\n","Epoch 971, Loss 65605662597120.00, Val loss 3477125988352.00\n","Epoch 972, Loss 65605622956032.00, Val loss 3477125464064.00\n","Epoch 973, Loss 65605583429632.00, Val loss 3477124415488.00\n","Epoch 974, Loss 65605546024960.00, Val loss 3477122842624.00\n","Epoch 975, Loss 65605507211264.00, Val loss 3477122318336.00\n","Epoch 976, Loss 65605469003776.00, Val loss 3477120745472.00\n","Epoch 977, Loss 65605429657600.00, Val loss 3477120221184.00\n","Epoch 978, Loss 65605391622144.00, Val loss 3477119172608.00\n","Epoch 979, Loss 65605352759296.00, Val loss 3477118124032.00\n","Epoch 980, Loss 65605313282048.00, Val loss 3477117075456.00\n","Epoch 981, Loss 65605274206208.00, Val loss 3477116289024.00\n","Epoch 982, Loss 65605235531776.00, Val loss 3477114978304.00\n","Epoch 983, Loss 65605198282752.00, Val loss 3477113929728.00\n","Epoch 984, Loss 65605158600704.00, Val loss 3477112619008.00\n","Epoch 985, Loss 65605120860160.00, Val loss 3477111832576.00\n","Epoch 986, Loss 65605082816512.00, Val loss 3477110784000.00\n","Epoch 987, Loss 65605044109312.00, Val loss 3477109735424.00\n","Epoch 988, Loss 65605004591104.00, Val loss 3477109211136.00\n","Epoch 989, Loss 65604965982208.00, Val loss 3477107900416.00\n","Epoch 990, Loss 65604928282624.00, Val loss 3477106851840.00\n","Epoch 991, Loss 65604888928256.00, Val loss 3477106065408.00\n","Epoch 992, Loss 65604850130944.00, Val loss 3477105016832.00\n","Epoch 993, Loss 65604810727424.00, Val loss 3477103968256.00\n","Epoch 994, Loss 65604773683200.00, Val loss 3477102657536.00\n","Epoch 995, Loss 65604733632512.00, Val loss 3477101871104.00\n","Epoch 996, Loss 65604696342528.00, Val loss 3477100560384.00\n","Epoch 997, Loss 65604657029120.00, Val loss 3477099773952.00\n","Epoch 998, Loss 65604618362880.00, Val loss 3477098725376.00\n","Epoch 999, Loss 65604580597760.00, Val loss 3477097676800.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"status":"ok","timestamp":1646782775442,"user_tz":360,"elapsed":352,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"d28cb3e0-bf65-4062-f891-85f3b2856476"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["3956005142528.0\n","tensor([508981.7500, 510838.5938, 512701.0312, 514589.3438, 516509.5625,\n","        518462.9062, 520436.5000, 522419.9375, 524439.5000, 526486.5000,\n","        528595.8125, 530682.0625, 532811.5000, 534978.8750, 537158.4375],\n","       grad_fn=<SelectBackward0>)\n","tensor([292187., 293697., 293697., 293697., 293697., 293697., 295701., 296870.,\n","        297729., 297729., 297729., 298362., 298626., 298808., 298993.])\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","    # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_cumulative_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_cumulative_infected_tensor':labeled_output # (52, 15)\n","}\n","\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)"],"metadata":{"id":"s3-8Yge6CAWM","executionInfo":{"status":"ok","timestamp":1646782864809,"user_tz":360,"elapsed":274,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["mean_squared_error = criterion(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","mae_function = torch.nn.L1Loss()\n","mean_absolute_error = mae_function(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","print(\"mean squared error: \", mean_squared_error)\n","print(\"mean absolute error: \", mean_absolute_error)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Re_cSYo66Jln","executionInfo":{"status":"ok","timestamp":1646783067762,"user_tz":360,"elapsed":15,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"76672625-fa8c-4f4f-fa6f-e0dec3de74d1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["mean squared error:  3956005142528.0\n","mean absolute error:  1142052.0\n"]}]}]}