{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_data_raw.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOWtyFQg3lykBGbORY07IKq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\"\n","! pip install epiweeks\n","! pip install haversine"],"metadata":{"id":"7yDiZvV2jd2R","executionInfo":{"status":"ok","timestamp":1646372723042,"user_tz":360,"elapsed":14435,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"94643158-710b-4b57-bf41-521b560d7d24","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. GNN (only) - Pytorch Geometric\n","Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Import libraries needed\n","\"\"\"\n","from data_downloader import GenerateTrainingData\n","from utils import gravity_law_commute_dist\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import torch"],"metadata":{"id":"xyi3R865jOGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables used to preprocess data\n","\"\"\"\n","START_DATE = '2020-04-12'\n","END_DATE = '2022-01-24'\n","valid_window = 25\n","test_window = 25\n","history_window=6\n","pred_window=15\n","slide_step=5"],"metadata":{"id":"3aRrgY3MfbR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uYf0Z0LfSNa","executionInfo":{"status":"ok","timestamp":1646364990932,"user_tz":360,"elapsed":81070,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"11c0529c-c0d3-4639-c6b5-db7f2e48bab2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Finish download\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self._setitem_single_column(ilocs[0], value, pi)\n"]}],"source":["\"\"\"\n","Download JHU data\n","\"\"\"\n","# Download data\n","GenerateTrainingData().download_jhu_data(START_DATE, END_DATE)\n","\n","#Merge population data with downloaded data\n","raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n","pop_data = pd.read_csv('./uszips.csv')\n","pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')"]},{"cell_type":"code","source":["\"\"\"\n","Create edge index to be passed to GNN architecture later in Pytorch Geometric\n","\"\"\"\n","# State name to state abbreviation mapping (so we can index the state adjacency map later)\n","# Reference: https://gist.github.com/rogerallen/1583593 \n","us_state_to_abbrev = {\n","    \"Alabama\": \"AL\",\n","    \"Alaska\": \"AK\",\n","    \"Arizona\": \"AZ\",\n","    \"Arkansas\": \"AR\",\n","    \"California\": \"CA\",\n","    \"Colorado\": \"CO\",\n","    \"Connecticut\": \"CT\",\n","    \"Delaware\": \"DE\",\n","    \"Florida\": \"FL\",\n","    \"Georgia\": \"GA\",\n","    \"Hawaii\": \"HI\",\n","    \"Idaho\": \"ID\",\n","    \"Illinois\": \"IL\",\n","    \"Indiana\": \"IN\",\n","    \"Iowa\": \"IA\",\n","    \"Kansas\": \"KS\",\n","    \"Kentucky\": \"KY\",\n","    \"Louisiana\": \"LA\",\n","    \"Maine\": \"ME\",\n","    \"Maryland\": \"MD\",\n","    \"Massachusetts\": \"MA\",\n","    \"Michigan\": \"MI\",\n","    \"Minnesota\": \"MN\",\n","    \"Mississippi\": \"MS\",\n","    \"Missouri\": \"MO\",\n","    \"Montana\": \"MT\",\n","    \"Nebraska\": \"NE\",\n","    \"Nevada\": \"NV\",\n","    \"New Hampshire\": \"NH\",\n","    \"New Jersey\": \"NJ\",\n","    \"New Mexico\": \"NM\",\n","    \"New York\": \"NY\",\n","    \"North Carolina\": \"NC\",\n","    \"North Dakota\": \"ND\",\n","    \"Ohio\": \"OH\",\n","    \"Oklahoma\": \"OK\",\n","    \"Oregon\": \"OR\",\n","    \"Pennsylvania\": \"PA\",\n","    \"Rhode Island\": \"RI\",\n","    \"South Carolina\": \"SC\",\n","    \"South Dakota\": \"SD\",\n","    \"Tennessee\": \"TN\",\n","    \"Texas\": \"TX\",\n","    \"Utah\": \"UT\",\n","    \"Vermont\": \"VT\",\n","    \"Virginia\": \"VA\",\n","    \"Washington\": \"WA\",\n","    \"West Virginia\": \"WV\",\n","    \"Wisconsin\": \"WI\",\n","    \"Wyoming\": \"WY\",\n","    \"District of Columbia\": \"DC\",\n","    \"American Samoa\": \"AS\",\n","    \"Guam\": \"GU\",\n","    \"Northern Mariana Islands\": \"MP\",\n","    \"Puerto Rico\": \"PR\",\n","    \"United States Minor Outlying Islands\": \"UM\",\n","    \"U.S. Virgin Islands\": \"VI\",\n","}\n","\n","# invert the dictionary\n","abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n","\n","# State abbreviation to state adjacency list mapping (for creation of map)\n","# Modified from: https://gist.github.com/rietta/4112447 \n","states_adjacency_list = {\n","    \"AK\": \"AK\",\n","    \"AL\": \"AL,MS,TN,GA,FL\",\n","    \"AR\": \"AR,MO,TN,MS,LA,TX,OK\",\n","    \"AZ\": \"AZ,CA,NV,UT,CO,NM\",\n","    \"CA\": \"CA,OR,NV,AZ\",\n","    \"CO\": \"CO,WY,NE,KS,OK,NM,AZ,UT\",\n","    \"CT\": \"CT,NY,MA,RI\",\n","    \"DC\": \"DC,MD,VA\",\n","    \"DE\": \"DE,MD,PA,NJ\",\n","    \"FL\": \"FL,AL,GA\",\n","    \"GA\": \"GA,FL,AL,TN,NC,SC\",\n","    \"HI\": \"HI\",\n","    \"IA\": \"IA,MN,WI,IL,MO,NE,SD\",\n","    \"ID\": \"ID,MT,WY,UT,NV,OR,WA\",\n","    \"IL\": \"IL,IN,KY,MO,IA,WI\",\n","    \"IN\": \"IN,MI,OH,KY,IL\",\n","    \"KS\": \"KS,NE,MO,OK,CO\",\n","    \"KY\": \"KY,IN,OH,WV,VA,TN,MO,IL\",\n","    \"LA\": \"LA,TX,AR,MS\",\n","    \"MA\": \"MA,RI,CT,NY,NH,VT\",\n","    \"MD\": \"MD,VA,WV,PA,DC,DE\",\n","    \"ME\": \"ME,NH\",\n","    \"MI\": \"MI,WI,IN,OH\",\n","    \"MN\": \"MN,WI,IA,SD,ND\",\n","    \"MO\": \"MO,IA,IL,KY,TN,AR,OK,KS,NE\",\n","    \"MS\": \"MS,LA,AR,TN,AL\",\n","    \"MT\": \"MT,ND,SD,WY,ID\",\n","    \"NC\": \"NC,VA,TN,GA,SC\",\n","    \"ND\": \"ND,MN,SD,MT\",\n","    \"NE\": \"NE,SD,IA,MO,KS,CO,WY\",\n","    \"NH\": \"NH,VT,ME,MA\",\n","    \"NJ\": \"NJ,DE,PA,NY\",\n","    \"NM\": \"NM,AZ,UT,CO,OK,TX\",\n","    \"NV\": \"NV,ID,UT,AZ,CA,OR\",\n","    \"NY\": \"NY,NJ,PA,VT,MA,CT\",\n","    \"OH\": \"OH,PA,WV,KY,IN,MI\",\n","    \"OK\": \"OK,KS,MO,AR,TX,NM,CO\",\n","    \"OR\": \"OR,CA,NV,ID,WA\",\n","    \"PA\": \"PA,NY,NJ,DE,MD,WV,OH\",\n","    \"PR\": \"PR\",\n","    \"RI\": \"RI,CT,MA\",\n","    \"SC\": \"SC,GA,NC\",\n","    \"SD\": \"SD,ND,MN,IA,NE,WY,MT\",\n","    \"TN\": \"TN,KY,VA,NC,GA,AL,MS,AR,MO\",\n","    \"TX\": \"TX,NM,OK,AR,LA\",\n","    \"UT\": \"UT,ID,WY,CO,NM,AZ,NV\",\n","    \"VA\": \"VA,NC,TN,KY,WV,MD,DC\",\n","    \"VT\": \"VT,NY,NH,MA\",\n","    \"WA\": \"WA,ID,OR\",\n","    \"WI\": \"WI,MI,MN,IA,IL\",\n","    \"WV\": \"WV,OH,PA,MD,VA,KY\",\n","    \"WY\": \"WY,MT,SD,NE,CO,UT,ID\"\n","}\n","\n","\n","# we will use undirected graph, where nodes are represented by ints\n","edge_list_source_node = []\n","edge_list_destination_node = []\n","\n","\n","state_list = list(raw_data['state'].unique())\n","for state_name in state_list:\n","  state_abbrev = us_state_to_abbrev[state_name]\n","  curr_state_and_neighbors = states_adjacency_list[state_abbrev]\n","  comma_delimited_list = curr_state_and_neighbors.split(\",\")\n","  \n","  source_state_abbrev = None\n","  dest_state_abbreviations = None\n","  if len(comma_delimited_list) == 1:\n","    source_state_abbrev = comma_delimited_list[0]\n","    dest_state_abbreviations = [comma_delimited_list[0]]\n","  else:\n","    source_state_abbrev = comma_delimited_list[0]\n","    dest_state_abbreviations = comma_delimited_list[1:]\n","  \n","  for dest_state_abbrev in dest_state_abbreviations:\n","    source_state_full_name = abbrev_to_us_state[source_state_abbrev]\n","    dest_state_full_name = abbrev_to_us_state[dest_state_abbrev]\n","\n","    source_state_int = state_list.index(source_state_full_name)\n","    dest_state_int = state_list.index(dest_state_full_name)\n","    \n","    edge_list_source_node.append(source_state_int)\n","    edge_list_destination_node.append(dest_state_int)\n","\n","edge_index = torch.tensor([edge_list_source_node,\n","                           edge_list_destination_node], dtype=torch.long)"],"metadata":{"id":"lVx9Ll_e8nre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Preprocess data by separating it into different groups\n","\"\"\"\n","# Preprocess features\n","confirmed_cases = []\n","death_cases = []\n","static_feat = []\n","\n","for i, each_loc in enumerate(state_list):\n","    confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","    death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","    static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","    \n","confirmed_cases = np.array(confirmed_cases)\n","death_cases = np.array(death_cases)\n","static_feat = np.array(static_feat)[:, 0, :]\n","\n","\n","# Calculate change in # cases and # deaths from previous day\n","daily_change_in_confirmed = np.concatenate((np.zeros((confirmed_cases.shape[0], 1), dtype=np.float32), np.diff(confirmed_cases)), axis=-1)\n","daily_change_in_deaths = np.concatenate((np.zeros((death_cases.shape[0], 1), dtype=np.float32), np.diff(death_cases)), axis=-1)"],"metadata":{"id":"UKHyvb_fg4HE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put data together into 1 big numpy array\n","\"\"\"\n","dynamic_feat = np.concatenate((np.expand_dims(confirmed_cases, axis=-1),\n","                               np.expand_dims(death_cases, axis=-1),\n","                               np.expand_dims(daily_change_in_confirmed, axis=-1), \n","                               np.expand_dims(daily_change_in_deaths, axis=-1)\n","                               ), axis=-1)"],"metadata":{"id":"dOMvjHswg8Jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Separate data into training, testing, and validation sets\n","\"\"\"\n","\n","#Split train-test\n","train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n","val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n","test_feat = dynamic_feat[:, -test_window:, :]\n","\n","# Helper function for creating each set of data used\n","def prepare_data(data):\n","  # Data shape num_locations, timestep, n_feat\n","  num_locations = data.shape[0]\n","  timestep = data.shape[1]\n","  n_feat = data.shape[2]\n","\n","  input_entries = []\n","  output_entries = []\n","\n","  for i in range(0, timestep, slide_step):\n","    if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","        break\n","\n","    # Shape = number nodes x num_input_features\n","    input_entry = data[:, i:i+history_window, :].reshape((num_locations, history_window*n_feat)).tolist()\n","\n","    # Shape = number nodes x num_output_features\n","    output_entry = data[:, i+history_window:i+history_window+pred_window, 0].reshape((num_locations, pred_window)).tolist()\n","\n","    input_entries.append(torch.tensor(input_entry))\n","    output_entries.append(torch.tensor(output_entry))\n","\n","  return input_entries, output_entries\n","\n","train_x, train_y = prepare_data(train_feat)\n","val_x, val_y = prepare_data(val_feat)\n","test_x, test_y = prepare_data(test_feat)"],"metadata":{"id":"QE_pnGeYhg8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Package/organize preprocessed data together into a dictionary called \"preprocessed_data\"\n","\"\"\"\n","training_variables = {'train_x':train_x, \n","                      'train_y':train_y\n","                      }\n","\n","validation_variables = {'val_x':val_x, \n","                        'val_y':val_y\n","                        }\n","\n","testing_variables = {'test_x':test_x, \n","                     'test_y':test_y\n","                     }\n","\n","preprocessed_data = {\n","    'training_variables':training_variables,\n","    'validation_variables':validation_variables,\n","    'testing_variables':testing_variables,\n","    'edge_index':edge_index\n","}"],"metadata":{"id":"XoBnRNlznfut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put above code into 1 file and 1 function\n","\"\"\"\n","%%writefile preprocess_data.py\n","\"\"\"\n","Import libraries needed\n","\"\"\"\n","from data_downloader import GenerateTrainingData\n","from utils import gravity_law_commute_dist\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import torch\n","\n","########################################################################################################################################################\n","\n","\"\"\"\n","Declare global variables used to preprocess data\n","\"\"\"\n","START_DATE = '2020-04-12'\n","END_DATE = '2022-01-24'\n","valid_window = 25\n","test_window = 25\n","history_window=6\n","pred_window=15\n","slide_step=5\n","\n","\n","\n","\n","def get_preprocessed_data():\n","  #####################################################################################################################################################\n","  \"\"\"\n","  Download JHU data\n","  \"\"\"\n","  # Download data\n","  GenerateTrainingData().download_jhu_data(START_DATE, END_DATE)\n","\n","  #Merge population data with downloaded data\n","  raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n","  pop_data = pd.read_csv('./uszips.csv')\n","  pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","  raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')\n","  #######################################################################################################################################################\n","  \"\"\"\n","  Create edge index to be passed to GNN architecture later in Pytorch Geometric\n","  \"\"\"\n","  # State name to state abbreviation mapping (so we can index the state adjacency map later)\n","  # Reference: https://gist.github.com/rogerallen/1583593 \n","  us_state_to_abbrev = {\n","      \"Alabama\": \"AL\",\n","      \"Alaska\": \"AK\",\n","      \"Arizona\": \"AZ\",\n","      \"Arkansas\": \"AR\",\n","      \"California\": \"CA\",\n","      \"Colorado\": \"CO\",\n","      \"Connecticut\": \"CT\",\n","      \"Delaware\": \"DE\",\n","      \"Florida\": \"FL\",\n","      \"Georgia\": \"GA\",\n","      \"Hawaii\": \"HI\",\n","      \"Idaho\": \"ID\",\n","      \"Illinois\": \"IL\",\n","      \"Indiana\": \"IN\",\n","      \"Iowa\": \"IA\",\n","      \"Kansas\": \"KS\",\n","      \"Kentucky\": \"KY\",\n","      \"Louisiana\": \"LA\",\n","      \"Maine\": \"ME\",\n","      \"Maryland\": \"MD\",\n","      \"Massachusetts\": \"MA\",\n","      \"Michigan\": \"MI\",\n","      \"Minnesota\": \"MN\",\n","      \"Mississippi\": \"MS\",\n","      \"Missouri\": \"MO\",\n","      \"Montana\": \"MT\",\n","      \"Nebraska\": \"NE\",\n","      \"Nevada\": \"NV\",\n","      \"New Hampshire\": \"NH\",\n","      \"New Jersey\": \"NJ\",\n","      \"New Mexico\": \"NM\",\n","      \"New York\": \"NY\",\n","      \"North Carolina\": \"NC\",\n","      \"North Dakota\": \"ND\",\n","      \"Ohio\": \"OH\",\n","      \"Oklahoma\": \"OK\",\n","      \"Oregon\": \"OR\",\n","      \"Pennsylvania\": \"PA\",\n","      \"Rhode Island\": \"RI\",\n","      \"South Carolina\": \"SC\",\n","      \"South Dakota\": \"SD\",\n","      \"Tennessee\": \"TN\",\n","      \"Texas\": \"TX\",\n","      \"Utah\": \"UT\",\n","      \"Vermont\": \"VT\",\n","      \"Virginia\": \"VA\",\n","      \"Washington\": \"WA\",\n","      \"West Virginia\": \"WV\",\n","      \"Wisconsin\": \"WI\",\n","      \"Wyoming\": \"WY\",\n","      \"District of Columbia\": \"DC\",\n","      \"American Samoa\": \"AS\",\n","      \"Guam\": \"GU\",\n","      \"Northern Mariana Islands\": \"MP\",\n","      \"Puerto Rico\": \"PR\",\n","      \"United States Minor Outlying Islands\": \"UM\",\n","      \"U.S. Virgin Islands\": \"VI\",\n","  }\n","\n","  # invert the dictionary\n","  abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n","\n","  # State abbreviation to state adjacency list mapping (for creation of map)\n","  # Modified from: https://gist.github.com/rietta/4112447 \n","  states_adjacency_list = {\n","      \"AK\": \"AK\",\n","      \"AL\": \"AL,MS,TN,GA,FL\",\n","      \"AR\": \"AR,MO,TN,MS,LA,TX,OK\",\n","      \"AZ\": \"AZ,CA,NV,UT,CO,NM\",\n","      \"CA\": \"CA,OR,NV,AZ\",\n","      \"CO\": \"CO,WY,NE,KS,OK,NM,AZ,UT\",\n","      \"CT\": \"CT,NY,MA,RI\",\n","      \"DC\": \"DC,MD,VA\",\n","      \"DE\": \"DE,MD,PA,NJ\",\n","      \"FL\": \"FL,AL,GA\",\n","      \"GA\": \"GA,FL,AL,TN,NC,SC\",\n","      \"HI\": \"HI\",\n","      \"IA\": \"IA,MN,WI,IL,MO,NE,SD\",\n","      \"ID\": \"ID,MT,WY,UT,NV,OR,WA\",\n","      \"IL\": \"IL,IN,KY,MO,IA,WI\",\n","      \"IN\": \"IN,MI,OH,KY,IL\",\n","      \"KS\": \"KS,NE,MO,OK,CO\",\n","      \"KY\": \"KY,IN,OH,WV,VA,TN,MO,IL\",\n","      \"LA\": \"LA,TX,AR,MS\",\n","      \"MA\": \"MA,RI,CT,NY,NH,VT\",\n","      \"MD\": \"MD,VA,WV,PA,DC,DE\",\n","      \"ME\": \"ME,NH\",\n","      \"MI\": \"MI,WI,IN,OH\",\n","      \"MN\": \"MN,WI,IA,SD,ND\",\n","      \"MO\": \"MO,IA,IL,KY,TN,AR,OK,KS,NE\",\n","      \"MS\": \"MS,LA,AR,TN,AL\",\n","      \"MT\": \"MT,ND,SD,WY,ID\",\n","      \"NC\": \"NC,VA,TN,GA,SC\",\n","      \"ND\": \"ND,MN,SD,MT\",\n","      \"NE\": \"NE,SD,IA,MO,KS,CO,WY\",\n","      \"NH\": \"NH,VT,ME,MA\",\n","      \"NJ\": \"NJ,DE,PA,NY\",\n","      \"NM\": \"NM,AZ,UT,CO,OK,TX\",\n","      \"NV\": \"NV,ID,UT,AZ,CA,OR\",\n","      \"NY\": \"NY,NJ,PA,VT,MA,CT\",\n","      \"OH\": \"OH,PA,WV,KY,IN,MI\",\n","      \"OK\": \"OK,KS,MO,AR,TX,NM,CO\",\n","      \"OR\": \"OR,CA,NV,ID,WA\",\n","      \"PA\": \"PA,NY,NJ,DE,MD,WV,OH\",\n","      \"PR\": \"PR\",\n","      \"RI\": \"RI,CT,MA\",\n","      \"SC\": \"SC,GA,NC\",\n","      \"SD\": \"SD,ND,MN,IA,NE,WY,MT\",\n","      \"TN\": \"TN,KY,VA,NC,GA,AL,MS,AR,MO\",\n","      \"TX\": \"TX,NM,OK,AR,LA\",\n","      \"UT\": \"UT,ID,WY,CO,NM,AZ,NV\",\n","      \"VA\": \"VA,NC,TN,KY,WV,MD,DC\",\n","      \"VT\": \"VT,NY,NH,MA\",\n","      \"WA\": \"WA,ID,OR\",\n","      \"WI\": \"WI,MI,MN,IA,IL\",\n","      \"WV\": \"WV,OH,PA,MD,VA,KY\",\n","      \"WY\": \"WY,MT,SD,NE,CO,UT,ID\"\n","  }\n","\n","\n","  # we will use undirected graph, where nodes are represented by ints\n","  edge_list_source_node = []\n","  edge_list_destination_node = []\n","\n","\n","  state_list = list(raw_data['state'].unique())\n","  for state_name in state_list:\n","    state_abbrev = us_state_to_abbrev[state_name]\n","    curr_state_and_neighbors = states_adjacency_list[state_abbrev]\n","    comma_delimited_list = curr_state_and_neighbors.split(\",\")\n","    \n","    source_state_abbrev = None\n","    dest_state_abbreviations = None\n","    if len(comma_delimited_list) == 1:\n","      source_state_abbrev = comma_delimited_list[0]\n","      dest_state_abbreviations = [comma_delimited_list[0]]\n","    else:\n","      source_state_abbrev = comma_delimited_list[0]\n","      dest_state_abbreviations = comma_delimited_list[1:]\n","    \n","    for dest_state_abbrev in dest_state_abbreviations:\n","      source_state_full_name = abbrev_to_us_state[source_state_abbrev]\n","      dest_state_full_name = abbrev_to_us_state[dest_state_abbrev]\n","\n","      source_state_int = state_list.index(source_state_full_name)\n","      dest_state_int = state_list.index(dest_state_full_name)\n","      \n","      edge_list_source_node.append(source_state_int)\n","      edge_list_destination_node.append(dest_state_int)\n","\n","  edge_index = torch.tensor([edge_list_source_node,\n","                            edge_list_destination_node], dtype=torch.long)\n","  #########################################################################################################################################\n","  \"\"\"\n","  Preprocess data by separating it into different groups\n","  \"\"\"\n","  # Preprocess features\n","  confirmed_cases = []\n","  death_cases = []\n","  static_feat = []\n","\n","  for i, each_loc in enumerate(state_list):\n","      confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","      death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","      static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","      \n","  confirmed_cases = np.array(confirmed_cases)\n","  death_cases = np.array(death_cases)\n","  static_feat = np.array(static_feat)[:, 0, :]\n","\n","\n","  # Calculate change in # cases and # deaths from previous day\n","  daily_change_in_confirmed = np.concatenate((np.zeros((confirmed_cases.shape[0], 1), dtype=np.float32), np.diff(confirmed_cases)), axis=-1)\n","  daily_change_in_deaths = np.concatenate((np.zeros((death_cases.shape[0], 1), dtype=np.float32), np.diff(death_cases)), axis=-1)\n","\n","  ########################################################################################################################################################\n","  \"\"\"\n","  Put data together into 1 big numpy array\n","  \"\"\"\n","  dynamic_feat = np.concatenate((np.expand_dims(confirmed_cases, axis=-1),\n","                                np.expand_dims(death_cases, axis=-1),\n","                                np.expand_dims(daily_change_in_confirmed, axis=-1), \n","                                np.expand_dims(daily_change_in_deaths, axis=-1)\n","                                ), axis=-1)\n","  ##########################################################################################################################################################\n","  \"\"\"\n","  Separate data into training, testing, and validation sets\n","  \"\"\"\n","\n","  #Split train-test\n","  train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n","  val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n","  test_feat = dynamic_feat[:, -test_window:, :]\n","\n","  # Helper function for creating each set of data used\n","  def prepare_data(data):\n","    # Data shape num_locations, timestep, n_feat\n","    num_locations = data.shape[0]\n","    timestep = data.shape[1]\n","    n_feat = data.shape[2]\n","\n","    input_entries = []\n","    output_entries = []\n","\n","    for i in range(0, timestep, slide_step):\n","      if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","          break\n","\n","      # Shape = number nodes x num_input_features\n","      input_entry = data[:, i:i+history_window, :].reshape((num_locations, history_window*n_feat)).tolist()\n","\n","      # Shape = number nodes x num_output_features\n","      output_entry = data[:, i+history_window:i+history_window+pred_window, 0].reshape((num_locations, pred_window)).tolist()\n","\n","      input_entries.append(torch.tensor(input_entry))\n","      output_entries.append(torch.tensor(output_entry))\n","\n","    return input_entries, output_entries\n","\n","  train_x, train_y = prepare_data(train_feat)\n","  val_x, val_y = prepare_data(val_feat)\n","  test_x, test_y = prepare_data(test_feat)\n","\n","  ################################################################################################################################################\n","  \"\"\"\n","  Package/organize preprocessed data together into a dictionary called \"preprocessed_data\"\n","  \"\"\"\n","  training_variables = {'train_x':train_x, \n","                        'train_y':train_y\n","                        }\n","\n","  validation_variables = {'val_x':val_x, \n","                          'val_y':val_y\n","                          }\n","\n","  testing_variables = {'test_x':test_x, \n","                      'test_y':test_y\n","                      }\n","\n","  preprocessed_data = {\n","      'training_variables':training_variables,\n","      'validation_variables':validation_variables,\n","      'testing_variables':testing_variables,\n","      'edge_index':edge_index\n","  }\n","\n","  return preprocessed_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuApCB1YrAc-","executionInfo":{"status":"ok","timestamp":1646369395214,"user_tz":360,"elapsed":171,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"c601ed47-a205-4c1c-a1a9-ecf820155019"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting preprocess_data.py\n"]}]}]}