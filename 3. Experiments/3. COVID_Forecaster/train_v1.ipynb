{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP6SesdwU4DoXISbsnNEamP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1646439223983,"user_tz":360,"elapsed":21169,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"58669f4d-e1e3-4e9b-9a3e-014c9b414764"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646439267028,"user_tz":360,"elapsed":43062,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"c1a6c990-bf6d-4ea2-d0a6-b1ff2e1dc98e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 11.5 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 9.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.12\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 237 kB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.5.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (747 kB)\n","\u001b[K     |████████████████████████████████| 747 kB 11.8 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.3.tar.gz (370 kB)\n","\u001b[K     |████████████████████████████████| 370 kB 12.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Collecting rdflib\n","  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n","\u001b[K     |████████████████████████████████| 482 kB 38.4 MB/s \n","\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (4.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Collecting isodate\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 704 kB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.10.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.3-py3-none-any.whl size=581968 sha256=04f253f32a50eae8dd837f41e6e9ff93363165e04b116e180edd63a1de098f7c\n","  Stored in directory: /root/.cache/pip/wheels/c3/2a/58/87ce0508964d4def1aafb92750c4f3ac77038efd1b9a89dcf5\n","Successfully built torch-geometric\n","Installing collected packages: isodate, yacs, rdflib, torch-geometric\n","Successfully installed isodate-0.6.1 rdflib-6.1.1 torch-geometric-2.0.3 yacs-0.1.8\n","Collecting torch-geometric-temporal\n","  Downloading torch_geometric_temporal-0.51.0.tar.gz (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: torch_sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.12)\n","Requirement already satisfied: torch_scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.13)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (6.1.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.6.3)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.1.8)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (4.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (57.4.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (0.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch_geometric->torch-geometric-temporal) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.1.0)\n","Building wheels for collected packages: torch-geometric-temporal\n","  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.51.0-py3-none-any.whl size=83569 sha256=5f76de78dd89ed167d50e573e1f4edbff0b81a623ccac05604da6373cbb8ecf0\n","  Stored in directory: /root/.cache/pip/wheels/a5/26/64/465700aa43b21fccca9ae446b407de2389f0ba16114e84db8d\n","Successfully built torch-geometric-temporal\n","Installing collected packages: torch-geometric-temporal\n","Successfully installed torch-geometric-temporal-0.51.0\n","Collecting ogb\n","  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Collecting outdated>=0.2.0\n","  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Collecting littleutils\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=72b98ae8bdf4c267a22dd424ab01cdc6dc84f276f0a03c47fc4ec368ee4faa28\n","  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n","PyTorch has version 1.10.0+cu111\n"]}],"source":["\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))"]},{"cell_type":"code","source":["\"\"\"\n","Import any other libraries we will use later on\n","\"\"\"\n","! pip install epiweeks\n","! pip install haversine\n","from preprocess_data import get_preprocessed_data\n","from torch_geometric.data import Data"],"metadata":{"id":"BfNcl5hIj8ci","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646439281082,"user_tz":360,"elapsed":14081,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"64e9ab35-f974-44b3-a679-a00a7053e4c8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting epiweeks\n","  Downloading epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n","Installing collected packages: epiweeks\n","Successfully installed epiweeks-2.1.4\n","Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Set device (CPU or GPU)\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uPbva_-KTSAt","executionInfo":{"status":"ok","timestamp":1646439281083,"user_tz":360,"elapsed":23,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"e289006a-b2fc-45b9-8842-e760104b5615"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Get preprocessed data\n","\"\"\"\n","\n","import pickle\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open('./data/preprocessed_data.pickle', 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open('./data/preprocessed_data.pickle', 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","\n","\n"],"metadata":{"id":"oWdP4J6ONPLU","executionInfo":{"status":"ok","timestamp":1646439282005,"user_tz":360,"elapsed":929,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Unpack preprocessed data\n","\"\"\"\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_change_in_confirmed_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_change_in_confirmed_smoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_change_in_confirmed_smoothed']"],"metadata":{"id":"gVfNhDK6kzKl","executionInfo":{"status":"ok","timestamp":1646439282006,"user_tz":360,"elapsed":11,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3","executionInfo":{"status":"ok","timestamp":1646439293742,"user_tz":360,"elapsed":11744,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","\"\"\"\n","# Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","\n","inputLayer_num_features = 24 \n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window\n","chebyshev_filter_size = 2\n","\n","class GCN(torch.nn.Module):\n","    # def __init__(self):\n","    #     super().__init__()\n","    #     self.conv1 = GCNConv(inputLayer_num_features, hiddenLayer1_num_features)\n","    #     self.conv2 = GCNConv(hiddenLayer1_num_features, outputLayer_num_features)\n","\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","\n","    # def forward(self, data):\n","    #     x, edge_index = data.x, data.edge_index\n","    #     x = self.conv1(x, edge_index)\n","    #     x = F.elu(x)\n","    #     x = self.conv2(x, edge_index)\n","    #     # print(x.shape)\n","    #     return x\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x)\n","      return x\n","\n","model = GCN().to(device)\n","\n","\"\"\"Predicting confirmed cases with UNSMOOTHED DATA\n","# optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 0, Loss 42769390666496.00, Val loss 1144893210624.00\n","# Epoch 1, Loss 43588523467264.00, Val loss 1144893210624.00\n","# Epoch 2, Loss 43588522479872.00, Val loss 1144893210624.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n","# ==================================================================\n","# Saved best model\n","# Epoch 0, Loss 42782691710720.00, Val loss 1145398493184.00\n","# Epoch 1, Loss 43682803331840.00, Val loss 1145398493184.00\n","# Epoch 2, Loss 43682803230208.00, Val loss 1145398493184.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.3)\n","# ==================================================================\n","# Saved best model\n","# Epoch 0, Loss 42842098256896.00, Val loss 1146748010496.00\n","# Epoch 1, Loss 44142974313472.00, Val loss 1146748010496.00\n","# Epoch 2, Loss 44142974349312.00, Val loss 1146748010496.00\n","# Epoch 3, Loss 44142974484992.00, Val loss 1146748010496.00\n","# Epoch 4, Loss 44142974195712.00, Val loss 1146748010496.00\n","# Epoch 5, Loss 44142974426624.00, Val loss 1146748010496.00\n","# Epoch 6, Loss 44142974675712.00, Val loss 1146748010496.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 30, Loss 45311937572864.00, Val loss 1152044630016.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","# Saved best model\n","# Epoch 11, Loss 52422146342912.00, Val loss 1338975584256.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=10) \n","# Saved best model\n","# Epoch 250, Loss 52617727680512.00, Val loss 1434736918528.00\n","# ==================================================================\n","\"\"\"\n","\n","\"\"\" Predicting CHANGE IN CONFIRMED CASES with UNSMOOTHED DATA\n","# optimizer = torch.optim.SGD(model.parameters(), lr=5)\n","# ==================================================================\n","# Saved best model\n","# Epoch 10, Loss 1406912358.97, Val loss 44110352.00\n","# Epoch 11, Loss 1406912362.75, Val loss 44111736.00\n","\n","## CURRENT ATTEMPT\n","# optimizer = torch.optim.SGD(model.parameters(), lr=3)\n","# ==================================================================\n","# Saved best model\n","# Epoch 1, Loss 9978198731645184.00, Val loss 4257833728.00\n","# Epoch 2, Loss 487194958592.00, Val loss 4257833728.00\n","# Epoch 3, Loss 487135717120.00, Val loss 4257833728.00\n","# Epoch 4, Loss 487123448320.00, Val loss 4257833728.00\n","# Epoch 5, Loss 487194958592.00, Val loss 4257833728.00\n","# Epoch 6, Loss 487194958592.00, Val loss 4257833728.00\n","# Epoch 7, Loss 487123672832.00, Val loss 4257833728.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 4, Loss 1254756897.00, Val loss 42590060.00\n","# Epoch 5, Loss 1254523141.75, Val loss 42590060.00\n","# Epoch 6, Loss 1254756964.66, Val loss 42590064.00\n","# Epoch 7, Loss 1254756966.41, Val loss 42590060.00\n","# Epoch 8, Loss 1254756963.06, Val loss 42590060.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","# ==================================================================\n","# Saved best model\n","# Epoch 249, Loss 1230884550.44, Val loss 39618176.00\n","# ==================================================================\n","# Saved best model\n","# Epoch 250, Loss 1229793268.94, Val loss 39596024.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 49, Loss 1303417711.12, Val loss 40663960.00\n","# Epoch 50, Loss 1307867300.75, Val loss 40700624.00\n","# Epoch 51, Loss 1307159946.75, Val loss 40732316.00\n","# Epoch 52, Loss 1306538857.00, Val loss 40759704.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 12, Loss 1340716037.00, Val loss 41348480.00\n","# Epoch 13, Loss 1338890136.38, Val loss 41497644.00\n","# Epoch 14, Loss 1339746923.50, Val loss 41397148.00\n","# Epoch 15, Loss 1340006091.38, Val loss 41366280.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=10)\n","# ==================================================================\n","# Saved best model\n","# Epoch 48, Loss 1410565205.31, Val loss 40933468.00\n","# Epoch 49, Loss 1410565144.25, Val loss 40933468.00\n","\"\"\"\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 2, Loss 844733180.70, Val loss 24130472.00\n","# Epoch 3, Loss 844733345.05, Val loss 24130472.00\n","# Epoch 4, Loss 844733344.17, Val loss 24130472.00\n","\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=10)\n","# ==================================================================\n","# Saved best model\n","# Epoch 0, Loss 981986268.34, Val loss 23840134.00\n","# Epoch 1, Loss 1002387839.06, Val loss 23864630.00\n","# Epoch 2, Loss 1001558686.19, Val loss 23866456.00\n","# Epoch 3, Loss 1001384564.81, Val loss 23867010.00\n","# Epoch 4, Loss 1001299320.44, Val loss 23867208.00\n","# Epoch 5, Loss 1001249199.88, Val loss 23867278.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 24, Loss 926364984.75, Val loss 24568306.00\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj","executionInfo":{"status":"ok","timestamp":1646439294635,"user_tz":360,"elapsed":907,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFKTgiyDqskX","executionInfo":{"status":"ok","timestamp":1646439294637,"user_tz":360,"elapsed":21,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"50b37464-53c4-4207-8ec5-2e62d5a4e59b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GCN(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","model_path = './saved_models/smoothed_adam'\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(1000):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, model_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHIlH0VdVKdK","executionInfo":{"status":"ok","timestamp":1646442198758,"user_tz":360,"elapsed":2904132,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"46a4dc08-5bd1-49bd-8623-dba6223dc9ca"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 1195170853.03, Val loss 32638086.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 1187919111.84, Val loss 32508062.00\n","==================================================================\n","Saved best model\n","Epoch 2, Loss 1181167663.00, Val loss 32383016.00\n","==================================================================\n","Saved best model\n","Epoch 3, Loss 1174625430.47, Val loss 32260538.00\n","==================================================================\n","Saved best model\n","Epoch 4, Loss 1168242720.26, Val loss 32140118.00\n","==================================================================\n","Saved best model\n","Epoch 5, Loss 1162003064.32, Val loss 32021536.00\n","==================================================================\n","Saved best model\n","Epoch 6, Loss 1155897721.48, Val loss 31904678.00\n","==================================================================\n","Saved best model\n","Epoch 7, Loss 1149920889.20, Val loss 31789462.00\n","==================================================================\n","Saved best model\n","Epoch 8, Loss 1144068223.09, Val loss 31675834.00\n","==================================================================\n","Saved best model\n","Epoch 9, Loss 1138336116.30, Val loss 31563742.00\n","==================================================================\n","Saved best model\n","Epoch 10, Loss 1132721481.65, Val loss 31453154.00\n","==================================================================\n","Saved best model\n","Epoch 11, Loss 1127221532.32, Val loss 31344024.00\n","==================================================================\n","Saved best model\n","Epoch 12, Loss 1121833719.92, Val loss 31236330.00\n","==================================================================\n","Saved best model\n","Epoch 13, Loss 1116555737.85, Val loss 31130048.00\n","==================================================================\n","Saved best model\n","Epoch 14, Loss 1111390683.54, Val loss 31025068.00\n","==================================================================\n","Saved best model\n","Epoch 15, Loss 1106308194.73, Val loss 30921422.00\n","==================================================================\n","Saved best model\n","Epoch 16, Loss 1101358661.15, Val loss 30819050.00\n","==================================================================\n","Saved best model\n","Epoch 17, Loss 1096408311.17, Val loss 30717858.00\n","==================================================================\n","Saved best model\n","Epoch 18, Loss 1091655391.66, Val loss 30618820.00\n","==================================================================\n","Saved best model\n","Epoch 19, Loss 1087008211.76, Val loss 30518610.00\n","==================================================================\n","Saved best model\n","Epoch 20, Loss 1082401355.22, Val loss 30421024.00\n","==================================================================\n","Saved best model\n","Epoch 21, Loss 1077852773.32, Val loss 30324690.00\n","==================================================================\n","Saved best model\n","Epoch 22, Loss 1073635830.73, Val loss 30229762.00\n","==================================================================\n","Saved best model\n","Epoch 23, Loss 1069047610.97, Val loss 30137292.00\n","==================================================================\n","Saved best model\n","Epoch 24, Loss 1064916484.78, Val loss 30046514.00\n","==================================================================\n","Saved best model\n","Epoch 25, Loss 1061675238.59, Val loss 30035804.00\n","==================================================================\n","Saved best model\n","Epoch 26, Loss 1060370118.02, Val loss 29946560.00\n","Epoch 27, Loss 1059349314.58, Val loss 30084438.00\n","==================================================================\n","Saved best model\n","Epoch 28, Loss 1053015713.23, Val loss 29775450.00\n","==================================================================\n","Saved best model\n","Epoch 29, Loss 1048291147.95, Val loss 29688844.00\n","==================================================================\n","Saved best model\n","Epoch 30, Loss 1044657472.12, Val loss 29604322.00\n","==================================================================\n","Saved best model\n","Epoch 31, Loss 1040961796.75, Val loss 29521086.00\n","==================================================================\n","Saved best model\n","Epoch 32, Loss 1037384389.52, Val loss 29442442.00\n","==================================================================\n","Saved best model\n","Epoch 33, Loss 1033579078.27, Val loss 29359200.00\n","==================================================================\n","Saved best model\n","Epoch 34, Loss 1030809150.84, Val loss 29279654.00\n","==================================================================\n","Saved best model\n","Epoch 35, Loss 1026780967.77, Val loss 29201278.00\n","==================================================================\n","Saved best model\n","Epoch 36, Loss 1023421365.73, Val loss 29123302.00\n","==================================================================\n","Saved best model\n","Epoch 37, Loss 1020050148.28, Val loss 29053570.00\n","==================================================================\n","Saved best model\n","Epoch 38, Loss 1016535377.22, Val loss 28976344.00\n","==================================================================\n","Saved best model\n","Epoch 39, Loss 1013241741.84, Val loss 28894308.00\n","==================================================================\n","Saved best model\n","Epoch 40, Loss 1010288896.28, Val loss 28826032.00\n","==================================================================\n","Saved best model\n","Epoch 41, Loss 1007055703.30, Val loss 28748050.00\n","==================================================================\n","Saved best model\n","Epoch 42, Loss 1003816697.77, Val loss 28681850.00\n","==================================================================\n","Saved best model\n","Epoch 43, Loss 1000665815.61, Val loss 28605598.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 997530976.25, Val loss 28535160.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 994489259.25, Val loss 28464404.00\n","==================================================================\n","Saved best model\n","Epoch 46, Loss 991564954.00, Val loss 28391688.00\n","==================================================================\n","Saved best model\n","Epoch 47, Loss 989556453.94, Val loss 28324006.00\n","==================================================================\n","Saved best model\n","Epoch 48, Loss 985970973.59, Val loss 28253520.00\n","==================================================================\n","Saved best model\n","Epoch 49, Loss 983096485.34, Val loss 28185330.00\n","==================================================================\n","Saved best model\n","Epoch 50, Loss 980497766.03, Val loss 28116820.00\n","==================================================================\n","Saved best model\n","Epoch 51, Loss 977810179.91, Val loss 28047648.00\n","==================================================================\n","Saved best model\n","Epoch 52, Loss 975487134.81, Val loss 27912510.00\n","==================================================================\n","Saved best model\n","Epoch 53, Loss 970030891.38, Val loss 27843480.00\n","==================================================================\n","Saved best model\n","Epoch 54, Loss 967417934.03, Val loss 27777754.00\n","==================================================================\n","Saved best model\n","Epoch 55, Loss 964237254.06, Val loss 27722478.00\n","==================================================================\n","Saved best model\n","Epoch 56, Loss 961339524.09, Val loss 27656486.00\n","==================================================================\n","Saved best model\n","Epoch 57, Loss 958535773.66, Val loss 27587322.00\n","==================================================================\n","Saved best model\n","Epoch 58, Loss 956104788.28, Val loss 27522044.00\n","==================================================================\n","Saved best model\n","Epoch 59, Loss 953334651.81, Val loss 27458308.00\n","==================================================================\n","Saved best model\n","Epoch 60, Loss 951019444.50, Val loss 27392784.00\n","==================================================================\n","Saved best model\n","Epoch 61, Loss 948528216.34, Val loss 27336616.00\n","==================================================================\n","Saved best model\n","Epoch 62, Loss 947424439.56, Val loss 27268738.00\n","==================================================================\n","Saved best model\n","Epoch 63, Loss 943871110.97, Val loss 27200940.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 941548020.31, Val loss 27140292.00\n","==================================================================\n","Saved best model\n","Epoch 65, Loss 938881615.97, Val loss 27081798.00\n","==================================================================\n","Saved best model\n","Epoch 66, Loss 936588645.62, Val loss 27020252.00\n","==================================================================\n","Saved best model\n","Epoch 67, Loss 933935032.78, Val loss 26963108.00\n","==================================================================\n","Saved best model\n","Epoch 68, Loss 931756681.09, Val loss 26899582.00\n","==================================================================\n","Saved best model\n","Epoch 69, Loss 929343103.66, Val loss 26850476.00\n","==================================================================\n","Saved best model\n","Epoch 70, Loss 927091978.84, Val loss 26789340.00\n","==================================================================\n","Saved best model\n","Epoch 71, Loss 924790182.25, Val loss 26729640.00\n","==================================================================\n","Saved best model\n","Epoch 72, Loss 922734327.81, Val loss 26667574.00\n","==================================================================\n","Saved best model\n","Epoch 73, Loss 920401850.62, Val loss 26613578.00\n","==================================================================\n","Saved best model\n","Epoch 74, Loss 918269736.06, Val loss 26553120.00\n","==================================================================\n","Saved best model\n","Epoch 75, Loss 916031298.72, Val loss 26496294.00\n","==================================================================\n","Saved best model\n","Epoch 76, Loss 914103794.84, Val loss 26436364.00\n","==================================================================\n","Saved best model\n","Epoch 77, Loss 911969803.47, Val loss 26381562.00\n","==================================================================\n","Saved best model\n","Epoch 78, Loss 910232236.59, Val loss 26331038.00\n","==================================================================\n","Saved best model\n","Epoch 79, Loss 907633978.44, Val loss 26276272.00\n","==================================================================\n","Saved best model\n","Epoch 80, Loss 905710104.44, Val loss 26223250.00\n","==================================================================\n","Saved best model\n","Epoch 81, Loss 903754338.78, Val loss 26174510.00\n","==================================================================\n","Saved best model\n","Epoch 82, Loss 901623507.47, Val loss 26117110.00\n","==================================================================\n","Saved best model\n","Epoch 83, Loss 899689916.41, Val loss 26063920.00\n","==================================================================\n","Saved best model\n","Epoch 84, Loss 897949642.28, Val loss 26007784.00\n","==================================================================\n","Saved best model\n","Epoch 85, Loss 896058598.53, Val loss 25956048.00\n","==================================================================\n","Saved best model\n","Epoch 86, Loss 894516517.59, Val loss 25899596.00\n","==================================================================\n","Saved best model\n","Epoch 87, Loss 892625403.22, Val loss 25850044.00\n","==================================================================\n","Saved best model\n","Epoch 88, Loss 890933196.47, Val loss 25794056.00\n","==================================================================\n","Saved best model\n","Epoch 89, Loss 889327391.03, Val loss 25746228.00\n","==================================================================\n","Saved best model\n","Epoch 90, Loss 887660317.53, Val loss 25691316.00\n","==================================================================\n","Saved best model\n","Epoch 91, Loss 886175741.53, Val loss 25643800.00\n","==================================================================\n","Saved best model\n","Epoch 92, Loss 884837059.88, Val loss 25591308.00\n","==================================================================\n","Saved best model\n","Epoch 93, Loss 882852118.50, Val loss 25543370.00\n","==================================================================\n","Saved best model\n","Epoch 94, Loss 881536899.88, Val loss 25490974.00\n","==================================================================\n","Saved best model\n","Epoch 95, Loss 880344013.91, Val loss 25450358.00\n","==================================================================\n","Saved best model\n","Epoch 96, Loss 878436881.53, Val loss 25404412.00\n","==================================================================\n","Saved best model\n","Epoch 97, Loss 876778995.25, Val loss 25355664.00\n","==================================================================\n","Saved best model\n","Epoch 98, Loss 875220684.75, Val loss 25308730.00\n","==================================================================\n","Saved best model\n","Epoch 99, Loss 873717533.59, Val loss 25263472.00\n","==================================================================\n","Saved best model\n","Epoch 100, Loss 872359805.72, Val loss 25213944.00\n","==================================================================\n","Saved best model\n","Epoch 101, Loss 870988882.72, Val loss 25170024.00\n","==================================================================\n","Saved best model\n","Epoch 102, Loss 869658818.25, Val loss 25121404.00\n","==================================================================\n","Saved best model\n","Epoch 103, Loss 868233233.41, Val loss 25080352.00\n","==================================================================\n","Saved best model\n","Epoch 104, Loss 866854322.16, Val loss 25035188.00\n","Epoch 105, Loss 881705644.38, Val loss 25507006.00\n","Epoch 106, Loss 879686149.00, Val loss 25459072.00\n","Epoch 107, Loss 878085199.84, Val loss 25415760.00\n","Epoch 108, Loss 876292269.75, Val loss 25368918.00\n","Epoch 109, Loss 874719996.03, Val loss 25320482.00\n","Epoch 110, Loss 873231387.50, Val loss 25274010.00\n","Epoch 111, Loss 871737933.62, Val loss 25227714.00\n","Epoch 112, Loss 870318082.00, Val loss 25186882.00\n","Epoch 113, Loss 868961149.84, Val loss 25135294.00\n","Epoch 114, Loss 867556341.31, Val loss 25100056.00\n","Epoch 115, Loss 866372461.00, Val loss 25045358.00\n","==================================================================\n","Saved best model\n","Epoch 116, Loss 865043597.19, Val loss 24999134.00\n","==================================================================\n","Saved best model\n","Epoch 117, Loss 863645889.38, Val loss 24956766.00\n","==================================================================\n","Saved best model\n","Epoch 118, Loss 862284734.47, Val loss 24924378.00\n","==================================================================\n","Saved best model\n","Epoch 119, Loss 861261661.34, Val loss 24870330.00\n","==================================================================\n","Saved best model\n","Epoch 120, Loss 859895516.72, Val loss 24839592.00\n","==================================================================\n","Saved best model\n","Epoch 121, Loss 858815467.12, Val loss 24782890.00\n","==================================================================\n","Saved best model\n","Epoch 122, Loss 857500542.97, Val loss 24742428.00\n","==================================================================\n","Saved best model\n","Epoch 123, Loss 856140656.06, Val loss 24708952.00\n","==================================================================\n","Saved best model\n","Epoch 124, Loss 855281597.97, Val loss 24661054.00\n","==================================================================\n","Saved best model\n","Epoch 125, Loss 853838662.00, Val loss 24634090.00\n","==================================================================\n","Saved best model\n","Epoch 126, Loss 852995791.72, Val loss 24595530.00\n","==================================================================\n","Saved best model\n","Epoch 127, Loss 851606378.91, Val loss 24583374.00\n","==================================================================\n","Saved best model\n","Epoch 128, Loss 850689511.91, Val loss 24524556.00\n","==================================================================\n","Saved best model\n","Epoch 129, Loss 849529578.88, Val loss 24521026.00\n","==================================================================\n","Saved best model\n","Epoch 130, Loss 847922362.56, Val loss 24457748.00\n","==================================================================\n","Saved best model\n","Epoch 131, Loss 846421893.97, Val loss 24451220.00\n","==================================================================\n","Saved best model\n","Epoch 132, Loss 845477077.84, Val loss 24418200.00\n","==================================================================\n","Saved best model\n","Epoch 133, Loss 844189187.38, Val loss 24378820.00\n","==================================================================\n","Saved best model\n","Epoch 134, Loss 843058326.47, Val loss 24343206.00\n","==================================================================\n","Saved best model\n","Epoch 135, Loss 842055337.84, Val loss 24303250.00\n","==================================================================\n","Saved best model\n","Epoch 136, Loss 840813075.12, Val loss 24275194.00\n","==================================================================\n","Saved best model\n","Epoch 137, Loss 839894854.97, Val loss 24230904.00\n","==================================================================\n","Saved best model\n","Epoch 138, Loss 838826294.44, Val loss 24189496.00\n","==================================================================\n","Saved best model\n","Epoch 139, Loss 838045736.06, Val loss 24172430.00\n","==================================================================\n","Saved best model\n","Epoch 140, Loss 836927031.84, Val loss 24123656.00\n","==================================================================\n","Saved best model\n","Epoch 141, Loss 835597313.53, Val loss 24119872.00\n","==================================================================\n","Saved best model\n","Epoch 142, Loss 834960524.41, Val loss 24063774.00\n","==================================================================\n","Saved best model\n","Epoch 143, Loss 833504877.41, Val loss 24047992.00\n","==================================================================\n","Saved best model\n","Epoch 144, Loss 832784855.06, Val loss 23994892.00\n","==================================================================\n","Saved best model\n","Epoch 145, Loss 831669300.38, Val loss 23978016.00\n","==================================================================\n","Saved best model\n","Epoch 146, Loss 831360486.75, Val loss 23929596.00\n","==================================================================\n","Saved best model\n","Epoch 147, Loss 830009492.28, Val loss 23909500.00\n","==================================================================\n","Saved best model\n","Epoch 148, Loss 829663900.38, Val loss 23865134.00\n","==================================================================\n","Saved best model\n","Epoch 149, Loss 828330224.97, Val loss 23840968.00\n","==================================================================\n","Saved best model\n","Epoch 150, Loss 827683404.38, Val loss 23799762.00\n","==================================================================\n","Saved best model\n","Epoch 151, Loss 826769617.22, Val loss 23776902.00\n","==================================================================\n","Saved best model\n","Epoch 152, Loss 826159979.56, Val loss 23741538.00\n","==================================================================\n","Saved best model\n","Epoch 153, Loss 825309335.03, Val loss 23718396.00\n","==================================================================\n","Saved best model\n","Epoch 154, Loss 824243697.09, Val loss 23687336.00\n","==================================================================\n","Saved best model\n","Epoch 155, Loss 823642123.31, Val loss 23657142.00\n","==================================================================\n","Saved best model\n","Epoch 156, Loss 822401863.69, Val loss 23628062.00\n","==================================================================\n","Saved best model\n","Epoch 157, Loss 820794662.81, Val loss 23602056.00\n","==================================================================\n","Saved best model\n","Epoch 158, Loss 819737218.19, Val loss 23565424.00\n","==================================================================\n","Saved best model\n","Epoch 159, Loss 819374993.28, Val loss 23545532.00\n","==================================================================\n","Saved best model\n","Epoch 160, Loss 819860655.47, Val loss 23524536.00\n","==================================================================\n","Saved best model\n","Epoch 161, Loss 817243028.78, Val loss 23511132.00\n","==================================================================\n","Saved best model\n","Epoch 162, Loss 816238005.03, Val loss 23460702.00\n","Epoch 163, Loss 815564067.25, Val loss 23466242.00\n","==================================================================\n","Saved best model\n","Epoch 164, Loss 814153098.38, Val loss 23420618.00\n","==================================================================\n","Saved best model\n","Epoch 165, Loss 813767199.00, Val loss 23366568.00\n","Epoch 166, Loss 811771804.91, Val loss 23379246.00\n","==================================================================\n","Saved best model\n","Epoch 167, Loss 811499450.47, Val loss 23349270.00\n","==================================================================\n","Saved best model\n","Epoch 168, Loss 810482986.16, Val loss 23328760.00\n","==================================================================\n","Saved best model\n","Epoch 169, Loss 808783720.09, Val loss 23298808.00\n","==================================================================\n","Saved best model\n","Epoch 170, Loss 808235923.66, Val loss 23268372.00\n","==================================================================\n","Saved best model\n","Epoch 171, Loss 806979939.03, Val loss 23241432.00\n","==================================================================\n","Saved best model\n","Epoch 172, Loss 806167693.88, Val loss 23222040.00\n","==================================================================\n","Saved best model\n","Epoch 173, Loss 805304838.47, Val loss 23189230.00\n","==================================================================\n","Saved best model\n","Epoch 174, Loss 804421994.69, Val loss 23156978.00\n","==================================================================\n","Saved best model\n","Epoch 175, Loss 803356153.69, Val loss 23132040.00\n","==================================================================\n","Saved best model\n","Epoch 176, Loss 803048008.41, Val loss 23105550.00\n","Epoch 177, Loss 809462032.72, Val loss 23660734.00\n","Epoch 178, Loss 806804597.81, Val loss 23295932.00\n","Epoch 179, Loss 805344561.41, Val loss 23273920.00\n","Epoch 180, Loss 804743790.38, Val loss 23234186.00\n","Epoch 181, Loss 803854743.72, Val loss 23204632.00\n","Epoch 182, Loss 803063250.62, Val loss 23181086.00\n","Epoch 183, Loss 802498377.25, Val loss 23144108.00\n","Epoch 184, Loss 801528490.00, Val loss 23129622.00\n","==================================================================\n","Saved best model\n","Epoch 185, Loss 800858145.97, Val loss 23079424.00\n","==================================================================\n","Saved best model\n","Epoch 186, Loss 800223223.28, Val loss 23049606.00\n","==================================================================\n","Saved best model\n","Epoch 187, Loss 799450478.12, Val loss 23019742.00\n","==================================================================\n","Saved best model\n","Epoch 188, Loss 798769887.00, Val loss 22993186.00\n","==================================================================\n","Saved best model\n","Epoch 189, Loss 798008053.84, Val loss 22969918.00\n","==================================================================\n","Saved best model\n","Epoch 190, Loss 797665437.28, Val loss 22936970.00\n","==================================================================\n","Saved best model\n","Epoch 191, Loss 796675481.59, Val loss 22909456.00\n","==================================================================\n","Saved best model\n","Epoch 192, Loss 795807591.38, Val loss 22880282.00\n","==================================================================\n","Saved best model\n","Epoch 193, Loss 795160571.84, Val loss 22854018.00\n","==================================================================\n","Saved best model\n","Epoch 194, Loss 794413981.41, Val loss 22834964.00\n","==================================================================\n","Saved best model\n","Epoch 195, Loss 793808449.47, Val loss 22807074.00\n","==================================================================\n","Saved best model\n","Epoch 196, Loss 793088301.38, Val loss 22786972.00\n","==================================================================\n","Saved best model\n","Epoch 197, Loss 796257205.84, Val loss 22759590.00\n","==================================================================\n","Saved best model\n","Epoch 198, Loss 791452377.44, Val loss 22743274.00\n","==================================================================\n","Saved best model\n","Epoch 199, Loss 790860362.22, Val loss 22715928.00\n","==================================================================\n","Saved best model\n","Epoch 200, Loss 790188512.91, Val loss 22690030.00\n","==================================================================\n","Saved best model\n","Epoch 201, Loss 789627522.78, Val loss 22662244.00\n","==================================================================\n","Saved best model\n","Epoch 202, Loss 788757511.00, Val loss 22635810.00\n","==================================================================\n","Saved best model\n","Epoch 203, Loss 788062464.94, Val loss 22616470.00\n","==================================================================\n","Saved best model\n","Epoch 204, Loss 787686210.81, Val loss 22584340.00\n","==================================================================\n","Saved best model\n","Epoch 205, Loss 787545807.62, Val loss 22558414.00\n","==================================================================\n","Saved best model\n","Epoch 206, Loss 786198821.81, Val loss 22535790.00\n","==================================================================\n","Saved best model\n","Epoch 207, Loss 785658381.12, Val loss 22507856.00\n","==================================================================\n","Saved best model\n","Epoch 208, Loss 785133059.28, Val loss 22481820.00\n","Epoch 209, Loss 790317121.91, Val loss 22732758.00\n","Epoch 210, Loss 787998619.62, Val loss 22705656.00\n","Epoch 211, Loss 787209479.78, Val loss 22680198.00\n","Epoch 212, Loss 786472698.28, Val loss 22662056.00\n","Epoch 213, Loss 786197524.62, Val loss 22627320.00\n","Epoch 214, Loss 785474399.56, Val loss 22599418.00\n","Epoch 215, Loss 784776799.56, Val loss 22574470.00\n","Epoch 216, Loss 784063167.44, Val loss 22559078.00\n","Epoch 217, Loss 783781373.44, Val loss 22519842.00\n","Epoch 218, Loss 782982400.66, Val loss 22496646.00\n","==================================================================\n","Saved best model\n","Epoch 219, Loss 782275489.03, Val loss 22481742.00\n","==================================================================\n","Saved best model\n","Epoch 220, Loss 781904911.50, Val loss 22452230.00\n","==================================================================\n","Saved best model\n","Epoch 221, Loss 781282461.28, Val loss 22421678.00\n","==================================================================\n","Saved best model\n","Epoch 222, Loss 780608716.22, Val loss 22409006.00\n","==================================================================\n","Saved best model\n","Epoch 223, Loss 780140334.16, Val loss 22380098.00\n","==================================================================\n","Saved best model\n","Epoch 224, Loss 779613304.69, Val loss 22351004.00\n","==================================================================\n","Saved best model\n","Epoch 225, Loss 778860086.94, Val loss 22340230.00\n","==================================================================\n","Saved best model\n","Epoch 226, Loss 778388768.88, Val loss 22316086.00\n","==================================================================\n","Saved best model\n","Epoch 227, Loss 777798757.88, Val loss 22306872.00\n","==================================================================\n","Saved best model\n","Epoch 228, Loss 777164200.53, Val loss 22275472.00\n","==================================================================\n","Saved best model\n","Epoch 229, Loss 776630101.12, Val loss 22270120.00\n","==================================================================\n","Saved best model\n","Epoch 230, Loss 776059065.16, Val loss 22261874.00\n","==================================================================\n","Saved best model\n","Epoch 231, Loss 775283812.03, Val loss 22241616.00\n","==================================================================\n","Saved best model\n","Epoch 232, Loss 774621146.25, Val loss 22223042.00\n","==================================================================\n","Saved best model\n","Epoch 233, Loss 774004170.56, Val loss 22200126.00\n","==================================================================\n","Saved best model\n","Epoch 234, Loss 773444238.44, Val loss 22179888.00\n","==================================================================\n","Saved best model\n","Epoch 235, Loss 772910368.31, Val loss 22156262.00\n","==================================================================\n","Saved best model\n","Epoch 236, Loss 772380952.16, Val loss 22138282.00\n","==================================================================\n","Saved best model\n","Epoch 237, Loss 771781196.88, Val loss 22115630.00\n","==================================================================\n","Saved best model\n","Epoch 238, Loss 771258473.75, Val loss 22095558.00\n","==================================================================\n","Saved best model\n","Epoch 239, Loss 770741516.84, Val loss 22072940.00\n","==================================================================\n","Saved best model\n","Epoch 240, Loss 770222394.41, Val loss 22053264.00\n","==================================================================\n","Saved best model\n","Epoch 241, Loss 770306792.72, Val loss 22031408.00\n","==================================================================\n","Saved best model\n","Epoch 242, Loss 769183725.06, Val loss 22010708.00\n","==================================================================\n","Saved best model\n","Epoch 243, Loss 768774628.97, Val loss 21997714.00\n","==================================================================\n","Saved best model\n","Epoch 244, Loss 768028856.31, Val loss 21968642.00\n","==================================================================\n","Saved best model\n","Epoch 245, Loss 767791604.66, Val loss 21956128.00\n","==================================================================\n","Saved best model\n","Epoch 246, Loss 767111454.72, Val loss 21926878.00\n","==================================================================\n","Saved best model\n","Epoch 247, Loss 766683990.91, Val loss 21917410.00\n","==================================================================\n","Saved best model\n","Epoch 248, Loss 766224569.69, Val loss 21886618.00\n","==================================================================\n","Saved best model\n","Epoch 249, Loss 765688317.50, Val loss 21884284.00\n","==================================================================\n","Saved best model\n","Epoch 250, Loss 765200939.53, Val loss 21872530.00\n","==================================================================\n","Saved best model\n","Epoch 251, Loss 764660828.03, Val loss 21854824.00\n","==================================================================\n","Saved best model\n","Epoch 252, Loss 764192861.38, Val loss 21835686.00\n","==================================================================\n","Saved best model\n","Epoch 253, Loss 763728430.19, Val loss 21816326.00\n","==================================================================\n","Saved best model\n","Epoch 254, Loss 763346347.66, Val loss 21796812.00\n","==================================================================\n","Saved best model\n","Epoch 255, Loss 762860449.16, Val loss 21777660.00\n","==================================================================\n","Saved best model\n","Epoch 256, Loss 762434197.69, Val loss 21758352.00\n","Epoch 257, Loss 778054401.62, Val loss 22397766.00\n","Epoch 258, Loss 772604570.12, Val loss 22372336.00\n","Epoch 259, Loss 771958672.59, Val loss 22324904.00\n","Epoch 260, Loss 771247446.47, Val loss 22266222.00\n","Epoch 261, Loss 770630453.78, Val loss 22238866.00\n","Epoch 262, Loss 770204174.38, Val loss 22209426.00\n","Epoch 263, Loss 769757154.53, Val loss 22191566.00\n","Epoch 264, Loss 769018938.47, Val loss 22168068.00\n","Epoch 265, Loss 768484908.19, Val loss 22146078.00\n","Epoch 266, Loss 767878766.22, Val loss 22124536.00\n","Epoch 267, Loss 767205783.47, Val loss 22106108.00\n","Epoch 268, Loss 766892005.06, Val loss 22122360.00\n","Epoch 269, Loss 766829143.16, Val loss 22058946.00\n","Epoch 270, Loss 765828974.31, Val loss 22037352.00\n","Epoch 271, Loss 765268525.66, Val loss 22013674.00\n","Epoch 272, Loss 764910641.78, Val loss 21994924.00\n","Epoch 273, Loss 764253622.53, Val loss 21966550.00\n","Epoch 274, Loss 763794037.88, Val loss 21955366.00\n","Epoch 275, Loss 763060184.88, Val loss 21927682.00\n","Epoch 276, Loss 762702921.03, Val loss 21913364.00\n","Epoch 277, Loss 761917615.94, Val loss 21887450.00\n","Epoch 278, Loss 761694817.25, Val loss 21872124.00\n","Epoch 279, Loss 760969580.91, Val loss 21855792.00\n","Epoch 280, Loss 760756791.97, Val loss 21833144.00\n","Epoch 281, Loss 760046212.72, Val loss 21822530.00\n","Epoch 282, Loss 759911690.12, Val loss 21794736.00\n","Epoch 283, Loss 759169576.75, Val loss 21785498.00\n","==================================================================\n","Saved best model\n","Epoch 284, Loss 759320628.78, Val loss 21757474.00\n","==================================================================\n","Saved best model\n","Epoch 285, Loss 758702127.84, Val loss 21728518.00\n","Epoch 286, Loss 757557150.78, Val loss 21735800.00\n","==================================================================\n","Saved best model\n","Epoch 287, Loss 758050567.75, Val loss 21706354.00\n","==================================================================\n","Saved best model\n","Epoch 288, Loss 757156152.66, Val loss 21699550.00\n","==================================================================\n","Saved best model\n","Epoch 289, Loss 756958400.72, Val loss 21670384.00\n","==================================================================\n","Saved best model\n","Epoch 290, Loss 756366431.50, Val loss 21661366.00\n","==================================================================\n","Saved best model\n","Epoch 291, Loss 756003049.69, Val loss 21631446.00\n","==================================================================\n","Saved best model\n","Epoch 292, Loss 755381897.38, Val loss 21626334.00\n","==================================================================\n","Saved best model\n","Epoch 293, Loss 755077885.50, Val loss 21605170.00\n","Epoch 294, Loss 754465288.22, Val loss 21609294.00\n","==================================================================\n","Saved best model\n","Epoch 295, Loss 754139353.94, Val loss 21586234.00\n","==================================================================\n","Saved best model\n","Epoch 296, Loss 753504072.84, Val loss 21577622.00\n","==================================================================\n","Saved best model\n","Epoch 297, Loss 753133642.62, Val loss 21553240.00\n","==================================================================\n","Saved best model\n","Epoch 298, Loss 752580918.75, Val loss 21540170.00\n","==================================================================\n","Saved best model\n","Epoch 299, Loss 752190606.47, Val loss 21517570.00\n","==================================================================\n","Saved best model\n","Epoch 300, Loss 751768183.34, Val loss 21502788.00\n","==================================================================\n","Saved best model\n","Epoch 301, Loss 751411076.94, Val loss 21482810.00\n","==================================================================\n","Saved best model\n","Epoch 302, Loss 750906911.81, Val loss 21466216.00\n","==================================================================\n","Saved best model\n","Epoch 303, Loss 750567618.94, Val loss 21451772.00\n","==================================================================\n","Saved best model\n","Epoch 304, Loss 749974941.69, Val loss 21432846.00\n","==================================================================\n","Saved best model\n","Epoch 305, Loss 750995161.41, Val loss 21411768.00\n","==================================================================\n","Saved best model\n","Epoch 306, Loss 749140631.78, Val loss 21408786.00\n","==================================================================\n","Saved best model\n","Epoch 307, Loss 748683298.00, Val loss 21385826.00\n","==================================================================\n","Saved best model\n","Epoch 308, Loss 748245774.47, Val loss 21376694.00\n","==================================================================\n","Saved best model\n","Epoch 309, Loss 747929496.53, Val loss 21357518.00\n","==================================================================\n","Saved best model\n","Epoch 310, Loss 747456037.22, Val loss 21344100.00\n","==================================================================\n","Saved best model\n","Epoch 311, Loss 746967108.00, Val loss 21335960.00\n","==================================================================\n","Saved best model\n","Epoch 312, Loss 746526637.50, Val loss 21324916.00\n","==================================================================\n","Saved best model\n","Epoch 313, Loss 746159895.53, Val loss 21315312.00\n","==================================================================\n","Saved best model\n","Epoch 314, Loss 745789532.81, Val loss 21299854.00\n","==================================================================\n","Saved best model\n","Epoch 315, Loss 745384395.53, Val loss 21282826.00\n","==================================================================\n","Saved best model\n","Epoch 316, Loss 745083837.16, Val loss 21269910.00\n","==================================================================\n","Saved best model\n","Epoch 317, Loss 744682570.53, Val loss 21250682.00\n","==================================================================\n","Saved best model\n","Epoch 318, Loss 744355775.16, Val loss 21239622.00\n","==================================================================\n","Saved best model\n","Epoch 319, Loss 745888159.16, Val loss 21217152.00\n","==================================================================\n","Saved best model\n","Epoch 320, Loss 743813664.06, Val loss 21200800.00\n","==================================================================\n","Saved best model\n","Epoch 321, Loss 743393025.34, Val loss 21185414.00\n","==================================================================\n","Saved best model\n","Epoch 322, Loss 743005028.75, Val loss 21169970.00\n","==================================================================\n","Saved best model\n","Epoch 323, Loss 742654289.34, Val loss 21155000.00\n","==================================================================\n","Saved best model\n","Epoch 324, Loss 743637820.44, Val loss 21140546.00\n","==================================================================\n","Saved best model\n","Epoch 325, Loss 741977536.59, Val loss 21125268.00\n","==================================================================\n","Saved best model\n","Epoch 326, Loss 741644408.94, Val loss 21110806.00\n","==================================================================\n","Saved best model\n","Epoch 327, Loss 741313658.38, Val loss 21096474.00\n","==================================================================\n","Saved best model\n","Epoch 328, Loss 740988848.38, Val loss 21082342.00\n","==================================================================\n","Saved best model\n","Epoch 329, Loss 740666343.62, Val loss 21068472.00\n","==================================================================\n","Saved best model\n","Epoch 330, Loss 740345225.56, Val loss 21054658.00\n","==================================================================\n","Saved best model\n","Epoch 331, Loss 740023487.47, Val loss 21041094.00\n","==================================================================\n","Saved best model\n","Epoch 332, Loss 739699267.91, Val loss 21027562.00\n","==================================================================\n","Saved best model\n","Epoch 333, Loss 739382572.75, Val loss 21014294.00\n","==================================================================\n","Saved best model\n","Epoch 334, Loss 739080327.88, Val loss 21000938.00\n","==================================================================\n","Saved best model\n","Epoch 335, Loss 738785528.16, Val loss 20987898.00\n","==================================================================\n","Saved best model\n","Epoch 336, Loss 738497860.91, Val loss 20974656.00\n","==================================================================\n","Saved best model\n","Epoch 337, Loss 738206214.66, Val loss 20961816.00\n","==================================================================\n","Saved best model\n","Epoch 338, Loss 737929239.69, Val loss 20948682.00\n","==================================================================\n","Saved best model\n","Epoch 339, Loss 737639753.19, Val loss 20935932.00\n","==================================================================\n","Saved best model\n","Epoch 340, Loss 737357506.91, Val loss 20923006.00\n","==================================================================\n","Saved best model\n","Epoch 341, Loss 737191715.16, Val loss 20909682.00\n","==================================================================\n","Saved best model\n","Epoch 342, Loss 736807840.91, Val loss 20897128.00\n","==================================================================\n","Saved best model\n","Epoch 343, Loss 736526345.09, Val loss 20883934.00\n","==================================================================\n","Saved best model\n","Epoch 344, Loss 736244521.28, Val loss 20870974.00\n","==================================================================\n","Saved best model\n","Epoch 345, Loss 735970664.16, Val loss 20857972.00\n","==================================================================\n","Saved best model\n","Epoch 346, Loss 735703630.09, Val loss 20844868.00\n","==================================================================\n","Saved best model\n","Epoch 347, Loss 735464442.59, Val loss 20832320.00\n","==================================================================\n","Saved best model\n","Epoch 348, Loss 735215862.88, Val loss 20819306.00\n","==================================================================\n","Saved best model\n","Epoch 349, Loss 734967817.06, Val loss 20806422.00\n","==================================================================\n","Saved best model\n","Epoch 350, Loss 734734022.62, Val loss 20793628.00\n","==================================================================\n","Saved best model\n","Epoch 351, Loss 734496286.31, Val loss 20781216.00\n","==================================================================\n","Saved best model\n","Epoch 352, Loss 734250925.94, Val loss 20768182.00\n","==================================================================\n","Saved best model\n","Epoch 353, Loss 734019125.94, Val loss 20756496.00\n","==================================================================\n","Saved best model\n","Epoch 354, Loss 733779799.56, Val loss 20743858.00\n","==================================================================\n","Saved best model\n","Epoch 355, Loss 733533508.06, Val loss 20731738.00\n","==================================================================\n","Saved best model\n","Epoch 356, Loss 733300284.06, Val loss 20719536.00\n","==================================================================\n","Saved best model\n","Epoch 357, Loss 733068452.75, Val loss 20707508.00\n","==================================================================\n","Saved best model\n","Epoch 358, Loss 732841249.50, Val loss 20695480.00\n","==================================================================\n","Saved best model\n","Epoch 359, Loss 732565884.88, Val loss 20683098.00\n","==================================================================\n","Saved best model\n","Epoch 360, Loss 732399623.31, Val loss 20672154.00\n","==================================================================\n","Saved best model\n","Epoch 361, Loss 732132986.25, Val loss 20659406.00\n","==================================================================\n","Saved best model\n","Epoch 362, Loss 731886890.38, Val loss 20647616.00\n","==================================================================\n","Saved best model\n","Epoch 363, Loss 731719111.12, Val loss 20637340.00\n","==================================================================\n","Saved best model\n","Epoch 364, Loss 731459226.19, Val loss 20625278.00\n","==================================================================\n","Saved best model\n","Epoch 365, Loss 731058965.88, Val loss 20606968.00\n","==================================================================\n","Saved best model\n","Epoch 366, Loss 731243455.75, Val loss 20604668.00\n","==================================================================\n","Saved best model\n","Epoch 367, Loss 730587096.62, Val loss 20583352.00\n","Epoch 368, Loss 731134911.69, Val loss 20583708.00\n","==================================================================\n","Saved best model\n","Epoch 369, Loss 730322858.19, Val loss 20561442.00\n","==================================================================\n","Saved best model\n","Epoch 370, Loss 730102836.69, Val loss 20550488.00\n","==================================================================\n","Saved best model\n","Epoch 371, Loss 729632866.19, Val loss 20538054.00\n","==================================================================\n","Saved best model\n","Epoch 372, Loss 729493738.75, Val loss 20527078.00\n","==================================================================\n","Saved best model\n","Epoch 373, Loss 729197903.75, Val loss 20515678.00\n","==================================================================\n","Saved best model\n","Epoch 374, Loss 728983792.50, Val loss 20504646.00\n","==================================================================\n","Saved best model\n","Epoch 375, Loss 728763467.69, Val loss 20493766.00\n","==================================================================\n","Saved best model\n","Epoch 376, Loss 728611235.94, Val loss 20480998.00\n","==================================================================\n","Saved best model\n","Epoch 377, Loss 728534694.88, Val loss 20472832.00\n","==================================================================\n","Saved best model\n","Epoch 378, Loss 728240857.25, Val loss 20458970.00\n","==================================================================\n","Saved best model\n","Epoch 379, Loss 728014138.94, Val loss 20458588.00\n","==================================================================\n","Saved best model\n","Epoch 380, Loss 727778267.62, Val loss 20441192.00\n","Epoch 381, Loss 727511246.25, Val loss 20444256.00\n","==================================================================\n","Saved best model\n","Epoch 382, Loss 727341507.88, Val loss 20420410.00\n","Epoch 383, Loss 727092736.62, Val loss 20425214.00\n","==================================================================\n","Saved best model\n","Epoch 384, Loss 726930019.50, Val loss 20400224.00\n","==================================================================\n","Saved best model\n","Epoch 385, Loss 727667438.44, Val loss 20399830.00\n","==================================================================\n","Saved best model\n","Epoch 386, Loss 726845010.94, Val loss 20379172.00\n","Epoch 387, Loss 726551071.69, Val loss 20385328.00\n","==================================================================\n","Saved best model\n","Epoch 388, Loss 726269439.88, Val loss 20359616.00\n","Epoch 389, Loss 725978516.12, Val loss 20363532.00\n","==================================================================\n","Saved best model\n","Epoch 390, Loss 725761367.69, Val loss 20338822.00\n","Epoch 391, Loss 725734471.00, Val loss 20340254.00\n","==================================================================\n","Saved best model\n","Epoch 392, Loss 725597632.06, Val loss 20319140.00\n","Epoch 393, Loss 725336625.50, Val loss 20324142.00\n","==================================================================\n","Saved best model\n","Epoch 394, Loss 725111002.75, Val loss 20295974.00\n","Epoch 395, Loss 724888697.44, Val loss 20298000.00\n","==================================================================\n","Saved best model\n","Epoch 396, Loss 724682243.94, Val loss 20288672.00\n","==================================================================\n","Saved best model\n","Epoch 397, Loss 724581356.94, Val loss 20282008.00\n","Epoch 398, Loss 724272467.69, Val loss 20283054.00\n","==================================================================\n","Saved best model\n","Epoch 399, Loss 724238368.94, Val loss 20262726.00\n","Epoch 400, Loss 723876949.31, Val loss 20268342.00\n","==================================================================\n","Saved best model\n","Epoch 401, Loss 723838271.94, Val loss 20243900.00\n","Epoch 402, Loss 723532716.31, Val loss 20272574.00\n","Epoch 403, Loss 723570478.00, Val loss 20244476.00\n","Epoch 404, Loss 723286631.75, Val loss 20256928.00\n","==================================================================\n","Saved best model\n","Epoch 405, Loss 723215237.62, Val loss 20226250.00\n","Epoch 406, Loss 722976796.88, Val loss 20241300.00\n","==================================================================\n","Saved best model\n","Epoch 407, Loss 722867799.38, Val loss 20208398.00\n","Epoch 408, Loss 722672869.19, Val loss 20225926.00\n","==================================================================\n","Saved best model\n","Epoch 409, Loss 722521690.44, Val loss 20190896.00\n","Epoch 410, Loss 722382676.69, Val loss 20211690.00\n","==================================================================\n","Saved best model\n","Epoch 411, Loss 722161948.00, Val loss 20174530.00\n","Epoch 412, Loss 722301129.94, Val loss 20200754.00\n","==================================================================\n","Saved best model\n","Epoch 413, Loss 721778672.44, Val loss 20155822.00\n","Epoch 414, Loss 722028759.31, Val loss 20189890.00\n","==================================================================\n","Saved best model\n","Epoch 415, Loss 721478202.00, Val loss 20139356.00\n","Epoch 416, Loss 721537330.31, Val loss 20175712.00\n","==================================================================\n","Saved best model\n","Epoch 417, Loss 721142452.94, Val loss 20130764.00\n","Epoch 418, Loss 721165971.56, Val loss 20159716.00\n","Epoch 419, Loss 720767651.44, Val loss 20130840.00\n","Epoch 420, Loss 720826479.50, Val loss 20139244.00\n","==================================================================\n","Saved best model\n","Epoch 421, Loss 720341079.19, Val loss 20126252.00\n","Epoch 422, Loss 720856228.25, Val loss 20126948.00\n","==================================================================\n","Saved best model\n","Epoch 423, Loss 719587061.69, Val loss 20122470.00\n","==================================================================\n","Saved best model\n","Epoch 424, Loss 719876501.56, Val loss 20113966.00\n","==================================================================\n","Saved best model\n","Epoch 425, Loss 718905919.56, Val loss 20107860.00\n","==================================================================\n","Saved best model\n","Epoch 426, Loss 719259597.69, Val loss 20096030.00\n","==================================================================\n","Saved best model\n","Epoch 427, Loss 718466445.88, Val loss 20093242.00\n","==================================================================\n","Saved best model\n","Epoch 428, Loss 718769067.62, Val loss 20085104.00\n","==================================================================\n","Saved best model\n","Epoch 429, Loss 718199184.38, Val loss 20080262.00\n","==================================================================\n","Saved best model\n","Epoch 430, Loss 718663263.81, Val loss 20068368.00\n","==================================================================\n","Saved best model\n","Epoch 431, Loss 717970653.88, Val loss 20066954.00\n","Epoch 432, Loss 724998097.31, Val loss 20293238.00\n","Epoch 433, Loss 734144250.16, Val loss 20972264.00\n","Epoch 434, Loss 731347326.16, Val loss 20958096.00\n","Epoch 435, Loss 730689564.69, Val loss 20949752.00\n","Epoch 436, Loss 730860294.91, Val loss 20927042.00\n","Epoch 437, Loss 729984605.91, Val loss 20919400.00\n","Epoch 438, Loss 730312078.12, Val loss 20872828.00\n","==================================================================\n","Saved best model\n","Epoch 439, Loss 732626028.66, Val loss 20024136.00\n","==================================================================\n","Saved best model\n","Epoch 440, Loss 717725982.50, Val loss 20004890.00\n","Epoch 441, Loss 716855712.56, Val loss 20007474.00\n","==================================================================\n","Saved best model\n","Epoch 442, Loss 717280088.88, Val loss 20000724.00\n","Epoch 443, Loss 716243228.06, Val loss 20015184.00\n","Epoch 444, Loss 716546805.25, Val loss 20004754.00\n","Epoch 445, Loss 715688781.00, Val loss 20006532.00\n","==================================================================\n","Saved best model\n","Epoch 446, Loss 716128152.00, Val loss 19992844.00\n","==================================================================\n","Saved best model\n","Epoch 447, Loss 715307863.50, Val loss 19991882.00\n","==================================================================\n","Saved best model\n","Epoch 448, Loss 715711311.38, Val loss 19977036.00\n","==================================================================\n","Saved best model\n","Epoch 449, Loss 714913368.44, Val loss 19974494.00\n","==================================================================\n","Saved best model\n","Epoch 450, Loss 715286433.88, Val loss 19959872.00\n","==================================================================\n","Saved best model\n","Epoch 451, Loss 714507135.81, Val loss 19956660.00\n","==================================================================\n","Saved best model\n","Epoch 452, Loss 714860450.25, Val loss 19942766.00\n","==================================================================\n","Saved best model\n","Epoch 453, Loss 714090868.19, Val loss 19937948.00\n","==================================================================\n","Saved best model\n","Epoch 454, Loss 714394239.44, Val loss 19923940.00\n","==================================================================\n","Saved best model\n","Epoch 455, Loss 713663241.94, Val loss 19915806.00\n","==================================================================\n","Saved best model\n","Epoch 456, Loss 714059717.81, Val loss 19905486.00\n","==================================================================\n","Saved best model\n","Epoch 457, Loss 713395962.44, Val loss 19899342.00\n","==================================================================\n","Saved best model\n","Epoch 458, Loss 713703378.31, Val loss 19890372.00\n","==================================================================\n","Saved best model\n","Epoch 459, Loss 713099292.81, Val loss 19884008.00\n","==================================================================\n","Saved best model\n","Epoch 460, Loss 713365875.81, Val loss 19875556.00\n","==================================================================\n","Saved best model\n","Epoch 461, Loss 712818564.12, Val loss 19869734.00\n","==================================================================\n","Saved best model\n","Epoch 462, Loss 713041406.12, Val loss 19861064.00\n","==================================================================\n","Saved best model\n","Epoch 463, Loss 712522555.94, Val loss 19855578.00\n","==================================================================\n","Saved best model\n","Epoch 464, Loss 712725217.75, Val loss 19846640.00\n","==================================================================\n","Saved best model\n","Epoch 465, Loss 712215491.19, Val loss 19841306.00\n","==================================================================\n","Saved best model\n","Epoch 466, Loss 712414065.88, Val loss 19832328.00\n","==================================================================\n","Saved best model\n","Epoch 467, Loss 711900048.69, Val loss 19827080.00\n","==================================================================\n","Saved best model\n","Epoch 468, Loss 712100310.00, Val loss 19818208.00\n","==================================================================\n","Saved best model\n","Epoch 469, Loss 711587440.38, Val loss 19813278.00\n","==================================================================\n","Saved best model\n","Epoch 470, Loss 711814889.75, Val loss 19804392.00\n","==================================================================\n","Saved best model\n","Epoch 471, Loss 711282635.00, Val loss 19799766.00\n","==================================================================\n","Saved best model\n","Epoch 472, Loss 711527036.25, Val loss 19791020.00\n","==================================================================\n","Saved best model\n","Epoch 473, Loss 710988732.38, Val loss 19786626.00\n","==================================================================\n","Saved best model\n","Epoch 474, Loss 711235245.25, Val loss 19778258.00\n","==================================================================\n","Saved best model\n","Epoch 475, Loss 710701753.31, Val loss 19774146.00\n","==================================================================\n","Saved best model\n","Epoch 476, Loss 710941389.25, Val loss 19766088.00\n","==================================================================\n","Saved best model\n","Epoch 477, Loss 710419448.44, Val loss 19761320.00\n","==================================================================\n","Saved best model\n","Epoch 478, Loss 710658742.44, Val loss 19752808.00\n","==================================================================\n","Saved best model\n","Epoch 479, Loss 710139878.88, Val loss 19748574.00\n","==================================================================\n","Saved best model\n","Epoch 480, Loss 710367359.00, Val loss 19741282.00\n","==================================================================\n","Saved best model\n","Epoch 481, Loss 709863241.56, Val loss 19737296.00\n","==================================================================\n","Saved best model\n","Epoch 482, Loss 710090114.69, Val loss 19730050.00\n","==================================================================\n","Saved best model\n","Epoch 483, Loss 709609290.12, Val loss 19725854.00\n","==================================================================\n","Saved best model\n","Epoch 484, Loss 709839247.69, Val loss 19718452.00\n","==================================================================\n","Saved best model\n","Epoch 485, Loss 709373075.88, Val loss 19713948.00\n","==================================================================\n","Saved best model\n","Epoch 486, Loss 709595529.38, Val loss 19706520.00\n","==================================================================\n","Saved best model\n","Epoch 487, Loss 709150445.94, Val loss 19701870.00\n","==================================================================\n","Saved best model\n","Epoch 488, Loss 709369061.00, Val loss 19694444.00\n","==================================================================\n","Saved best model\n","Epoch 489, Loss 708935579.88, Val loss 19689658.00\n","==================================================================\n","Saved best model\n","Epoch 490, Loss 709141459.19, Val loss 19682314.00\n","==================================================================\n","Saved best model\n","Epoch 491, Loss 708727144.00, Val loss 19677500.00\n","==================================================================\n","Saved best model\n","Epoch 492, Loss 708927667.88, Val loss 19670184.00\n","==================================================================\n","Saved best model\n","Epoch 493, Loss 708521323.00, Val loss 19665266.00\n","==================================================================\n","Saved best model\n","Epoch 494, Loss 708702127.94, Val loss 19658090.00\n","==================================================================\n","Saved best model\n","Epoch 495, Loss 708318624.81, Val loss 19653282.00\n","==================================================================\n","Saved best model\n","Epoch 496, Loss 708503144.25, Val loss 19646082.00\n","==================================================================\n","Saved best model\n","Epoch 497, Loss 708116061.62, Val loss 19641104.00\n","==================================================================\n","Saved best model\n","Epoch 498, Loss 708263877.50, Val loss 19634152.00\n","==================================================================\n","Saved best model\n","Epoch 499, Loss 707908434.62, Val loss 19629592.00\n","==================================================================\n","Saved best model\n","Epoch 500, Loss 708094377.75, Val loss 19622372.00\n","==================================================================\n","Saved best model\n","Epoch 501, Loss 707712691.56, Val loss 19617404.00\n","==================================================================\n","Saved best model\n","Epoch 502, Loss 707826120.25, Val loss 19610600.00\n","==================================================================\n","Saved best model\n","Epoch 503, Loss 707516520.62, Val loss 19605832.00\n","==================================================================\n","Saved best model\n","Epoch 504, Loss 707643759.94, Val loss 19599036.00\n","==================================================================\n","Saved best model\n","Epoch 505, Loss 707312117.44, Val loss 19594532.00\n","==================================================================\n","Saved best model\n","Epoch 506, Loss 707468818.06, Val loss 19587614.00\n","==================================================================\n","Saved best model\n","Epoch 507, Loss 707128431.75, Val loss 19582964.00\n","==================================================================\n","Saved best model\n","Epoch 508, Loss 707232996.00, Val loss 19576278.00\n","==================================================================\n","Saved best model\n","Epoch 509, Loss 706923671.81, Val loss 19571888.00\n","==================================================================\n","Saved best model\n","Epoch 510, Loss 707062518.00, Val loss 19565132.00\n","==================================================================\n","Saved best model\n","Epoch 511, Loss 706744013.31, Val loss 19560784.00\n","==================================================================\n","Saved best model\n","Epoch 512, Loss 706853428.38, Val loss 19554144.00\n","==================================================================\n","Saved best model\n","Epoch 513, Loss 706549278.38, Val loss 19549972.00\n","==================================================================\n","Saved best model\n","Epoch 514, Loss 706669681.81, Val loss 19543372.00\n","==================================================================\n","Saved best model\n","Epoch 515, Loss 706367394.69, Val loss 19539352.00\n","==================================================================\n","Saved best model\n","Epoch 516, Loss 706480961.50, Val loss 19532826.00\n","==================================================================\n","Saved best model\n","Epoch 517, Loss 706186313.69, Val loss 19528990.00\n","==================================================================\n","Saved best model\n","Epoch 518, Loss 706296702.81, Val loss 19522546.00\n","==================================================================\n","Saved best model\n","Epoch 519, Loss 706009217.12, Val loss 19518926.00\n","==================================================================\n","Saved best model\n","Epoch 520, Loss 706114726.75, Val loss 19512572.00\n","==================================================================\n","Saved best model\n","Epoch 521, Loss 705834582.69, Val loss 19509206.00\n","==================================================================\n","Saved best model\n","Epoch 522, Loss 705933916.38, Val loss 19502974.00\n","==================================================================\n","Saved best model\n","Epoch 523, Loss 705660905.50, Val loss 19499934.00\n","==================================================================\n","Saved best model\n","Epoch 524, Loss 705752595.75, Val loss 19493888.00\n","==================================================================\n","Saved best model\n","Epoch 525, Loss 705481065.44, Val loss 19492144.00\n","Epoch 526, Loss 718427954.59, Val loss 20215426.00\n","Epoch 527, Loss 712965725.62, Val loss 20285306.00\n","Epoch 528, Loss 712117406.12, Val loss 20305606.00\n","Epoch 529, Loss 711436663.50, Val loss 20301590.00\n","Epoch 530, Loss 711128233.31, Val loss 20289838.00\n","Epoch 531, Loss 710932864.88, Val loss 20275582.00\n","Epoch 532, Loss 710723248.88, Val loss 20263228.00\n","Epoch 533, Loss 710553583.38, Val loss 20248300.00\n","Epoch 534, Loss 710357700.75, Val loss 20234970.00\n","Epoch 535, Loss 710194839.06, Val loss 20219488.00\n","Epoch 536, Loss 710073921.94, Val loss 20205306.00\n","Epoch 537, Loss 709822556.81, Val loss 20187540.00\n","Epoch 538, Loss 709631950.94, Val loss 20172900.00\n","Epoch 539, Loss 709458811.94, Val loss 20158140.00\n","Epoch 540, Loss 709276431.12, Val loss 20145220.00\n","Epoch 541, Loss 709100137.50, Val loss 20130864.00\n","Epoch 542, Loss 708930060.38, Val loss 20118600.00\n","Epoch 543, Loss 708748080.12, Val loss 20104704.00\n","Epoch 544, Loss 708584150.50, Val loss 20092886.00\n","Epoch 545, Loss 708399595.12, Val loss 20079434.00\n","Epoch 546, Loss 708243316.50, Val loss 20067968.00\n","Epoch 547, Loss 708072077.56, Val loss 20055326.00\n","Epoch 548, Loss 707950280.38, Val loss 20044724.00\n","Epoch 549, Loss 707841803.94, Val loss 20033566.00\n","Epoch 550, Loss 707683535.12, Val loss 20021910.00\n","Epoch 551, Loss 707454872.81, Val loss 20009434.00\n","Epoch 552, Loss 707198834.62, Val loss 19997200.00\n","Epoch 553, Loss 706983696.50, Val loss 19985686.00\n","Epoch 554, Loss 706794791.88, Val loss 19975444.00\n","Epoch 555, Loss 706616934.94, Val loss 19965754.00\n","Epoch 556, Loss 706435020.94, Val loss 19956644.00\n","Epoch 557, Loss 706254244.38, Val loss 19947972.00\n","Epoch 558, Loss 706707224.81, Val loss 19939026.00\n","Epoch 559, Loss 705878988.38, Val loss 19931880.00\n","Epoch 560, Loss 705708071.19, Val loss 19923398.00\n","Epoch 561, Loss 705524266.75, Val loss 19915926.00\n","Epoch 562, Loss 705340342.38, Val loss 19907992.00\n","Epoch 563, Loss 705151591.50, Val loss 19901036.00\n","Epoch 564, Loss 704955544.50, Val loss 19893086.00\n","Epoch 565, Loss 704751952.94, Val loss 19884050.00\n","Epoch 566, Loss 704517745.31, Val loss 19871156.00\n","Epoch 567, Loss 704194717.06, Val loss 19854812.00\n","Epoch 568, Loss 703889099.19, Val loss 19842522.00\n","Epoch 569, Loss 703836250.25, Val loss 19839844.00\n","Epoch 570, Loss 703402727.38, Val loss 19827442.00\n","Epoch 571, Loss 703316563.50, Val loss 19822902.00\n","Epoch 572, Loss 703048189.44, Val loss 19813380.00\n","Epoch 573, Loss 702898992.06, Val loss 19807246.00\n","Epoch 574, Loss 702689274.19, Val loss 19799536.00\n","Epoch 575, Loss 702521508.75, Val loss 19792914.00\n","Epoch 576, Loss 702377982.69, Val loss 19786016.00\n","Epoch 577, Loss 702211395.62, Val loss 19779554.00\n","Epoch 578, Loss 702059326.75, Val loss 19773978.00\n","Epoch 579, Loss 701944694.62, Val loss 19769754.00\n","Epoch 580, Loss 702003260.62, Val loss 19765926.00\n","Epoch 581, Loss 702375537.88, Val loss 19760104.00\n","Epoch 582, Loss 702169911.12, Val loss 19752166.00\n","Epoch 583, Loss 701805389.38, Val loss 19743790.00\n","Epoch 584, Loss 701517674.69, Val loss 19735006.00\n","Epoch 585, Loss 701261319.50, Val loss 19726296.00\n","Epoch 586, Loss 701061396.25, Val loss 19717656.00\n","Epoch 587, Loss 700883563.81, Val loss 19709146.00\n","Epoch 588, Loss 700712716.81, Val loss 19700732.00\n","Epoch 589, Loss 700542220.38, Val loss 19692388.00\n","Epoch 590, Loss 700371222.62, Val loss 19684096.00\n","Epoch 591, Loss 700202288.81, Val loss 19675824.00\n","Epoch 592, Loss 700036109.06, Val loss 19667566.00\n","Epoch 593, Loss 699875662.31, Val loss 19659298.00\n","Epoch 594, Loss 699721858.62, Val loss 19651010.00\n","Epoch 595, Loss 699576640.06, Val loss 19642688.00\n","Epoch 596, Loss 699439924.12, Val loss 19634348.00\n","Epoch 597, Loss 699311526.56, Val loss 19626006.00\n","Epoch 598, Loss 699189799.25, Val loss 19617700.00\n","Epoch 599, Loss 699073200.12, Val loss 19609454.00\n","Epoch 600, Loss 698959840.25, Val loss 19601294.00\n","Epoch 601, Loss 698848511.75, Val loss 19593238.00\n","Epoch 602, Loss 698738121.94, Val loss 19585290.00\n","Epoch 603, Loss 698628324.75, Val loss 19577444.00\n","Epoch 604, Loss 698518741.12, Val loss 19569698.00\n","Epoch 605, Loss 698409444.06, Val loss 19562046.00\n","Epoch 606, Loss 698300376.81, Val loss 19554474.00\n","Epoch 607, Loss 698191664.19, Val loss 19546982.00\n","Epoch 608, Loss 698083379.62, Val loss 19539556.00\n","Epoch 609, Loss 697975675.25, Val loss 19532196.00\n","Epoch 610, Loss 697868566.69, Val loss 19524894.00\n","Epoch 611, Loss 697762201.94, Val loss 19517654.00\n","Epoch 612, Loss 697656600.69, Val loss 19510464.00\n","Epoch 613, Loss 697551837.12, Val loss 19503326.00\n","Epoch 614, Loss 697447932.25, Val loss 19496242.00\n","==================================================================\n","Saved best model\n","Epoch 615, Loss 697344939.88, Val loss 19489206.00\n","==================================================================\n","Saved best model\n","Epoch 616, Loss 697242775.81, Val loss 19482220.00\n","==================================================================\n","Saved best model\n","Epoch 617, Loss 697141528.38, Val loss 19475282.00\n","==================================================================\n","Saved best model\n","Epoch 618, Loss 697041158.00, Val loss 19468394.00\n","==================================================================\n","Saved best model\n","Epoch 619, Loss 696941659.00, Val loss 19461554.00\n","==================================================================\n","Saved best model\n","Epoch 620, Loss 696842951.31, Val loss 19454762.00\n","==================================================================\n","Saved best model\n","Epoch 621, Loss 696745078.00, Val loss 19448014.00\n","==================================================================\n","Saved best model\n","Epoch 622, Loss 696647992.31, Val loss 19441316.00\n","==================================================================\n","Saved best model\n","Epoch 623, Loss 696551663.88, Val loss 19434662.00\n","==================================================================\n","Saved best model\n","Epoch 624, Loss 696456042.94, Val loss 19428050.00\n","==================================================================\n","Saved best model\n","Epoch 625, Loss 696361175.62, Val loss 19421482.00\n","==================================================================\n","Saved best model\n","Epoch 626, Loss 696267014.88, Val loss 19414954.00\n","==================================================================\n","Saved best model\n","Epoch 627, Loss 696173546.44, Val loss 19408464.00\n","==================================================================\n","Saved best model\n","Epoch 628, Loss 696080751.69, Val loss 19402010.00\n","==================================================================\n","Saved best model\n","Epoch 629, Loss 695988684.31, Val loss 19395594.00\n","==================================================================\n","Saved best model\n","Epoch 630, Loss 695897269.75, Val loss 19389206.00\n","==================================================================\n","Saved best model\n","Epoch 631, Loss 695806622.44, Val loss 19382848.00\n","==================================================================\n","Saved best model\n","Epoch 632, Loss 695716663.12, Val loss 19376518.00\n","==================================================================\n","Saved best model\n","Epoch 633, Loss 695627460.56, Val loss 19370214.00\n","==================================================================\n","Saved best model\n","Epoch 634, Loss 695539002.81, Val loss 19363928.00\n","==================================================================\n","Saved best model\n","Epoch 635, Loss 695451336.50, Val loss 19357658.00\n","==================================================================\n","Saved best model\n","Epoch 636, Loss 695364508.94, Val loss 19351398.00\n","==================================================================\n","Saved best model\n","Epoch 637, Loss 695278554.06, Val loss 19345140.00\n","==================================================================\n","Saved best model\n","Epoch 638, Loss 695193533.88, Val loss 19338878.00\n","==================================================================\n","Saved best model\n","Epoch 639, Loss 695109630.75, Val loss 19332598.00\n","==================================================================\n","Saved best model\n","Epoch 640, Loss 695027074.12, Val loss 19326288.00\n","==================================================================\n","Saved best model\n","Epoch 641, Loss 694946164.38, Val loss 19319924.00\n","==================================================================\n","Saved best model\n","Epoch 642, Loss 694867472.94, Val loss 19313482.00\n","==================================================================\n","Saved best model\n","Epoch 643, Loss 694792091.19, Val loss 19306922.00\n","==================================================================\n","Saved best model\n","Epoch 644, Loss 694721815.25, Val loss 19300204.00\n","==================================================================\n","Saved best model\n","Epoch 645, Loss 694660272.88, Val loss 19293302.00\n","==================================================================\n","Saved best model\n","Epoch 646, Loss 694613464.88, Val loss 19286342.00\n","==================================================================\n","Saved best model\n","Epoch 647, Loss 694580966.50, Val loss 19279942.00\n","==================================================================\n","Saved best model\n","Epoch 648, Loss 694519379.56, Val loss 19275226.00\n","==================================================================\n","Saved best model\n","Epoch 649, Loss 694383454.19, Val loss 19272314.00\n","==================================================================\n","Saved best model\n","Epoch 650, Loss 694204167.12, Val loss 19270648.00\n","Epoch 651, Loss 694034930.25, Val loss 19271140.00\n","==================================================================\n","Saved best model\n","Epoch 652, Loss 694306045.12, Val loss 19269226.00\n","==================================================================\n","Saved best model\n","Epoch 653, Loss 694334146.75, Val loss 19264962.00\n","==================================================================\n","Saved best model\n","Epoch 654, Loss 694095748.81, Val loss 19260314.00\n","==================================================================\n","Saved best model\n","Epoch 655, Loss 693823211.06, Val loss 19255578.00\n","==================================================================\n","Saved best model\n","Epoch 656, Loss 693607026.94, Val loss 19250812.00\n","==================================================================\n","Saved best model\n","Epoch 657, Loss 693432449.44, Val loss 19245992.00\n","==================================================================\n","Saved best model\n","Epoch 658, Loss 693258653.56, Val loss 19241146.00\n","==================================================================\n","Saved best model\n","Epoch 659, Loss 693095245.56, Val loss 19236054.00\n","==================================================================\n","Saved best model\n","Epoch 660, Loss 692954912.44, Val loss 19230464.00\n","==================================================================\n","Saved best model\n","Epoch 661, Loss 692847271.44, Val loss 19224148.00\n","==================================================================\n","Saved best model\n","Epoch 662, Loss 692773437.56, Val loss 19216850.00\n","==================================================================\n","Saved best model\n","Epoch 663, Loss 692740877.25, Val loss 19208972.00\n","==================================================================\n","Saved best model\n","Epoch 664, Loss 692731988.94, Val loss 19201738.00\n","==================================================================\n","Saved best model\n","Epoch 665, Loss 692708347.12, Val loss 19195392.00\n","==================================================================\n","Saved best model\n","Epoch 666, Loss 692663664.19, Val loss 19189726.00\n","==================================================================\n","Saved best model\n","Epoch 667, Loss 692603991.81, Val loss 19184586.00\n","==================================================================\n","Saved best model\n","Epoch 668, Loss 692531672.50, Val loss 19179882.00\n","==================================================================\n","Saved best model\n","Epoch 669, Loss 692444222.75, Val loss 19175572.00\n","==================================================================\n","Saved best model\n","Epoch 670, Loss 692336791.38, Val loss 19171714.00\n","==================================================================\n","Saved best model\n","Epoch 671, Loss 692205080.38, Val loss 19168660.00\n","==================================================================\n","Saved best model\n","Epoch 672, Loss 692051209.44, Val loss 19167268.00\n","==================================================================\n","Saved best model\n","Epoch 673, Loss 691975643.88, Val loss 19166444.00\n","==================================================================\n","Saved best model\n","Epoch 674, Loss 692185982.06, Val loss 19163638.00\n","==================================================================\n","Saved best model\n","Epoch 675, Loss 692052286.19, Val loss 19160114.00\n","==================================================================\n","Saved best model\n","Epoch 676, Loss 691843508.19, Val loss 19156382.00\n","==================================================================\n","Saved best model\n","Epoch 677, Loss 691631538.56, Val loss 19152392.00\n","==================================================================\n","Saved best model\n","Epoch 678, Loss 691446208.81, Val loss 19148310.00\n","==================================================================\n","Saved best model\n","Epoch 679, Loss 691273589.19, Val loss 19143822.00\n","==================================================================\n","Saved best model\n","Epoch 680, Loss 691343034.69, Val loss 19138948.00\n","==================================================================\n","Saved best model\n","Epoch 681, Loss 691014964.50, Val loss 19133034.00\n","==================================================================\n","Saved best model\n","Epoch 682, Loss 691147371.19, Val loss 19123726.00\n","==================================================================\n","Saved best model\n","Epoch 683, Loss 691036272.94, Val loss 19119106.00\n","==================================================================\n","Saved best model\n","Epoch 684, Loss 690312063.81, Val loss 19112650.00\n","==================================================================\n","Saved best model\n","Epoch 685, Loss 691247696.12, Val loss 19109006.00\n","==================================================================\n","Saved best model\n","Epoch 686, Loss 691434367.31, Val loss 19101076.00\n","==================================================================\n","Saved best model\n","Epoch 687, Loss 691051888.38, Val loss 19099378.00\n","==================================================================\n","Saved best model\n","Epoch 688, Loss 691028577.94, Val loss 19095902.00\n","==================================================================\n","Saved best model\n","Epoch 689, Loss 690809473.62, Val loss 19092660.00\n","==================================================================\n","Saved best model\n","Epoch 690, Loss 690782016.62, Val loss 19091104.00\n","==================================================================\n","Saved best model\n","Epoch 691, Loss 690502558.06, Val loss 19089878.00\n","==================================================================\n","Saved best model\n","Epoch 692, Loss 690750665.94, Val loss 19087866.00\n","==================================================================\n","Saved best model\n","Epoch 693, Loss 690519531.06, Val loss 19084248.00\n","==================================================================\n","Saved best model\n","Epoch 694, Loss 690462941.75, Val loss 19080588.00\n","==================================================================\n","Saved best model\n","Epoch 695, Loss 690255219.88, Val loss 19076372.00\n","==================================================================\n","Saved best model\n","Epoch 696, Loss 690054227.69, Val loss 19071982.00\n","==================================================================\n","Saved best model\n","Epoch 697, Loss 689865211.50, Val loss 19067244.00\n","==================================================================\n","Saved best model\n","Epoch 698, Loss 689739074.19, Val loss 19062090.00\n","==================================================================\n","Saved best model\n","Epoch 699, Loss 689706898.88, Val loss 19056040.00\n","==================================================================\n","Saved best model\n","Epoch 700, Loss 689758872.31, Val loss 19049714.00\n","==================================================================\n","Saved best model\n","Epoch 701, Loss 689801016.75, Val loss 19044080.00\n","==================================================================\n","Saved best model\n","Epoch 702, Loss 689755538.38, Val loss 19039164.00\n","==================================================================\n","Saved best model\n","Epoch 703, Loss 689657936.81, Val loss 19034762.00\n","==================================================================\n","Saved best model\n","Epoch 704, Loss 689566383.19, Val loss 19030794.00\n","==================================================================\n","Saved best model\n","Epoch 705, Loss 689496273.25, Val loss 19027230.00\n","==================================================================\n","Saved best model\n","Epoch 706, Loss 689419762.00, Val loss 19024196.00\n","==================================================================\n","Saved best model\n","Epoch 707, Loss 689325160.62, Val loss 19022162.00\n","==================================================================\n","Saved best model\n","Epoch 708, Loss 689226405.94, Val loss 19021458.00\n","==================================================================\n","Saved best model\n","Epoch 709, Loss 689227763.44, Val loss 19020570.00\n","==================================================================\n","Saved best model\n","Epoch 710, Loss 689239310.31, Val loss 19018476.00\n","==================================================================\n","Saved best model\n","Epoch 711, Loss 689135928.94, Val loss 19015892.00\n","==================================================================\n","Saved best model\n","Epoch 712, Loss 689018052.00, Val loss 19012990.00\n","==================================================================\n","Saved best model\n","Epoch 713, Loss 688900661.31, Val loss 19009606.00\n","==================================================================\n","Saved best model\n","Epoch 714, Loss 688786694.56, Val loss 19005480.00\n","==================================================================\n","Saved best model\n","Epoch 715, Loss 688672134.94, Val loss 19000476.00\n","==================================================================\n","Saved best model\n","Epoch 716, Loss 688581928.38, Val loss 18994592.00\n","==================================================================\n","Saved best model\n","Epoch 717, Loss 688569214.88, Val loss 18988368.00\n","==================================================================\n","Saved best model\n","Epoch 718, Loss 688603207.62, Val loss 18982992.00\n","==================================================================\n","Saved best model\n","Epoch 719, Loss 688618943.38, Val loss 18978622.00\n","==================================================================\n","Saved best model\n","Epoch 720, Loss 688609690.50, Val loss 18974990.00\n","==================================================================\n","Saved best model\n","Epoch 721, Loss 688586733.69, Val loss 18971938.00\n","==================================================================\n","Saved best model\n","Epoch 722, Loss 688562356.25, Val loss 18969534.00\n","==================================================================\n","Saved best model\n","Epoch 723, Loss 688557517.06, Val loss 18967854.00\n","==================================================================\n","Saved best model\n","Epoch 724, Loss 688514303.88, Val loss 18966540.00\n","==================================================================\n","Saved best model\n","Epoch 725, Loss 688358055.88, Val loss 18965472.00\n","==================================================================\n","Saved best model\n","Epoch 726, Loss 688238621.81, Val loss 18964126.00\n","==================================================================\n","Saved best model\n","Epoch 727, Loss 688099244.19, Val loss 18961642.00\n","==================================================================\n","Saved best model\n","Epoch 728, Loss 687989713.88, Val loss 18958598.00\n","==================================================================\n","Saved best model\n","Epoch 729, Loss 687880586.25, Val loss 18954946.00\n","==================================================================\n","Saved best model\n","Epoch 730, Loss 687802120.44, Val loss 18950744.00\n","==================================================================\n","Saved best model\n","Epoch 731, Loss 687752480.31, Val loss 18945826.00\n","==================================================================\n","Saved best model\n","Epoch 732, Loss 687755244.56, Val loss 18940324.00\n","==================================================================\n","Saved best model\n","Epoch 733, Loss 687793285.94, Val loss 18935200.00\n","==================================================================\n","Saved best model\n","Epoch 734, Loss 687825516.81, Val loss 18931058.00\n","==================================================================\n","Saved best model\n","Epoch 735, Loss 687834372.12, Val loss 18927848.00\n","==================================================================\n","Saved best model\n","Epoch 736, Loss 687835639.00, Val loss 18925464.00\n","==================================================================\n","Saved best model\n","Epoch 737, Loss 687847323.19, Val loss 18923448.00\n","==================================================================\n","Saved best model\n","Epoch 738, Loss 687787832.81, Val loss 18921258.00\n","==================================================================\n","Saved best model\n","Epoch 739, Loss 687587100.50, Val loss 18919734.00\n","==================================================================\n","Saved best model\n","Epoch 740, Loss 687381304.00, Val loss 18919118.00\n","==================================================================\n","Saved best model\n","Epoch 741, Loss 687273511.69, Val loss 18918046.00\n","==================================================================\n","Saved best model\n","Epoch 742, Loss 687157909.00, Val loss 18916154.00\n","==================================================================\n","Saved best model\n","Epoch 743, Loss 687040470.69, Val loss 18913002.00\n","==================================================================\n","Saved best model\n","Epoch 744, Loss 686962774.00, Val loss 18908832.00\n","==================================================================\n","Saved best model\n","Epoch 745, Loss 686887817.06, Val loss 18903888.00\n","==================================================================\n","Saved best model\n","Epoch 746, Loss 686854707.88, Val loss 18899500.00\n","==================================================================\n","Saved best model\n","Epoch 747, Loss 686784581.69, Val loss 18897164.00\n","==================================================================\n","Saved best model\n","Epoch 748, Loss 686698635.69, Val loss 18895268.00\n","==================================================================\n","Saved best model\n","Epoch 749, Loss 686602108.62, Val loss 18892902.00\n","==================================================================\n","Saved best model\n","Epoch 750, Loss 686485230.94, Val loss 18890180.00\n","==================================================================\n","Saved best model\n","Epoch 751, Loss 686376984.69, Val loss 18890068.00\n","==================================================================\n","Saved best model\n","Epoch 752, Loss 686286660.81, Val loss 18889540.00\n","==================================================================\n","Saved best model\n","Epoch 753, Loss 686264394.38, Val loss 18886836.00\n","==================================================================\n","Saved best model\n","Epoch 754, Loss 686231311.12, Val loss 18882534.00\n","==================================================================\n","Saved best model\n","Epoch 755, Loss 686110764.19, Val loss 18878968.00\n","==================================================================\n","Saved best model\n","Epoch 756, Loss 686003613.69, Val loss 18874812.00\n","==================================================================\n","Saved best model\n","Epoch 757, Loss 685960015.19, Val loss 18870106.00\n","==================================================================\n","Saved best model\n","Epoch 758, Loss 685904355.38, Val loss 18864980.00\n","==================================================================\n","Saved best model\n","Epoch 759, Loss 685899634.75, Val loss 18859822.00\n","==================================================================\n","Saved best model\n","Epoch 760, Loss 685907076.75, Val loss 18855228.00\n","==================================================================\n","Saved best model\n","Epoch 761, Loss 685899096.81, Val loss 18851382.00\n","==================================================================\n","Saved best model\n","Epoch 762, Loss 685871554.06, Val loss 18848188.00\n","==================================================================\n","Saved best model\n","Epoch 763, Loss 685830534.19, Val loss 18845520.00\n","==================================================================\n","Saved best model\n","Epoch 764, Loss 685775385.31, Val loss 18843216.00\n","==================================================================\n","Saved best model\n","Epoch 765, Loss 685679963.81, Val loss 18841044.00\n","Epoch 766, Loss 684577229.88, Val loss 18841096.00\n","==================================================================\n","Saved best model\n","Epoch 767, Loss 685359533.25, Val loss 18835872.00\n","==================================================================\n","Saved best model\n","Epoch 768, Loss 685373134.69, Val loss 18834314.00\n","==================================================================\n","Saved best model\n","Epoch 769, Loss 685267996.94, Val loss 18832054.00\n","==================================================================\n","Saved best model\n","Epoch 770, Loss 685226254.06, Val loss 18829326.00\n","==================================================================\n","Saved best model\n","Epoch 771, Loss 685085684.81, Val loss 18826276.00\n","==================================================================\n","Saved best model\n","Epoch 772, Loss 685029668.00, Val loss 18822696.00\n","==================================================================\n","Saved best model\n","Epoch 773, Loss 684989053.69, Val loss 18819060.00\n","==================================================================\n","Saved best model\n","Epoch 774, Loss 684946950.88, Val loss 18815654.00\n","==================================================================\n","Saved best model\n","Epoch 775, Loss 684916287.19, Val loss 18812660.00\n","==================================================================\n","Saved best model\n","Epoch 776, Loss 684836699.12, Val loss 18810246.00\n","==================================================================\n","Saved best model\n","Epoch 777, Loss 684755782.56, Val loss 18808350.00\n","==================================================================\n","Saved best model\n","Epoch 778, Loss 684655811.69, Val loss 18806954.00\n","==================================================================\n","Saved best model\n","Epoch 779, Loss 684540767.62, Val loss 18805850.00\n","==================================================================\n","Saved best model\n","Epoch 780, Loss 684419172.19, Val loss 18804586.00\n","==================================================================\n","Saved best model\n","Epoch 781, Loss 684300402.31, Val loss 18802672.00\n","==================================================================\n","Saved best model\n","Epoch 782, Loss 684192255.25, Val loss 18799976.00\n","==================================================================\n","Saved best model\n","Epoch 783, Loss 684096975.25, Val loss 18796760.00\n","==================================================================\n","Saved best model\n","Epoch 784, Loss 684012244.69, Val loss 18793340.00\n","==================================================================\n","Saved best model\n","Epoch 785, Loss 683934090.25, Val loss 18789994.00\n","==================================================================\n","Saved best model\n","Epoch 786, Loss 683857428.56, Val loss 18786922.00\n","==================================================================\n","Saved best model\n","Epoch 787, Loss 683777117.88, Val loss 18784146.00\n","==================================================================\n","Saved best model\n","Epoch 788, Loss 683691053.12, Val loss 18781634.00\n","==================================================================\n","Saved best model\n","Epoch 789, Loss 683600785.12, Val loss 18779388.00\n","==================================================================\n","Saved best model\n","Epoch 790, Loss 683509878.50, Val loss 18777290.00\n","==================================================================\n","Saved best model\n","Epoch 791, Loss 683423220.81, Val loss 18774946.00\n","==================================================================\n","Saved best model\n","Epoch 792, Loss 683346511.31, Val loss 18771838.00\n","==================================================================\n","Saved best model\n","Epoch 793, Loss 683284062.12, Val loss 18767890.00\n","==================================================================\n","Saved best model\n","Epoch 794, Loss 683231326.25, Val loss 18763748.00\n","==================================================================\n","Saved best model\n","Epoch 795, Loss 683167822.81, Val loss 18760542.00\n","Epoch 796, Loss 683084163.00, Val loss 18761188.00\n","Epoch 797, Loss 682960722.56, Val loss 18763386.00\n","Epoch 798, Loss 682759511.25, Val loss 18761998.00\n","==================================================================\n","Saved best model\n","Epoch 799, Loss 682671362.94, Val loss 18758888.00\n","==================================================================\n","Saved best model\n","Epoch 800, Loss 682635494.31, Val loss 18755492.00\n","==================================================================\n","Saved best model\n","Epoch 801, Loss 682606669.06, Val loss 18752372.00\n","==================================================================\n","Saved best model\n","Epoch 802, Loss 682546995.06, Val loss 18749814.00\n","==================================================================\n","Saved best model\n","Epoch 803, Loss 682480368.50, Val loss 18747556.00\n","==================================================================\n","Saved best model\n","Epoch 804, Loss 682397254.81, Val loss 18745206.00\n","==================================================================\n","Saved best model\n","Epoch 805, Loss 682320549.31, Val loss 18742760.00\n","==================================================================\n","Saved best model\n","Epoch 806, Loss 682245363.38, Val loss 18740120.00\n","==================================================================\n","Saved best model\n","Epoch 807, Loss 682177280.38, Val loss 18737282.00\n","==================================================================\n","Saved best model\n","Epoch 808, Loss 682118740.31, Val loss 18734326.00\n","==================================================================\n","Saved best model\n","Epoch 809, Loss 682083885.56, Val loss 18731480.00\n","==================================================================\n","Saved best model\n","Epoch 810, Loss 682160993.50, Val loss 18728720.00\n","==================================================================\n","Saved best model\n","Epoch 811, Loss 681980343.19, Val loss 18725502.00\n","==================================================================\n","Saved best model\n","Epoch 812, Loss 681919797.56, Val loss 18722434.00\n","==================================================================\n","Saved best model\n","Epoch 813, Loss 681863468.88, Val loss 18719904.00\n","==================================================================\n","Saved best model\n","Epoch 814, Loss 681812862.69, Val loss 18717546.00\n","==================================================================\n","Saved best model\n","Epoch 815, Loss 681742940.69, Val loss 18715312.00\n","==================================================================\n","Saved best model\n","Epoch 816, Loss 681682939.88, Val loss 18713190.00\n","==================================================================\n","Saved best model\n","Epoch 817, Loss 681615980.62, Val loss 18711074.00\n","==================================================================\n","Saved best model\n","Epoch 818, Loss 681554313.50, Val loss 18708950.00\n","==================================================================\n","Saved best model\n","Epoch 819, Loss 681493900.69, Val loss 18706766.00\n","==================================================================\n","Saved best model\n","Epoch 820, Loss 681437466.94, Val loss 18704534.00\n","==================================================================\n","Saved best model\n","Epoch 821, Loss 681385654.81, Val loss 18702324.00\n","==================================================================\n","Saved best model\n","Epoch 822, Loss 681371615.75, Val loss 18700318.00\n","==================================================================\n","Saved best model\n","Epoch 823, Loss 681428777.94, Val loss 18698190.00\n","==================================================================\n","Saved best model\n","Epoch 824, Loss 681230746.69, Val loss 18695462.00\n","==================================================================\n","Saved best model\n","Epoch 825, Loss 681168648.75, Val loss 18692986.00\n","==================================================================\n","Saved best model\n","Epoch 826, Loss 681121947.12, Val loss 18690636.00\n","==================================================================\n","Saved best model\n","Epoch 827, Loss 681068696.25, Val loss 18688388.00\n","==================================================================\n","Saved best model\n","Epoch 828, Loss 681024073.69, Val loss 18686144.00\n","==================================================================\n","Saved best model\n","Epoch 829, Loss 680969170.44, Val loss 18683922.00\n","==================================================================\n","Saved best model\n","Epoch 830, Loss 680921919.88, Val loss 18681718.00\n","==================================================================\n","Saved best model\n","Epoch 831, Loss 680870725.56, Val loss 18679572.00\n","==================================================================\n","Saved best model\n","Epoch 832, Loss 680825193.75, Val loss 18677348.00\n","==================================================================\n","Saved best model\n","Epoch 833, Loss 680773493.56, Val loss 18675314.00\n","==================================================================\n","Saved best model\n","Epoch 834, Loss 680743032.25, Val loss 18672932.00\n","==================================================================\n","Saved best model\n","Epoch 835, Loss 680826054.06, Val loss 18671978.00\n","==================================================================\n","Saved best model\n","Epoch 836, Loss 680780571.56, Val loss 18668998.00\n","==================================================================\n","Saved best model\n","Epoch 837, Loss 680600803.50, Val loss 18666890.00\n","==================================================================\n","Saved best model\n","Epoch 838, Loss 680543133.06, Val loss 18663892.00\n","==================================================================\n","Saved best model\n","Epoch 839, Loss 680520895.00, Val loss 18662634.00\n","==================================================================\n","Saved best model\n","Epoch 840, Loss 680446766.00, Val loss 18659836.00\n","==================================================================\n","Saved best model\n","Epoch 841, Loss 680458209.94, Val loss 18657356.00\n","==================================================================\n","Saved best model\n","Epoch 842, Loss 681691570.19, Val loss 18648848.00\n","Epoch 843, Loss 680888136.88, Val loss 18651700.00\n","Epoch 844, Loss 680335379.81, Val loss 18652298.00\n","Epoch 845, Loss 680170503.06, Val loss 18649520.00\n","==================================================================\n","Saved best model\n","Epoch 846, Loss 680167642.12, Val loss 18645856.00\n","==================================================================\n","Saved best model\n","Epoch 847, Loss 680333499.19, Val loss 18641852.00\n","Epoch 848, Loss 680222681.75, Val loss 18644186.00\n","Epoch 849, Loss 679978692.44, Val loss 18642734.00\n","==================================================================\n","Saved best model\n","Epoch 850, Loss 680076868.94, Val loss 18641086.00\n","==================================================================\n","Saved best model\n","Epoch 851, Loss 680070924.69, Val loss 18637720.00\n","==================================================================\n","Saved best model\n","Epoch 852, Loss 680335692.12, Val loss 18631952.00\n","Epoch 853, Loss 680064528.81, Val loss 18635038.00\n","==================================================================\n","Saved best model\n","Epoch 854, Loss 679770301.75, Val loss 18631416.00\n","==================================================================\n","Saved best model\n","Epoch 855, Loss 679968961.56, Val loss 18627456.00\n","Epoch 856, Loss 679873039.44, Val loss 18627912.00\n","==================================================================\n","Saved best model\n","Epoch 857, Loss 679741278.50, Val loss 18623984.00\n","Epoch 858, Loss 679713297.56, Val loss 18624218.00\n","==================================================================\n","Saved best model\n","Epoch 859, Loss 679647205.38, Val loss 18620548.00\n","Epoch 860, Loss 679615731.25, Val loss 18621154.00\n","==================================================================\n","Saved best model\n","Epoch 861, Loss 679541962.62, Val loss 18617654.00\n","Epoch 862, Loss 679505897.00, Val loss 18618872.00\n","==================================================================\n","Saved best model\n","Epoch 863, Loss 679480079.69, Val loss 18615124.00\n","Epoch 864, Loss 679601023.06, Val loss 18616364.00\n","Epoch 865, Loss 679291079.06, Val loss 18615344.00\n","==================================================================\n","Saved best model\n","Epoch 866, Loss 679323239.25, Val loss 18611842.00\n","Epoch 867, Loss 679236510.88, Val loss 18613396.00\n","==================================================================\n","Saved best model\n","Epoch 868, Loss 679163975.62, Val loss 18609766.00\n","Epoch 869, Loss 679102611.94, Val loss 18612928.00\n","Epoch 870, Loss 678994884.38, Val loss 18611612.00\n","==================================================================\n","Saved best model\n","Epoch 871, Loss 678979893.62, Val loss 18609294.00\n","==================================================================\n","Saved best model\n","Epoch 872, Loss 678995001.69, Val loss 18605824.00\n","Epoch 873, Loss 678918826.00, Val loss 18607272.00\n","==================================================================\n","Saved best model\n","Epoch 874, Loss 678816824.56, Val loss 18604114.00\n","Epoch 875, Loss 678608231.44, Val loss 18614972.00\n","==================================================================\n","Saved best model\n","Epoch 876, Loss 678754543.75, Val loss 18602024.00\n","Epoch 877, Loss 678817689.38, Val loss 18609300.00\n","==================================================================\n","Saved best model\n","Epoch 878, Loss 678678108.19, Val loss 18600110.00\n","Epoch 879, Loss 678540281.94, Val loss 18608098.00\n","==================================================================\n","Saved best model\n","Epoch 880, Loss 678526925.25, Val loss 18596658.00\n","Epoch 881, Loss 678467575.31, Val loss 18604548.00\n","==================================================================\n","Saved best model\n","Epoch 882, Loss 678433361.44, Val loss 18594190.00\n","Epoch 883, Loss 678387478.81, Val loss 18600846.00\n","==================================================================\n","Saved best model\n","Epoch 884, Loss 678347096.75, Val loss 18591920.00\n","Epoch 885, Loss 678300053.06, Val loss 18596872.00\n","==================================================================\n","Saved best model\n","Epoch 886, Loss 678263826.56, Val loss 18589884.00\n","Epoch 887, Loss 678219334.50, Val loss 18592074.00\n","==================================================================\n","Saved best model\n","Epoch 888, Loss 678182177.94, Val loss 18588730.00\n","==================================================================\n","Saved best model\n","Epoch 889, Loss 678325488.38, Val loss 18583800.00\n","Epoch 890, Loss 678227708.88, Val loss 18586414.00\n","Epoch 891, Loss 678014707.44, Val loss 18585648.00\n","Epoch 892, Loss 677997663.19, Val loss 18587466.00\n","Epoch 893, Loss 677970485.31, Val loss 18604980.00\n","==================================================================\n","Saved best model\n","Epoch 894, Loss 678067424.69, Val loss 18579470.00\n","Epoch 895, Loss 677902252.69, Val loss 18585022.00\n","==================================================================\n","Saved best model\n","Epoch 896, Loss 677822913.06, Val loss 18578100.00\n","Epoch 897, Loss 677712996.50, Val loss 18589812.00\n","==================================================================\n","Saved best model\n","Epoch 898, Loss 677715726.12, Val loss 18573710.00\n","Epoch 899, Loss 677708902.56, Val loss 18585006.00\n","==================================================================\n","Saved best model\n","Epoch 900, Loss 677618365.50, Val loss 18571132.00\n","Epoch 901, Loss 677620882.75, Val loss 18582918.00\n","==================================================================\n","Saved best model\n","Epoch 902, Loss 677530483.62, Val loss 18568662.00\n","Epoch 903, Loss 677535121.94, Val loss 18580344.00\n","==================================================================\n","Saved best model\n","Epoch 904, Loss 676487001.88, Val loss 18566312.00\n","Epoch 905, Loss 677664512.31, Val loss 18566994.00\n","Epoch 906, Loss 677341718.50, Val loss 18586096.00\n","==================================================================\n","Saved best model\n","Epoch 907, Loss 677344467.94, Val loss 18562980.00\n","Epoch 908, Loss 677543163.62, Val loss 18572880.00\n","==================================================================\n","Saved best model\n","Epoch 909, Loss 677234399.50, Val loss 18559962.00\n","Epoch 910, Loss 677251981.88, Val loss 18571220.00\n","==================================================================\n","Saved best model\n","Epoch 911, Loss 677143522.69, Val loss 18556926.00\n","Epoch 912, Loss 677179073.00, Val loss 18568800.00\n","==================================================================\n","Saved best model\n","Epoch 913, Loss 677058742.69, Val loss 18554328.00\n","Epoch 914, Loss 677102020.50, Val loss 18565502.00\n","==================================================================\n","Saved best model\n","Epoch 915, Loss 676987761.12, Val loss 18551968.00\n","Epoch 916, Loss 677211496.19, Val loss 18562730.00\n","==================================================================\n","Saved best model\n","Epoch 917, Loss 676882555.00, Val loss 18549112.00\n","Epoch 918, Loss 676954293.19, Val loss 18559404.00\n","==================================================================\n","Saved best model\n","Epoch 919, Loss 676814938.25, Val loss 18546352.00\n","Epoch 920, Loss 676897566.94, Val loss 18555944.00\n","==================================================================\n","Saved best model\n","Epoch 921, Loss 676743657.88, Val loss 18543150.00\n","Epoch 922, Loss 676833903.69, Val loss 18552022.00\n","==================================================================\n","Saved best model\n","Epoch 923, Loss 676658256.88, Val loss 18539352.00\n","Epoch 924, Loss 676737786.12, Val loss 18547612.00\n","==================================================================\n","Saved best model\n","Epoch 925, Loss 676578302.50, Val loss 18534828.00\n","Epoch 926, Loss 676644837.44, Val loss 18542228.00\n","==================================================================\n","Saved best model\n","Epoch 927, Loss 676473135.31, Val loss 18529278.00\n","Epoch 928, Loss 676524982.44, Val loss 18536360.00\n","==================================================================\n","Saved best model\n","Epoch 929, Loss 676493192.25, Val loss 18525130.00\n","Epoch 930, Loss 676488513.25, Val loss 18531614.00\n","==================================================================\n","Saved best model\n","Epoch 931, Loss 676187596.38, Val loss 18519268.00\n","Epoch 932, Loss 676249893.06, Val loss 18526208.00\n","==================================================================\n","Saved best model\n","Epoch 933, Loss 676085890.38, Val loss 18515018.00\n","Epoch 934, Loss 676149866.19, Val loss 18521798.00\n","==================================================================\n","Saved best model\n","Epoch 935, Loss 675996912.88, Val loss 18510546.00\n","Epoch 936, Loss 676097752.25, Val loss 18517942.00\n","==================================================================\n","Saved best model\n","Epoch 937, Loss 675909012.00, Val loss 18506326.00\n","Epoch 938, Loss 676107391.06, Val loss 18514416.00\n","==================================================================\n","Saved best model\n","Epoch 939, Loss 675828994.00, Val loss 18503046.00\n","Epoch 940, Loss 676042781.06, Val loss 18511086.00\n","==================================================================\n","Saved best model\n","Epoch 941, Loss 675747730.69, Val loss 18499896.00\n","Epoch 942, Loss 675993830.69, Val loss 18507882.00\n","==================================================================\n","Saved best model\n","Epoch 943, Loss 675672027.75, Val loss 18496972.00\n","Epoch 944, Loss 675959507.81, Val loss 18504852.00\n","==================================================================\n","Saved best model\n","Epoch 945, Loss 675595694.00, Val loss 18494138.00\n","Epoch 946, Loss 675898984.06, Val loss 18501938.00\n","==================================================================\n","Saved best model\n","Epoch 947, Loss 675522380.56, Val loss 18491572.00\n","Epoch 948, Loss 675819859.50, Val loss 18499320.00\n","==================================================================\n","Saved best model\n","Epoch 949, Loss 675630001.31, Val loss 18489496.00\n","Epoch 950, Loss 675714574.44, Val loss 18496494.00\n","==================================================================\n","Saved best model\n","Epoch 951, Loss 675355137.56, Val loss 18486516.00\n","Epoch 952, Loss 675610205.38, Val loss 18493420.00\n","==================================================================\n","Saved best model\n","Epoch 953, Loss 675280299.62, Val loss 18483726.00\n","Epoch 954, Loss 675514220.31, Val loss 18490524.00\n","==================================================================\n","Saved best model\n","Epoch 955, Loss 675192803.25, Val loss 18481158.00\n","Epoch 956, Loss 675418707.25, Val loss 18487708.00\n","==================================================================\n","Saved best model\n","Epoch 957, Loss 675111513.62, Val loss 18478484.00\n","Epoch 958, Loss 675321122.94, Val loss 18484944.00\n","==================================================================\n","Saved best model\n","Epoch 959, Loss 675030000.44, Val loss 18476050.00\n","Epoch 960, Loss 675233034.75, Val loss 18482532.00\n","==================================================================\n","Saved best model\n","Epoch 961, Loss 675140169.88, Val loss 18473922.00\n","Epoch 962, Loss 675439098.06, Val loss 18478026.00\n","==================================================================\n","Saved best model\n","Epoch 963, Loss 675164929.81, Val loss 18469900.00\n","Epoch 964, Loss 675135319.31, Val loss 18478206.00\n","==================================================================\n","Saved best model\n","Epoch 965, Loss 674752489.44, Val loss 18469234.00\n","Epoch 966, Loss 674903254.56, Val loss 18474326.00\n","==================================================================\n","Saved best model\n","Epoch 967, Loss 674670395.75, Val loss 18466392.00\n","Epoch 968, Loss 674815397.62, Val loss 18471270.00\n","==================================================================\n","Saved best model\n","Epoch 969, Loss 674586060.38, Val loss 18463778.00\n","Epoch 970, Loss 674725694.31, Val loss 18468156.00\n","==================================================================\n","Saved best model\n","Epoch 971, Loss 674503897.75, Val loss 18461340.00\n","Epoch 972, Loss 674637279.25, Val loss 18465070.00\n","==================================================================\n","Saved best model\n","Epoch 973, Loss 674428044.38, Val loss 18458956.00\n","Epoch 974, Loss 674552898.06, Val loss 18462082.00\n","==================================================================\n","Saved best model\n","Epoch 975, Loss 674360054.56, Val loss 18456632.00\n","Epoch 976, Loss 674472824.50, Val loss 18459310.00\n","==================================================================\n","Saved best model\n","Epoch 977, Loss 674302582.75, Val loss 18454478.00\n","Epoch 978, Loss 674404698.25, Val loss 18456776.00\n","==================================================================\n","Saved best model\n","Epoch 979, Loss 674225521.00, Val loss 18452282.00\n","Epoch 980, Loss 674335066.38, Val loss 18454318.00\n","==================================================================\n","Saved best model\n","Epoch 981, Loss 674140773.44, Val loss 18450012.00\n","Epoch 982, Loss 674263504.06, Val loss 18451984.00\n","==================================================================\n","Saved best model\n","Epoch 983, Loss 674045048.69, Val loss 18447904.00\n","Epoch 984, Loss 674333581.12, Val loss 18450418.00\n","==================================================================\n","Saved best model\n","Epoch 985, Loss 673928599.50, Val loss 18446450.00\n","Epoch 986, Loss 674036438.50, Val loss 18447332.00\n","==================================================================\n","Saved best model\n","Epoch 987, Loss 673833323.94, Val loss 18443114.00\n","Epoch 988, Loss 673998683.31, Val loss 18445026.00\n","==================================================================\n","Saved best model\n","Epoch 989, Loss 673736030.62, Val loss 18440814.00\n","Epoch 990, Loss 673906704.62, Val loss 18442176.00\n","==================================================================\n","Saved best model\n","Epoch 991, Loss 673645031.88, Val loss 18438170.00\n","Epoch 992, Loss 673816435.25, Val loss 18439286.00\n","==================================================================\n","Saved best model\n","Epoch 993, Loss 673557042.56, Val loss 18435550.00\n","Epoch 994, Loss 673728270.69, Val loss 18436420.00\n","==================================================================\n","Saved best model\n","Epoch 995, Loss 673467531.62, Val loss 18432978.00\n","Epoch 996, Loss 673641656.44, Val loss 18433602.00\n","==================================================================\n","Saved best model\n","Epoch 997, Loss 673225168.31, Val loss 18430300.00\n","==================================================================\n","Saved best model\n","Epoch 998, Loss 673884786.00, Val loss 18430242.00\n","==================================================================\n","Saved best model\n","Epoch 999, Loss 673317338.25, Val loss 18428902.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","import matplotlib.pyplot as plt\n","\n","predicted_output = None\n","labeled_output = None\n","for data in validation_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 23\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"status":"ok","timestamp":1646442457741,"user_tz":360,"elapsed":724,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"9b5f3b0b-d1eb-452b-cc9d-1397b767772c"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["18428902.0\n","tensor([1442.2783, 1444.0994, 1444.4552, 1454.6398, 1456.0919, 1451.7969,\n","        1452.5723, 1451.3713, 1461.3315, 1465.0835, 1460.5835, 1461.7496,\n","        1460.9199, 1473.3818, 1477.2175], device='cuda:0',\n","       grad_fn=<SelectBackward0>)\n","tensor([3277.7500, 3369.6250, 3381.3750, 2813.0000, 2813.0000, 3235.6250,\n","        3702.7500, 2925.1250, 3069.6250, 2597.1250, 2117.6250, 2117.6250,\n","        2634.7500, 3445.1250, 3173.5000], device='cuda:0')\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]}]}