{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3_reorder_skip_and_active.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNXhwWk+DSyRb8jsOaPA7g/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1646515599357,"user_tz":360,"elapsed":1006,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"1b3447cf-b5b2-428d-e3fb-82ec11476438"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f5fb659-bec1-4088-a357-a6f7b2333a4e","executionInfo":{"status":"ok","timestamp":1646515639546,"user_tz":360,"elapsed":40191,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (4.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.10.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: torch-geometric-temporal in /usr/local/lib/python3.7/dist-packages (0.51.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.3)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.12)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (0.4)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (6.1.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.6.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (3.13)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (0.1.8)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric->torch-geometric-temporal) (57.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric->torch-geometric-temporal) (4.11.2)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric->torch-geometric-temporal) (0.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric->torch-geometric-temporal) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.3)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","PyTorch has version 1.10.0+cu111\n","Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 36 \n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3_reorder_input_active_cases.pickle'\n","save_model_relative_path = './saved_models/v3_reorder_skip_and_active'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3_archived_output.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1646516727120,"user_tz":360,"elapsed":263,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"1d97b41f-10de-40dd-fc97-9c11f57f638e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_active_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_active_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_active_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU","executionInfo":{"status":"ok","timestamp":1646515639548,"user_tz":360,"elapsed":54,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3","executionInfo":{"status":"ok","timestamp":1646515639549,"user_tz":360,"elapsed":54,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class GCN(torch.nn.Module):\n","    # def __init__(self):\n","    #     super().__init__()\n","    #     self.conv1 = GCNConv(inputLayer_num_features, hiddenLayer1_num_features)\n","    #     self.conv2 = GCNConv(hiddenLayer1_num_features, outputLayer_num_features)\n","\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","      self.linear2 = Linear(history_window, outputLayer_num_features)\n","      self.linear3 = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x) + self.linear2(data.x[:, 0:history_window])\n","\n","      x = F.elu(x)\n","      x = self.linear3(x)\n","\n","      return x\n","\n","model = GCN().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj","executionInfo":{"status":"ok","timestamp":1646515639550,"user_tz":360,"elapsed":53,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646515639551,"user_tz":360,"elapsed":53,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"41be4b9f-e5c3-4d41-b4ed-2c69617278e3"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GCN(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n","  (linear2): Linear(in_features=6, out_features=15, bias=True)\n","  (linear3): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646516085603,"user_tz":360,"elapsed":446099,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"60e2ed5b-276f-458b-a061-304cd94a1776"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 20506920576.00, Val loss 347813088.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 1474892290.00, Val loss 58686244.00\n","==================================================================\n","Saved best model\n","Epoch 2, Loss 878944152.00, Val loss 58327884.00\n","==================================================================\n","Saved best model\n","Epoch 3, Loss 835704728.00, Val loss 52317044.00\n","==================================================================\n","Saved best model\n","Epoch 4, Loss 866953801.00, Val loss 49369804.00\n","==================================================================\n","Saved best model\n","Epoch 5, Loss 886865260.00, Val loss 45857408.00\n","==================================================================\n","Saved best model\n","Epoch 6, Loss 901472165.00, Val loss 43283756.00\n","==================================================================\n","Saved best model\n","Epoch 7, Loss 917194859.00, Val loss 41913768.00\n","==================================================================\n","Saved best model\n","Epoch 8, Loss 898765659.00, Val loss 40695116.00\n","==================================================================\n","Saved best model\n","Epoch 9, Loss 881596163.00, Val loss 39847216.00\n","==================================================================\n","Saved best model\n","Epoch 10, Loss 730533815.00, Val loss 39167844.00\n","Epoch 11, Loss 682805387.00, Val loss 55913708.00\n","Epoch 12, Loss 545872711.50, Val loss 43668436.00\n","Epoch 13, Loss 520354276.50, Val loss 47619380.00\n","Epoch 14, Loss 502801221.00, Val loss 46748504.00\n","Epoch 15, Loss 489743940.50, Val loss 43677768.00\n","Epoch 16, Loss 481137455.25, Val loss 39728896.00\n","==================================================================\n","Saved best model\n","Epoch 17, Loss 473895954.00, Val loss 36448004.00\n","==================================================================\n","Saved best model\n","Epoch 18, Loss 466220793.75, Val loss 34125804.00\n","==================================================================\n","Saved best model\n","Epoch 19, Loss 458093045.50, Val loss 32369910.00\n","==================================================================\n","Saved best model\n","Epoch 20, Loss 450268495.50, Val loss 30900334.00\n","==================================================================\n","Saved best model\n","Epoch 21, Loss 443194842.25, Val loss 29669076.00\n","==================================================================\n","Saved best model\n","Epoch 22, Loss 436963491.00, Val loss 28634806.00\n","==================================================================\n","Saved best model\n","Epoch 23, Loss 431511186.00, Val loss 27827686.00\n","==================================================================\n","Saved best model\n","Epoch 24, Loss 426687191.75, Val loss 27229262.00\n","==================================================================\n","Saved best model\n","Epoch 25, Loss 422600581.50, Val loss 26786716.00\n","==================================================================\n","Saved best model\n","Epoch 26, Loss 419477429.75, Val loss 26463122.00\n","==================================================================\n","Saved best model\n","Epoch 27, Loss 417320387.50, Val loss 26227218.00\n","==================================================================\n","Saved best model\n","Epoch 28, Loss 415890250.50, Val loss 26090640.00\n","==================================================================\n","Saved best model\n","Epoch 29, Loss 415249408.50, Val loss 25922348.00\n","==================================================================\n","Saved best model\n","Epoch 30, Loss 415167031.25, Val loss 25764740.00\n","==================================================================\n","Saved best model\n","Epoch 31, Loss 415425976.75, Val loss 25617912.00\n","==================================================================\n","Saved best model\n","Epoch 32, Loss 415935271.75, Val loss 25479104.00\n","==================================================================\n","Saved best model\n","Epoch 33, Loss 416506526.50, Val loss 25349352.00\n","==================================================================\n","Saved best model\n","Epoch 34, Loss 417152121.75, Val loss 25230056.00\n","==================================================================\n","Saved best model\n","Epoch 35, Loss 417809833.50, Val loss 25119032.00\n","==================================================================\n","Saved best model\n","Epoch 36, Loss 418473727.00, Val loss 25017316.00\n","==================================================================\n","Saved best model\n","Epoch 37, Loss 419125408.00, Val loss 24923542.00\n","==================================================================\n","Saved best model\n","Epoch 38, Loss 419764113.50, Val loss 24837272.00\n","==================================================================\n","Saved best model\n","Epoch 39, Loss 420381993.00, Val loss 24757564.00\n","==================================================================\n","Saved best model\n","Epoch 40, Loss 420977964.50, Val loss 24683836.00\n","==================================================================\n","Saved best model\n","Epoch 41, Loss 421544333.25, Val loss 24614168.00\n","==================================================================\n","Saved best model\n","Epoch 42, Loss 421936364.75, Val loss 24577312.00\n","==================================================================\n","Saved best model\n","Epoch 43, Loss 422624757.50, Val loss 24523902.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 423033014.75, Val loss 24470514.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 423500439.25, Val loss 24419192.00\n","==================================================================\n","Saved best model\n","Epoch 46, Loss 423943477.75, Val loss 24374728.00\n","==================================================================\n","Saved best model\n","Epoch 47, Loss 424380818.50, Val loss 24327218.00\n","==================================================================\n","Saved best model\n","Epoch 48, Loss 426284586.25, Val loss 24296170.00\n","==================================================================\n","Saved best model\n","Epoch 49, Loss 425828267.50, Val loss 24219360.00\n","==================================================================\n","Saved best model\n","Epoch 50, Loss 425081505.00, Val loss 24217522.00\n","Epoch 51, Loss 2284770067.50, Val loss 31559748.00\n","Epoch 52, Loss 490681300.00, Val loss 29525770.00\n","Epoch 53, Loss 429714007.00, Val loss 35455080.00\n","Epoch 54, Loss 415027092.75, Val loss 34096744.00\n","Epoch 55, Loss 409381404.50, Val loss 32408550.00\n","Epoch 56, Loss 406382458.00, Val loss 30983048.00\n","Epoch 57, Loss 404443709.75, Val loss 29745314.00\n","Epoch 58, Loss 403085339.00, Val loss 28686746.00\n","Epoch 59, Loss 402090824.25, Val loss 27808564.00\n","Epoch 60, Loss 401340006.25, Val loss 27097480.00\n","Epoch 61, Loss 400769316.25, Val loss 26529996.00\n","Epoch 62, Loss 400344831.25, Val loss 26077862.00\n","Epoch 63, Loss 400047464.50, Val loss 25714618.00\n","Epoch 64, Loss 399859247.00, Val loss 25417150.00\n","Epoch 65, Loss 399762622.00, Val loss 25167304.00\n","Epoch 66, Loss 399745349.00, Val loss 24953572.00\n","Epoch 67, Loss 399803088.25, Val loss 24766296.00\n","Epoch 68, Loss 399921810.00, Val loss 24599608.00\n","Epoch 69, Loss 400087677.75, Val loss 24449236.00\n","Epoch 70, Loss 400300186.00, Val loss 24313354.00\n","==================================================================\n","Saved best model\n","Epoch 71, Loss 400553350.75, Val loss 24187854.00\n","==================================================================\n","Saved best model\n","Epoch 72, Loss 400846372.50, Val loss 24073382.00\n","==================================================================\n","Saved best model\n","Epoch 73, Loss 401176207.75, Val loss 23967422.00\n","==================================================================\n","Saved best model\n","Epoch 74, Loss 401519023.00, Val loss 23873142.00\n","==================================================================\n","Saved best model\n","Epoch 75, Loss 401924310.00, Val loss 23779838.00\n","==================================================================\n","Saved best model\n","Epoch 76, Loss 402316482.50, Val loss 23698280.00\n","==================================================================\n","Saved best model\n","Epoch 77, Loss 402733691.75, Val loss 23622624.00\n","==================================================================\n","Saved best model\n","Epoch 78, Loss 403155735.25, Val loss 23554494.00\n","==================================================================\n","Saved best model\n","Epoch 79, Loss 403612140.50, Val loss 23489806.00\n","==================================================================\n","Saved best model\n","Epoch 80, Loss 404071986.75, Val loss 23433310.00\n","==================================================================\n","Saved best model\n","Epoch 81, Loss 404530771.75, Val loss 23379866.00\n","==================================================================\n","Saved best model\n","Epoch 82, Loss 404989494.25, Val loss 23334168.00\n","==================================================================\n","Saved best model\n","Epoch 83, Loss 405449948.00, Val loss 23290302.00\n","==================================================================\n","Saved best model\n","Epoch 84, Loss 405891248.00, Val loss 23253772.00\n","==================================================================\n","Saved best model\n","Epoch 85, Loss 406317395.50, Val loss 23218854.00\n","==================================================================\n","Saved best model\n","Epoch 86, Loss 406716435.25, Val loss 23190556.00\n","==================================================================\n","Saved best model\n","Epoch 87, Loss 407085193.50, Val loss 23163784.00\n","==================================================================\n","Saved best model\n","Epoch 88, Loss 407394670.50, Val loss 23145018.00\n","==================================================================\n","Saved best model\n","Epoch 89, Loss 407715258.50, Val loss 23115324.00\n","==================================================================\n","Saved best model\n","Epoch 90, Loss 407940110.25, Val loss 23097280.00\n","==================================================================\n","Saved best model\n","Epoch 91, Loss 408152197.75, Val loss 23079162.00\n","==================================================================\n","Saved best model\n","Epoch 92, Loss 408326092.50, Val loss 23063140.00\n","==================================================================\n","Saved best model\n","Epoch 93, Loss 408453434.00, Val loss 23048670.00\n","==================================================================\n","Saved best model\n","Epoch 94, Loss 408526140.50, Val loss 23035540.00\n","==================================================================\n","Saved best model\n","Epoch 95, Loss 408560396.00, Val loss 23023590.00\n","==================================================================\n","Saved best model\n","Epoch 96, Loss 408543510.00, Val loss 23012048.00\n","==================================================================\n","Saved best model\n","Epoch 97, Loss 408493710.00, Val loss 23000424.00\n","==================================================================\n","Saved best model\n","Epoch 98, Loss 408398291.75, Val loss 22989480.00\n","==================================================================\n","Saved best model\n","Epoch 99, Loss 408268485.50, Val loss 22979020.00\n","==================================================================\n","Saved best model\n","Epoch 100, Loss 408096355.00, Val loss 22968846.00\n","==================================================================\n","Saved best model\n","Epoch 101, Loss 407882482.50, Val loss 22959056.00\n","==================================================================\n","Saved best model\n","Epoch 102, Loss 407642161.50, Val loss 22949822.00\n","==================================================================\n","Saved best model\n","Epoch 103, Loss 407379534.50, Val loss 22939796.00\n","==================================================================\n","Saved best model\n","Epoch 104, Loss 407083779.50, Val loss 22931978.00\n","==================================================================\n","Saved best model\n","Epoch 105, Loss 406777048.00, Val loss 22924968.00\n","==================================================================\n","Saved best model\n","Epoch 106, Loss 406456449.00, Val loss 22918436.00\n","==================================================================\n","Saved best model\n","Epoch 107, Loss 406128163.75, Val loss 22911376.00\n","==================================================================\n","Saved best model\n","Epoch 108, Loss 405780042.75, Val loss 22906602.00\n","==================================================================\n","Saved best model\n","Epoch 109, Loss 405441688.75, Val loss 22905310.00\n","Epoch 110, Loss 405119424.00, Val loss 22918170.00\n","==================================================================\n","Saved best model\n","Epoch 111, Loss 404806759.50, Val loss 22886684.00\n","Epoch 112, Loss 404392543.00, Val loss 22890168.00\n","Epoch 113, Loss 403998200.75, Val loss 22925286.00\n","Epoch 114, Loss 403885224.00, Val loss 22903264.00\n","==================================================================\n","Saved best model\n","Epoch 115, Loss 403495949.25, Val loss 22876538.00\n","==================================================================\n","Saved best model\n","Epoch 116, Loss 403181318.75, Val loss 22873994.00\n","Epoch 117, Loss 402975886.50, Val loss 22879122.00\n","==================================================================\n","Saved best model\n","Epoch 118, Loss 402801476.25, Val loss 22842100.00\n","==================================================================\n","Saved best model\n","Epoch 119, Loss 402720498.25, Val loss 22830206.00\n","==================================================================\n","Saved best model\n","Epoch 120, Loss 402527133.75, Val loss 22778686.00\n","==================================================================\n","Saved best model\n","Epoch 121, Loss 402531592.00, Val loss 22737968.00\n","==================================================================\n","Saved best model\n","Epoch 122, Loss 402442711.50, Val loss 22662438.00\n","==================================================================\n","Saved best model\n","Epoch 123, Loss 402775615.50, Val loss 22576332.00\n","==================================================================\n","Saved best model\n","Epoch 124, Loss 402873955.75, Val loss 22484010.00\n","==================================================================\n","Saved best model\n","Epoch 125, Loss 402950491.50, Val loss 22387492.00\n","==================================================================\n","Saved best model\n","Epoch 126, Loss 402925497.75, Val loss 22293234.00\n","==================================================================\n","Saved best model\n","Epoch 127, Loss 402552258.50, Val loss 22277164.00\n","==================================================================\n","Saved best model\n","Epoch 128, Loss 402272425.75, Val loss 22226020.00\n","==================================================================\n","Saved best model\n","Epoch 129, Loss 401418246.25, Val loss 22161334.00\n","Epoch 130, Loss 400141905.50, Val loss 22206936.00\n","Epoch 131, Loss 399994792.00, Val loss 22183778.00\n","==================================================================\n","Saved best model\n","Epoch 132, Loss 398871622.75, Val loss 22153340.00\n","Epoch 133, Loss 397980706.25, Val loss 22190690.00\n","Epoch 134, Loss 397455968.50, Val loss 22194840.00\n","Epoch 135, Loss 397043821.00, Val loss 22169752.00\n","==================================================================\n","Saved best model\n","Epoch 136, Loss 396346039.00, Val loss 22128952.00\n","Epoch 137, Loss 395662890.75, Val loss 22150628.00\n","Epoch 138, Loss 395360002.00, Val loss 22139408.00\n","==================================================================\n","Saved best model\n","Epoch 139, Loss 395086780.00, Val loss 22082196.00\n","Epoch 140, Loss 394414859.00, Val loss 22107288.00\n","Epoch 141, Loss 394139506.75, Val loss 22087746.00\n","==================================================================\n","Saved best model\n","Epoch 142, Loss 393823247.00, Val loss 22041610.00\n","Epoch 143, Loss 393024838.25, Val loss 22047284.00\n","==================================================================\n","Saved best model\n","Epoch 144, Loss 392686998.25, Val loss 22017296.00\n","==================================================================\n","Saved best model\n","Epoch 145, Loss 392101430.75, Val loss 22002108.00\n","==================================================================\n","Saved best model\n","Epoch 146, Loss 391540402.00, Val loss 22000718.00\n","Epoch 147, Loss 391101502.75, Val loss 22020974.00\n","Epoch 148, Loss 390479158.75, Val loss 22009620.00\n","Epoch 149, Loss 390105565.25, Val loss 22001560.00\n","==================================================================\n","Saved best model\n","Epoch 150, Loss 389900732.25, Val loss 21976950.00\n","Epoch 151, Loss 389658081.25, Val loss 21977128.00\n","==================================================================\n","Saved best model\n","Epoch 152, Loss 389829417.75, Val loss 21921742.00\n","Epoch 153, Loss 389197741.00, Val loss 21945086.00\n","==================================================================\n","Saved best model\n","Epoch 154, Loss 389543249.25, Val loss 21862814.00\n","Epoch 155, Loss 388336826.75, Val loss 21907670.00\n","==================================================================\n","Saved best model\n","Epoch 156, Loss 388928218.00, Val loss 21801546.00\n","Epoch 157, Loss 387073198.75, Val loss 21883274.00\n","==================================================================\n","Saved best model\n","Epoch 158, Loss 388090016.25, Val loss 21735962.00\n","Epoch 159, Loss 385996919.00, Val loss 21837992.00\n","==================================================================\n","Saved best model\n","Epoch 160, Loss 387025933.50, Val loss 21720948.00\n","Epoch 161, Loss 385899831.25, Val loss 21756012.00\n","==================================================================\n","Saved best model\n","Epoch 162, Loss 387029138.25, Val loss 21647748.00\n","Epoch 163, Loss 383967439.75, Val loss 21767768.00\n","==================================================================\n","Saved best model\n","Epoch 164, Loss 386262569.50, Val loss 21609636.00\n","Epoch 165, Loss 382235248.25, Val loss 21744344.00\n","==================================================================\n","Saved best model\n","Epoch 166, Loss 384910595.50, Val loss 21576042.00\n","Epoch 167, Loss 380902368.00, Val loss 21742720.00\n","==================================================================\n","Saved best model\n","Epoch 168, Loss 384393476.50, Val loss 21552982.00\n","Epoch 169, Loss 379731391.50, Val loss 21719974.00\n","==================================================================\n","Saved best model\n","Epoch 170, Loss 383415906.25, Val loss 21520474.00\n","Epoch 171, Loss 378226077.25, Val loss 21687496.00\n","==================================================================\n","Saved best model\n","Epoch 172, Loss 382380067.25, Val loss 21509898.00\n","Epoch 173, Loss 377540167.50, Val loss 21668144.00\n","==================================================================\n","Saved best model\n","Epoch 174, Loss 381579108.75, Val loss 21460368.00\n","Epoch 175, Loss 375779996.00, Val loss 21671826.00\n","==================================================================\n","Saved best model\n","Epoch 176, Loss 381004376.25, Val loss 21391282.00\n","Epoch 177, Loss 374595402.25, Val loss 21609426.00\n","==================================================================\n","Saved best model\n","Epoch 178, Loss 379854686.50, Val loss 21356098.00\n","Epoch 179, Loss 373638063.50, Val loss 21593764.00\n","==================================================================\n","Saved best model\n","Epoch 180, Loss 379716023.75, Val loss 21266326.00\n","Epoch 181, Loss 372752286.00, Val loss 21530492.00\n","==================================================================\n","Saved best model\n","Epoch 182, Loss 378764997.00, Val loss 21227664.00\n","Epoch 183, Loss 371833296.25, Val loss 21550930.00\n","==================================================================\n","Saved best model\n","Epoch 184, Loss 378902008.50, Val loss 21147714.00\n","Epoch 185, Loss 370336863.75, Val loss 21562684.00\n","==================================================================\n","Saved best model\n","Epoch 186, Loss 378356956.25, Val loss 21093138.00\n","Epoch 187, Loss 369313216.00, Val loss 21498570.00\n","==================================================================\n","Saved best model\n","Epoch 188, Loss 376264637.50, Val loss 21079056.00\n","Epoch 189, Loss 368432088.75, Val loss 21498892.00\n","==================================================================\n","Saved best model\n","Epoch 190, Loss 375201854.50, Val loss 20995198.00\n","Epoch 191, Loss 366408423.25, Val loss 21488776.00\n","Epoch 192, Loss 374294291.25, Val loss 21052028.00\n","Epoch 193, Loss 366259454.25, Val loss 21353590.00\n","==================================================================\n","Saved best model\n","Epoch 194, Loss 373059392.50, Val loss 20877182.00\n","Epoch 195, Loss 363743319.50, Val loss 21436716.00\n","Epoch 196, Loss 372495920.50, Val loss 20891580.00\n","Epoch 197, Loss 362209237.00, Val loss 21374822.00\n","Epoch 198, Loss 369813997.25, Val loss 20948392.00\n","Epoch 199, Loss 360851939.75, Val loss 21490268.00\n","Epoch 200, Loss 369765731.00, Val loss 20906184.00\n","Epoch 201, Loss 358724779.00, Val loss 21358710.00\n","Epoch 202, Loss 365580091.75, Val loss 20973138.00\n","Epoch 203, Loss 356288833.00, Val loss 21415418.00\n","Epoch 204, Loss 363823967.75, Val loss 20989184.00\n","Epoch 205, Loss 354223159.75, Val loss 21482666.00\n","Epoch 206, Loss 361746715.75, Val loss 21178900.00\n","Epoch 207, Loss 353004342.75, Val loss 21514678.00\n","Epoch 208, Loss 360011480.00, Val loss 21170902.00\n","Epoch 209, Loss 350039088.00, Val loss 21584780.00\n","Epoch 210, Loss 357589366.00, Val loss 21280104.00\n","Epoch 211, Loss 347667144.50, Val loss 21537442.00\n","Epoch 212, Loss 355233091.75, Val loss 21380096.00\n","Epoch 213, Loss 345879957.50, Val loss 21473358.00\n","Epoch 214, Loss 352683614.00, Val loss 21602924.00\n","Epoch 215, Loss 345107732.50, Val loss 21548864.00\n","Epoch 216, Loss 353175173.25, Val loss 21468826.00\n","Epoch 217, Loss 344165051.00, Val loss 21276698.00\n","Epoch 218, Loss 349571532.75, Val loss 21589112.00\n","Epoch 219, Loss 344719234.50, Val loss 21120006.00\n","Epoch 220, Loss 346016230.50, Val loss 21417538.00\n","Epoch 221, Loss 343032472.50, Val loss 21416162.00\n","Epoch 222, Loss 347222989.75, Val loss 21386296.00\n","Epoch 223, Loss 342641201.50, Val loss 21094612.00\n","Epoch 224, Loss 343074720.00, Val loss 21272136.00\n","Epoch 225, Loss 342411463.00, Val loss 21037802.00\n","Epoch 226, Loss 337690207.00, Val loss 21143632.00\n","Epoch 227, Loss 341275914.50, Val loss 21295078.00\n","Epoch 228, Loss 336821932.00, Val loss 21306184.00\n","Epoch 229, Loss 341007045.50, Val loss 21339136.00\n","Epoch 230, Loss 336185540.75, Val loss 20918516.00\n","Epoch 231, Loss 336199011.00, Val loss 21195548.00\n","Epoch 232, Loss 336164301.25, Val loss 20940854.00\n","Epoch 233, Loss 330762329.75, Val loss 20989220.00\n","Epoch 234, Loss 333948283.00, Val loss 21302422.00\n","Epoch 235, Loss 331966540.25, Val loss 21147604.00\n","Epoch 236, Loss 330832718.00, Val loss 21054968.00\n","Epoch 237, Loss 327063036.50, Val loss 21036144.00\n","Epoch 238, Loss 330007629.50, Val loss 21244204.00\n","==================================================================\n","Saved best model\n","Epoch 239, Loss 328971882.00, Val loss 20819328.00\n","Epoch 240, Loss 324877158.75, Val loss 20843006.00\n","Epoch 241, Loss 322361695.00, Val loss 21343138.00\n","Epoch 242, Loss 324879981.25, Val loss 21191762.00\n","Epoch 243, Loss 323045256.25, Val loss 20952344.00\n","Epoch 244, Loss 317999074.25, Val loss 20948412.00\n","Epoch 245, Loss 321548788.50, Val loss 21234452.00\n","Epoch 246, Loss 318360484.50, Val loss 21161026.00\n","Epoch 247, Loss 317576446.50, Val loss 21191358.00\n","Epoch 248, Loss 316365824.75, Val loss 21035318.00\n","Epoch 249, Loss 313784442.00, Val loss 21008794.00\n","Epoch 250, Loss 312229523.00, Val loss 21005968.00\n","Epoch 251, Loss 310112519.50, Val loss 21601318.00\n","Epoch 252, Loss 312597282.25, Val loss 21378532.00\n","Epoch 253, Loss 309280996.50, Val loss 21272000.00\n","Epoch 254, Loss 306889892.50, Val loss 21222968.00\n","Epoch 255, Loss 305161661.50, Val loss 21120946.00\n","Epoch 256, Loss 302900563.50, Val loss 21622526.00\n","Epoch 257, Loss 304255577.25, Val loss 21544332.00\n","Epoch 258, Loss 302347012.00, Val loss 21287994.00\n","Epoch 259, Loss 299739994.25, Val loss 21264562.00\n","Epoch 260, Loss 298009015.50, Val loss 21262048.00\n","Epoch 261, Loss 296001610.25, Val loss 21779652.00\n","Epoch 262, Loss 297208715.25, Val loss 21694332.00\n","Epoch 263, Loss 295264075.75, Val loss 21476444.00\n","Epoch 264, Loss 292553736.75, Val loss 21389766.00\n","Epoch 265, Loss 291205856.00, Val loss 22069766.00\n","Epoch 266, Loss 299649461.25, Val loss 32061362.00\n","Epoch 267, Loss 298727416.75, Val loss 21807878.00\n","==================================================================\n","Saved best model\n","Epoch 268, Loss 291429467.00, Val loss 19897586.00\n","Epoch 269, Loss 281151250.25, Val loss 20758714.00\n","Epoch 270, Loss 282136279.00, Val loss 21160484.00\n","Epoch 271, Loss 282407457.50, Val loss 21339648.00\n","Epoch 272, Loss 281757646.25, Val loss 21460676.00\n","Epoch 273, Loss 281516476.25, Val loss 22166436.00\n","Epoch 274, Loss 280741865.00, Val loss 21925390.00\n","Epoch 275, Loss 280753027.00, Val loss 21967316.00\n","Epoch 276, Loss 276031872.00, Val loss 22752340.00\n","Epoch 277, Loss 279557223.00, Val loss 22548088.00\n","Epoch 278, Loss 275991751.25, Val loss 22128362.00\n","Epoch 279, Loss 275850242.75, Val loss 22235382.00\n","Epoch 280, Loss 273848730.50, Val loss 22523164.00\n","Epoch 281, Loss 273234853.75, Val loss 22417964.00\n","Epoch 282, Loss 270194885.25, Val loss 22196508.00\n","Epoch 283, Loss 270744021.00, Val loss 22877588.00\n","Epoch 284, Loss 271101701.50, Val loss 22694820.00\n","Epoch 285, Loss 268914299.50, Val loss 22447364.00\n","Epoch 286, Loss 266566474.75, Val loss 22767304.00\n","Epoch 287, Loss 265168174.25, Val loss 22999768.00\n","Epoch 288, Loss 266378634.50, Val loss 22840764.00\n","Epoch 289, Loss 264588336.00, Val loss 23044424.00\n","Epoch 290, Loss 263938735.25, Val loss 22899068.00\n","Epoch 291, Loss 262340328.50, Val loss 22746602.00\n","Epoch 292, Loss 260744352.25, Val loss 23025208.00\n","Epoch 293, Loss 260575708.75, Val loss 22940368.00\n","Epoch 294, Loss 259192558.25, Val loss 22822894.00\n","Epoch 295, Loss 257847868.50, Val loss 23081220.00\n","Epoch 296, Loss 254826723.00, Val loss 23054460.00\n","Epoch 297, Loss 255427692.75, Val loss 23178938.00\n","Epoch 298, Loss 253391173.25, Val loss 23149180.00\n","Epoch 299, Loss 252619479.75, Val loss 23280234.00\n","Epoch 300, Loss 252032742.50, Val loss 23261294.00\n","Epoch 301, Loss 250171490.00, Val loss 23180870.00\n","Epoch 302, Loss 248030046.50, Val loss 23079556.00\n","Epoch 303, Loss 247290400.50, Val loss 23229010.00\n","Epoch 304, Loss 248000626.50, Val loss 23664480.00\n","Epoch 305, Loss 244647742.00, Val loss 23225112.00\n","Epoch 306, Loss 242418343.00, Val loss 22911630.00\n","Epoch 307, Loss 241289151.00, Val loss 22794142.00\n","Epoch 308, Loss 239436078.25, Val loss 22627296.00\n","Epoch 309, Loss 237683528.50, Val loss 22440258.00\n","Epoch 310, Loss 235330301.25, Val loss 21848160.00\n","Epoch 311, Loss 234235897.75, Val loss 21508832.00\n","Epoch 312, Loss 232036109.75, Val loss 21165394.00\n","Epoch 313, Loss 230186921.25, Val loss 20339574.00\n","==================================================================\n","Saved best model\n","Epoch 314, Loss 227884786.75, Val loss 19679580.00\n","==================================================================\n","Saved best model\n","Epoch 315, Loss 225895800.50, Val loss 19025378.00\n","==================================================================\n","Saved best model\n","Epoch 316, Loss 223822844.12, Val loss 18265272.00\n","==================================================================\n","Saved best model\n","Epoch 317, Loss 222155680.25, Val loss 17006178.00\n","==================================================================\n","Saved best model\n","Epoch 318, Loss 219683248.75, Val loss 15741596.00\n","==================================================================\n","Saved best model\n","Epoch 319, Loss 218358629.75, Val loss 15242111.00\n","==================================================================\n","Saved best model\n","Epoch 320, Loss 216258208.62, Val loss 14265127.00\n","==================================================================\n","Saved best model\n","Epoch 321, Loss 214507222.00, Val loss 13772306.00\n","==================================================================\n","Saved best model\n","Epoch 322, Loss 212556102.38, Val loss 13321689.00\n","==================================================================\n","Saved best model\n","Epoch 323, Loss 210649792.00, Val loss 12939319.00\n","==================================================================\n","Saved best model\n","Epoch 324, Loss 208758563.38, Val loss 12632793.00\n","==================================================================\n","Saved best model\n","Epoch 325, Loss 206919451.50, Val loss 12277639.00\n","==================================================================\n","Saved best model\n","Epoch 326, Loss 205827301.12, Val loss 12093917.00\n","==================================================================\n","Saved best model\n","Epoch 327, Loss 204142493.62, Val loss 11965819.00\n","==================================================================\n","Saved best model\n","Epoch 328, Loss 202030502.38, Val loss 11901282.00\n","==================================================================\n","Saved best model\n","Epoch 329, Loss 202650503.50, Val loss 11662066.00\n","==================================================================\n","Saved best model\n","Epoch 330, Loss 201773646.38, Val loss 11645147.00\n","Epoch 331, Loss 201143639.25, Val loss 11685531.00\n","Epoch 332, Loss 202736608.38, Val loss 11731250.00\n","Epoch 333, Loss 206081014.88, Val loss 11794099.00\n","Epoch 334, Loss 201808946.25, Val loss 11959467.00\n","Epoch 335, Loss 205834224.38, Val loss 12093496.00\n","Epoch 336, Loss 200128376.50, Val loss 12309819.00\n","Epoch 337, Loss 209077675.88, Val loss 12730633.00\n","Epoch 338, Loss 204493529.38, Val loss 12731531.00\n","Epoch 339, Loss 208936393.62, Val loss 12718022.00\n","Epoch 340, Loss 204476992.00, Val loss 12545067.00\n","Epoch 341, Loss 207481913.75, Val loss 12397070.00\n","Epoch 342, Loss 200517175.00, Val loss 12579089.00\n","Epoch 343, Loss 207158725.50, Val loss 12610515.00\n","Epoch 344, Loss 202630098.25, Val loss 12592238.00\n","Epoch 345, Loss 204706815.88, Val loss 12571995.00\n","Epoch 346, Loss 203031560.88, Val loss 12645121.00\n","Epoch 347, Loss 203341708.25, Val loss 12597129.00\n","Epoch 348, Loss 199244111.62, Val loss 12600344.00\n","Epoch 349, Loss 204314161.25, Val loss 12631012.00\n","Epoch 350, Loss 197139475.25, Val loss 12631921.00\n","Epoch 351, Loss 201314017.75, Val loss 12754444.00\n","Epoch 352, Loss 200192041.50, Val loss 12787721.00\n","Epoch 353, Loss 199291015.75, Val loss 12868181.00\n","Epoch 354, Loss 200235730.12, Val loss 12922052.00\n","Epoch 355, Loss 199943771.25, Val loss 12862721.00\n","Epoch 356, Loss 198925794.00, Val loss 12681912.00\n","Epoch 357, Loss 195542459.12, Val loss 12763022.00\n","Epoch 358, Loss 197747512.62, Val loss 12794285.00\n","Epoch 359, Loss 196870797.88, Val loss 12861051.00\n","Epoch 360, Loss 196490906.12, Val loss 12915730.00\n","Epoch 361, Loss 196587997.50, Val loss 12916899.00\n","Epoch 362, Loss 195622224.25, Val loss 12866406.00\n","Epoch 363, Loss 194851956.38, Val loss 12830170.00\n","Epoch 364, Loss 193049656.25, Val loss 12953104.00\n","Epoch 365, Loss 195331244.75, Val loss 12985802.00\n","Epoch 366, Loss 193846148.25, Val loss 12935093.00\n","Epoch 367, Loss 192825514.50, Val loss 13017226.00\n","Epoch 368, Loss 193658825.00, Val loss 13063578.00\n","Epoch 369, Loss 192791225.88, Val loss 13035936.00\n","Epoch 370, Loss 192136305.25, Val loss 13008212.00\n","Epoch 371, Loss 190547464.38, Val loss 13116180.00\n","Epoch 372, Loss 193043349.25, Val loss 13213212.00\n","Epoch 373, Loss 190217301.12, Val loss 13040257.00\n","Epoch 374, Loss 190550034.38, Val loss 13188116.00\n","Epoch 375, Loss 189871864.88, Val loss 13119161.00\n","Epoch 376, Loss 190226815.62, Val loss 13205127.00\n","Epoch 377, Loss 187880642.25, Val loss 13087449.00\n","Epoch 378, Loss 189914863.12, Val loss 13238805.00\n","Epoch 379, Loss 188299142.38, Val loss 13145995.00\n","Epoch 380, Loss 188837617.25, Val loss 13252269.00\n","Epoch 381, Loss 187540314.75, Val loss 13237726.00\n","Epoch 382, Loss 187028932.12, Val loss 13174787.00\n","Epoch 383, Loss 186425651.12, Val loss 13206603.00\n","Epoch 384, Loss 187933704.38, Val loss 13364691.00\n","Epoch 385, Loss 186716804.88, Val loss 13316512.00\n","Epoch 386, Loss 185604344.12, Val loss 13261878.00\n","Epoch 387, Loss 184998294.75, Val loss 13247907.00\n","Epoch 388, Loss 184712127.75, Val loss 13266687.00\n","Epoch 389, Loss 183783345.50, Val loss 13165405.00\n","Epoch 390, Loss 185356884.00, Val loss 13353712.00\n","Epoch 391, Loss 184180924.25, Val loss 13298517.00\n","Epoch 392, Loss 183647788.38, Val loss 13281367.00\n","Epoch 393, Loss 182946487.88, Val loss 13277125.00\n","Epoch 394, Loss 182440730.25, Val loss 13244022.00\n","Epoch 395, Loss 181926366.12, Val loss 13180134.00\n","Epoch 396, Loss 182649281.62, Val loss 13297782.00\n","Epoch 397, Loss 181456691.38, Val loss 13230257.00\n","Epoch 398, Loss 181366983.38, Val loss 13228854.00\n","Epoch 399, Loss 180575517.62, Val loss 13217178.00\n","Epoch 400, Loss 179674583.75, Val loss 13085919.00\n","Epoch 401, Loss 181067342.38, Val loss 13215111.00\n","Epoch 402, Loss 180040424.88, Val loss 13159152.00\n","Epoch 403, Loss 179194961.88, Val loss 13124227.00\n","Epoch 404, Loss 178857747.25, Val loss 13044217.00\n","Epoch 405, Loss 178610763.75, Val loss 12921842.00\n","Epoch 406, Loss 179611367.00, Val loss 13034085.00\n","Epoch 407, Loss 178684353.62, Val loss 13017742.00\n","Epoch 408, Loss 177961763.88, Val loss 12950197.00\n","Epoch 409, Loss 177721952.62, Val loss 12897493.00\n","Epoch 410, Loss 177814936.62, Val loss 12909618.00\n","Epoch 411, Loss 177048795.62, Val loss 12737007.00\n","Epoch 412, Loss 178548235.62, Val loss 12858900.00\n","Epoch 413, Loss 177418330.50, Val loss 12820052.00\n","Epoch 414, Loss 176887631.50, Val loss 12767862.00\n","Epoch 415, Loss 176744324.88, Val loss 12712816.00\n","Epoch 416, Loss 176835302.12, Val loss 12734144.00\n","Epoch 417, Loss 176369198.12, Val loss 12593527.00\n","Epoch 418, Loss 177696699.62, Val loss 12701752.00\n","Epoch 419, Loss 176585570.50, Val loss 12670975.00\n","Epoch 420, Loss 176073309.00, Val loss 12625916.00\n","Epoch 421, Loss 175796052.25, Val loss 12656790.00\n","Epoch 422, Loss 175983806.38, Val loss 12564433.00\n","Epoch 423, Loss 176194897.88, Val loss 12491411.00\n","Epoch 424, Loss 176387192.12, Val loss 12660575.00\n","Epoch 425, Loss 175751010.12, Val loss 12523603.00\n","Epoch 426, Loss 175638632.00, Val loss 12524470.00\n","Epoch 427, Loss 175174926.50, Val loss 12540786.00\n","Epoch 428, Loss 175127724.62, Val loss 12467460.00\n","Epoch 429, Loss 175499469.50, Val loss 12423227.00\n","Epoch 430, Loss 175990070.50, Val loss 12525476.00\n","Epoch 431, Loss 175005961.75, Val loss 12501127.00\n","Epoch 432, Loss 174549594.12, Val loss 12469848.00\n","Epoch 433, Loss 174585791.00, Val loss 12446716.00\n","Epoch 434, Loss 174512471.62, Val loss 12471853.00\n","Epoch 435, Loss 174280167.38, Val loss 12377025.00\n","Epoch 436, Loss 175278673.25, Val loss 12480907.00\n","Epoch 437, Loss 174097574.50, Val loss 12437451.00\n","Epoch 438, Loss 173786706.62, Val loss 12419396.00\n","Epoch 439, Loss 173812773.50, Val loss 12406268.00\n","Epoch 440, Loss 173679577.75, Val loss 12421683.00\n","Epoch 441, Loss 173584142.25, Val loss 12351555.00\n","Epoch 442, Loss 174381514.00, Val loss 12439429.00\n","Epoch 443, Loss 173326443.25, Val loss 12410044.00\n","Epoch 444, Loss 172967823.88, Val loss 12378415.00\n","Epoch 445, Loss 172993428.12, Val loss 12369090.00\n","Epoch 446, Loss 172900780.75, Val loss 12388259.00\n","Epoch 447, Loss 172857241.25, Val loss 12331817.00\n","Epoch 448, Loss 173478732.75, Val loss 12400788.00\n","Epoch 449, Loss 172509016.38, Val loss 12380511.00\n","Epoch 450, Loss 172199541.50, Val loss 12346743.00\n","Epoch 451, Loss 172226802.62, Val loss 12346456.00\n","Epoch 452, Loss 172129871.25, Val loss 12356733.00\n","Epoch 453, Loss 172180825.12, Val loss 12324001.00\n","Epoch 454, Loss 172758620.00, Val loss 12393275.00\n","Epoch 455, Loss 171610759.88, Val loss 12348774.00\n","Epoch 456, Loss 171434103.88, Val loss 12318007.00\n","Epoch 457, Loss 171589948.38, Val loss 12347341.00\n","Epoch 458, Loss 171372105.88, Val loss 12341525.00\n","Epoch 459, Loss 171308817.12, Val loss 12299411.00\n","Epoch 460, Loss 171921302.75, Val loss 12370193.00\n","Epoch 461, Loss 170916940.88, Val loss 12361631.00\n","Epoch 462, Loss 170712098.62, Val loss 12300165.00\n","Epoch 463, Loss 170814102.75, Val loss 12299020.00\n","Epoch 464, Loss 170952862.12, Val loss 12246525.00\n","Epoch 465, Loss 170664671.25, Val loss 12414287.00\n","Epoch 466, Loss 170529539.00, Val loss 12248588.00\n","Epoch 467, Loss 170306427.62, Val loss 12322329.00\n","Epoch 468, Loss 169942112.00, Val loss 12278423.00\n","Epoch 469, Loss 169819801.38, Val loss 12266995.00\n","Epoch 470, Loss 170502423.62, Val loss 12315542.00\n","Epoch 471, Loss 169568808.38, Val loss 12295986.00\n","Epoch 472, Loss 169434338.75, Val loss 12256290.00\n","Epoch 473, Loss 169552195.62, Val loss 12259331.00\n","Epoch 474, Loss 169992531.50, Val loss 12227177.00\n","Epoch 475, Loss 169718361.12, Val loss 12341673.00\n","Epoch 476, Loss 169197484.50, Val loss 12268308.00\n","Epoch 477, Loss 168841901.62, Val loss 12306462.00\n","Epoch 478, Loss 168997133.12, Val loss 12250132.00\n","Epoch 479, Loss 168732696.38, Val loss 12313788.00\n","Epoch 480, Loss 168618425.25, Val loss 12216744.00\n","Epoch 481, Loss 169107245.62, Val loss 12336597.00\n","Epoch 482, Loss 168469527.62, Val loss 12257666.00\n","Epoch 483, Loss 168156698.75, Val loss 12284995.00\n","Epoch 484, Loss 168299141.00, Val loss 12222640.00\n","Epoch 485, Loss 168389455.75, Val loss 12219424.00\n","Epoch 486, Loss 168877506.50, Val loss 12300495.00\n","Epoch 487, Loss 167916674.88, Val loss 12259676.00\n","Epoch 488, Loss 167811348.25, Val loss 12213711.00\n","Epoch 489, Loss 167501348.50, Val loss 12248860.00\n","Epoch 490, Loss 167768984.25, Val loss 12228738.00\n","Epoch 491, Loss 163710297.50, Val loss 12203138.00\n","Epoch 492, Loss 173069681.88, Val loss 12748230.00\n","Epoch 493, Loss 167035816.25, Val loss 12144874.00\n","Epoch 494, Loss 167528840.38, Val loss 12168256.00\n","Epoch 495, Loss 167294720.75, Val loss 12145710.00\n","Epoch 496, Loss 167869884.12, Val loss 12224638.00\n","Epoch 497, Loss 166957619.50, Val loss 12201232.00\n","Epoch 498, Loss 166901869.88, Val loss 12162652.00\n","Epoch 499, Loss 166842315.38, Val loss 12171664.00\n","Epoch 500, Loss 166760957.62, Val loss 12163138.00\n","Epoch 501, Loss 166295434.38, Val loss 12174891.00\n","Epoch 502, Loss 166849823.62, Val loss 12085265.00\n","Epoch 503, Loss 166633744.75, Val loss 12252694.00\n","Epoch 504, Loss 166289134.00, Val loss 12119712.00\n","Epoch 505, Loss 166115969.62, Val loss 12160272.00\n","Epoch 506, Loss 166082060.50, Val loss 12066917.00\n","Epoch 507, Loss 166552601.75, Val loss 12176868.00\n","Epoch 508, Loss 166005426.50, Val loss 12115384.00\n","Epoch 509, Loss 165614683.75, Val loss 12138818.00\n","Epoch 510, Loss 165759740.25, Val loss 12074740.00\n","Epoch 511, Loss 166004217.50, Val loss 12108539.00\n","Epoch 512, Loss 166023183.38, Val loss 12117518.00\n","Epoch 513, Loss 165362149.38, Val loss 12059643.00\n","Epoch 514, Loss 165503188.25, Val loss 12058376.00\n","Epoch 515, Loss 165140436.25, Val loss 12103070.00\n","Epoch 516, Loss 165540764.62, Val loss 12070367.00\n","Epoch 517, Loss 183804769.38, Val loss 12344797.00\n","Epoch 518, Loss 165559835.00, Val loss 12464871.00\n","Epoch 519, Loss 164009189.38, Val loss 12166726.00\n","Epoch 520, Loss 163923059.12, Val loss 12134313.00\n","Epoch 521, Loss 163683321.12, Val loss 12154297.00\n","Epoch 522, Loss 164078960.38, Val loss 12115708.00\n","Epoch 523, Loss 164020516.62, Val loss 12029601.00\n","Epoch 524, Loss 164212787.38, Val loss 12051501.00\n","Epoch 525, Loss 163785033.88, Val loss 12079998.00\n","Epoch 526, Loss 164163411.38, Val loss 12000953.00\n","Epoch 527, Loss 164054688.38, Val loss 12058138.00\n","Epoch 528, Loss 164231978.38, Val loss 12001489.00\n","Epoch 529, Loss 163695516.62, Val loss 12090308.00\n","Epoch 530, Loss 166155673.50, Val loss 12200029.00\n","Epoch 531, Loss 163512215.88, Val loss 12033499.00\n","Epoch 532, Loss 163806937.25, Val loss 11961759.00\n","Epoch 533, Loss 163641116.12, Val loss 12017627.00\n","Epoch 534, Loss 163521183.50, Val loss 11980892.00\n","Epoch 535, Loss 163453950.50, Val loss 11941381.00\n","Epoch 536, Loss 163498501.00, Val loss 11888686.00\n","Epoch 537, Loss 163851010.75, Val loss 11986402.00\n","Epoch 538, Loss 163156963.75, Val loss 11891434.00\n","Epoch 539, Loss 163157980.88, Val loss 11927179.00\n","Epoch 540, Loss 163351715.00, Val loss 11901991.00\n","Epoch 541, Loss 163034756.25, Val loss 11880935.00\n","Epoch 542, Loss 163562595.12, Val loss 11903241.00\n","Epoch 543, Loss 163092246.00, Val loss 11954089.00\n","Epoch 544, Loss 162892893.12, Val loss 11897223.00\n","Epoch 545, Loss 162867728.50, Val loss 11975872.00\n","Epoch 546, Loss 162373528.62, Val loss 11832424.00\n","Epoch 547, Loss 162729213.12, Val loss 11791255.00\n","Epoch 548, Loss 163309565.12, Val loss 11902052.00\n","Epoch 549, Loss 162445235.62, Val loss 11901716.00\n","Epoch 550, Loss 162543164.38, Val loss 11865734.00\n","Epoch 551, Loss 162355559.88, Val loss 11847516.00\n","Epoch 552, Loss 162553930.62, Val loss 11826720.00\n","Epoch 553, Loss 162027189.88, Val loss 11819117.00\n","Epoch 554, Loss 162733575.88, Val loss 11897225.00\n","Epoch 555, Loss 161588415.88, Val loss 11813922.00\n","Epoch 556, Loss 162206446.25, Val loss 11729386.00\n","Epoch 557, Loss 161871372.25, Val loss 11804856.00\n","Epoch 558, Loss 162305779.62, Val loss 11761396.00\n","Epoch 559, Loss 162115084.75, Val loss 11841302.00\n","Epoch 560, Loss 161754067.12, Val loss 11769238.00\n","Epoch 561, Loss 161773190.88, Val loss 11836101.00\n","Epoch 562, Loss 161727707.12, Val loss 11745628.00\n","Epoch 563, Loss 161620267.62, Val loss 11727901.00\n","Epoch 564, Loss 162010444.12, Val loss 11800711.00\n","Epoch 565, Loss 161618628.50, Val loss 11817116.00\n","Epoch 566, Loss 161310051.25, Val loss 11735637.00\n","Epoch 567, Loss 161582355.50, Val loss 11760349.00\n","Epoch 568, Loss 161040218.00, Val loss 11718449.00\n","Epoch 569, Loss 161324987.00, Val loss 11653351.00\n","Epoch 570, Loss 161597649.50, Val loss 11740332.00\n","Epoch 571, Loss 161051508.62, Val loss 11741531.00\n","Epoch 572, Loss 161248688.00, Val loss 11754710.00\n","Epoch 573, Loss 161068275.00, Val loss 11737693.00\n","Epoch 574, Loss 161291348.62, Val loss 11732765.00\n","Epoch 575, Loss 160699505.88, Val loss 11734737.00\n","==================================================================\n","Saved best model\n","Epoch 576, Loss 161005189.38, Val loss 11626992.00\n","Epoch 577, Loss 160985733.12, Val loss 11729751.00\n","Epoch 578, Loss 160770374.38, Val loss 11693191.00\n","Epoch 579, Loss 160475957.38, Val loss 11746097.00\n","Epoch 580, Loss 160847160.62, Val loss 11679143.00\n","Epoch 581, Loss 160564073.75, Val loss 11733986.00\n","==================================================================\n","Saved best model\n","Epoch 582, Loss 160515066.38, Val loss 11620033.00\n","Epoch 583, Loss 160416506.00, Val loss 11627290.00\n","Epoch 584, Loss 161088551.75, Val loss 11721475.00\n","Epoch 585, Loss 160197223.00, Val loss 11705123.00\n","Epoch 586, Loss 160406639.75, Val loss 11680316.00\n","Epoch 587, Loss 160350310.75, Val loss 11726374.00\n","Epoch 588, Loss 160342324.12, Val loss 11666410.00\n","Epoch 589, Loss 159962596.88, Val loss 11685515.00\n","==================================================================\n","Saved best model\n","Epoch 590, Loss 160126590.75, Val loss 11591559.00\n","Epoch 591, Loss 160194563.12, Val loss 11637919.00\n","Epoch 592, Loss 160600508.12, Val loss 11638460.00\n","Epoch 593, Loss 160334239.50, Val loss 11682254.00\n","Epoch 594, Loss 159857971.88, Val loss 11628901.00\n","Epoch 595, Loss 160043690.62, Val loss 11663385.00\n","Epoch 596, Loss 159759232.25, Val loss 11666059.00\n","Epoch 597, Loss 159583933.75, Val loss 11651819.00\n","Epoch 598, Loss 159833672.75, Val loss 11616721.00\n","==================================================================\n","Saved best model\n","Epoch 599, Loss 159679743.38, Val loss 11566039.00\n","Epoch 600, Loss 159602630.88, Val loss 11591358.00\n","Epoch 601, Loss 159215166.38, Val loss 11574210.00\n","Epoch 602, Loss 159961531.25, Val loss 11655593.00\n","Epoch 603, Loss 159141687.50, Val loss 11604694.00\n","Epoch 604, Loss 159804319.50, Val loss 11613550.00\n","Epoch 605, Loss 159364725.25, Val loss 11595752.00\n","==================================================================\n","Saved best model\n","Epoch 606, Loss 159183877.62, Val loss 11522812.00\n","Epoch 607, Loss 159376649.25, Val loss 11582863.00\n","==================================================================\n","Saved best model\n","Epoch 608, Loss 159021040.00, Val loss 11521124.00\n","Epoch 609, Loss 159259843.62, Val loss 11566542.00\n","==================================================================\n","Saved best model\n","Epoch 610, Loss 159364220.50, Val loss 11469323.00\n","Epoch 611, Loss 159321296.88, Val loss 11576577.00\n","Epoch 612, Loss 159185184.12, Val loss 11581179.00\n","Epoch 613, Loss 158789094.38, Val loss 11577717.00\n","Epoch 614, Loss 159004155.38, Val loss 11512434.00\n","Epoch 615, Loss 158734722.12, Val loss 11486065.00\n","Epoch 616, Loss 158955357.75, Val loss 11552247.00\n","Epoch 617, Loss 158801727.12, Val loss 11511763.00\n","Epoch 618, Loss 158815570.12, Val loss 11577355.00\n","Epoch 619, Loss 158806730.62, Val loss 11549512.00\n","Epoch 620, Loss 158465772.25, Val loss 11504795.00\n","==================================================================\n","Saved best model\n","Epoch 621, Loss 158452055.75, Val loss 11464890.00\n","Epoch 622, Loss 159017198.50, Val loss 11595702.00\n","Epoch 623, Loss 158404739.50, Val loss 11551000.00\n","Epoch 624, Loss 158220677.88, Val loss 11473299.00\n","Epoch 625, Loss 158456882.12, Val loss 11540543.00\n","==================================================================\n","Saved best model\n","Epoch 626, Loss 158620007.75, Val loss 11449450.00\n","Epoch 627, Loss 158589419.12, Val loss 11548436.00\n","Epoch 628, Loss 158042503.25, Val loss 11490945.00\n","Epoch 629, Loss 158554815.12, Val loss 11528003.00\n","Epoch 630, Loss 158102617.25, Val loss 11550247.00\n","Epoch 631, Loss 158052035.62, Val loss 11480986.00\n","Epoch 632, Loss 157896350.38, Val loss 11492586.00\n","Epoch 633, Loss 158449890.88, Val loss 11520642.00\n","Epoch 634, Loss 157938327.75, Val loss 11536295.00\n","==================================================================\n","Saved best model\n","Epoch 635, Loss 157920864.88, Val loss 11437089.00\n","Epoch 636, Loss 157853210.38, Val loss 11533074.00\n","==================================================================\n","Saved best model\n","Epoch 637, Loss 157972998.75, Val loss 11430958.00\n","Epoch 638, Loss 157943477.50, Val loss 11526564.00\n","Epoch 639, Loss 157754780.00, Val loss 11459067.00\n","Epoch 640, Loss 157904412.88, Val loss 11496544.00\n","Epoch 641, Loss 157325341.62, Val loss 11464136.00\n","Epoch 642, Loss 157838280.25, Val loss 11533643.00\n","Epoch 643, Loss 157692661.75, Val loss 11496825.00\n","Epoch 644, Loss 157088641.88, Val loss 11485053.00\n","Epoch 645, Loss 157760611.50, Val loss 11501202.00\n","Epoch 646, Loss 157710886.75, Val loss 11442936.00\n","Epoch 647, Loss 157232566.25, Val loss 11547922.00\n","Epoch 648, Loss 157297924.62, Val loss 11461439.00\n","Epoch 649, Loss 156936555.62, Val loss 11486251.00\n","Epoch 650, Loss 157288052.88, Val loss 11444639.00\n","==================================================================\n","Saved best model\n","Epoch 651, Loss 157391228.25, Val loss 11402161.00\n","Epoch 652, Loss 157298097.12, Val loss 11533850.00\n","Epoch 653, Loss 157034690.12, Val loss 11441991.00\n","Epoch 654, Loss 157121156.38, Val loss 11432260.00\n","Epoch 655, Loss 156982978.88, Val loss 11463977.00\n","Epoch 656, Loss 156970650.12, Val loss 11432045.00\n","Epoch 657, Loss 156656526.62, Val loss 11437459.00\n","Epoch 658, Loss 157287028.12, Val loss 11488314.00\n","Epoch 659, Loss 156639143.62, Val loss 11493901.00\n","Epoch 660, Loss 156695874.88, Val loss 11510802.00\n","Epoch 661, Loss 156304521.38, Val loss 11550355.00\n","Epoch 662, Loss 156262233.75, Val loss 11503285.00\n","Epoch 663, Loss 156076221.88, Val loss 11500376.00\n","Epoch 664, Loss 156226008.88, Val loss 11451360.00\n","Epoch 665, Loss 156167862.75, Val loss 11529170.00\n","Epoch 666, Loss 155805011.12, Val loss 11468853.00\n","Epoch 667, Loss 156605947.50, Val loss 11547036.00\n","Epoch 668, Loss 156094813.12, Val loss 11529754.00\n","Epoch 669, Loss 155821179.12, Val loss 11489705.00\n","Epoch 670, Loss 155708051.12, Val loss 11489482.00\n","Epoch 671, Loss 156360883.00, Val loss 11439935.00\n","Epoch 672, Loss 157491431.75, Val loss 11504934.00\n","Epoch 673, Loss 156285458.25, Val loss 11507305.00\n","Epoch 674, Loss 184183290.12, Val loss 13274763.00\n","==================================================================\n","Saved best model\n","Epoch 675, Loss 162931300.75, Val loss 11391684.00\n","==================================================================\n","Saved best model\n","Epoch 676, Loss 160405147.25, Val loss 11183777.00\n","Epoch 677, Loss 158309473.50, Val loss 11448690.00\n","Epoch 678, Loss 156972673.88, Val loss 11483263.00\n","Epoch 679, Loss 156711272.00, Val loss 11504602.00\n","Epoch 680, Loss 156261877.38, Val loss 11451392.00\n","Epoch 681, Loss 156503683.38, Val loss 11451888.00\n","Epoch 682, Loss 156379357.38, Val loss 11450242.00\n","Epoch 683, Loss 156106372.38, Val loss 11447250.00\n","Epoch 684, Loss 156069097.00, Val loss 11462060.00\n","Epoch 685, Loss 156296583.75, Val loss 11428482.00\n","Epoch 686, Loss 155965685.88, Val loss 11444468.00\n","Epoch 687, Loss 155995260.75, Val loss 11447068.00\n","Epoch 688, Loss 156244393.38, Val loss 11396524.00\n","Epoch 689, Loss 156144597.25, Val loss 11460680.00\n","Epoch 690, Loss 156191920.25, Val loss 11450120.00\n","Epoch 691, Loss 155982828.75, Val loss 11445282.00\n","Epoch 692, Loss 155973521.00, Val loss 11449118.00\n","Epoch 693, Loss 155487787.25, Val loss 11441973.00\n","Epoch 694, Loss 155567991.62, Val loss 11438820.00\n","Epoch 695, Loss 155623597.88, Val loss 11422285.00\n","Epoch 696, Loss 155503729.25, Val loss 11426893.00\n","Epoch 697, Loss 155765906.00, Val loss 11434019.00\n","Epoch 698, Loss 155782223.50, Val loss 11392634.00\n","Epoch 699, Loss 155767608.62, Val loss 11503306.00\n","Epoch 700, Loss 155043700.38, Val loss 11420339.00\n","Epoch 701, Loss 155266880.75, Val loss 11428839.00\n","Epoch 702, Loss 155234661.62, Val loss 11418526.00\n","Epoch 703, Loss 155125076.88, Val loss 11416457.00\n","Epoch 704, Loss 155249623.50, Val loss 11409307.00\n","Epoch 705, Loss 155627675.12, Val loss 11361571.00\n","Epoch 706, Loss 155434683.75, Val loss 11469643.00\n","Epoch 707, Loss 155082389.25, Val loss 11437921.00\n","Epoch 708, Loss 154981487.25, Val loss 11414261.00\n","Epoch 709, Loss 154876261.12, Val loss 11428003.00\n","Epoch 710, Loss 155100261.38, Val loss 11391388.00\n","Epoch 711, Loss 155211984.62, Val loss 11431655.00\n","Epoch 712, Loss 154874791.50, Val loss 11476941.00\n","Epoch 713, Loss 154301569.75, Val loss 11409850.00\n","Epoch 714, Loss 154847722.62, Val loss 11403688.00\n","Epoch 715, Loss 154961670.00, Val loss 11392029.00\n","Epoch 716, Loss 154491393.75, Val loss 11412080.00\n","Epoch 717, Loss 154862554.62, Val loss 11346813.00\n","Epoch 718, Loss 154768774.00, Val loss 11427669.00\n","Epoch 719, Loss 154546885.50, Val loss 11425027.00\n","Epoch 720, Loss 154503982.50, Val loss 11471640.00\n","Epoch 721, Loss 154703137.88, Val loss 11413644.00\n","Epoch 722, Loss 154601148.38, Val loss 11471302.00\n","Epoch 723, Loss 154102900.50, Val loss 11421284.00\n","Epoch 724, Loss 154155337.25, Val loss 11435481.00\n","Epoch 725, Loss 154036336.62, Val loss 11379560.00\n","Epoch 726, Loss 154420982.12, Val loss 11378366.00\n","Epoch 727, Loss 154422830.75, Val loss 11364120.00\n","Epoch 728, Loss 154419104.50, Val loss 11469034.00\n","Epoch 729, Loss 153911445.00, Val loss 11424102.00\n","Epoch 730, Loss 153666624.75, Val loss 11376116.00\n","Epoch 731, Loss 153854994.00, Val loss 11381370.00\n","Epoch 732, Loss 154403267.88, Val loss 11331160.00\n","Epoch 733, Loss 154142205.00, Val loss 11404356.00\n","Epoch 734, Loss 153884858.12, Val loss 11384854.00\n","Epoch 735, Loss 154079480.25, Val loss 11396750.00\n","Epoch 736, Loss 153917266.75, Val loss 11386084.00\n","Epoch 737, Loss 153466594.00, Val loss 11377948.00\n","Epoch 738, Loss 153490330.88, Val loss 11379569.00\n","Epoch 739, Loss 153830907.25, Val loss 11360088.00\n","Epoch 740, Loss 153979575.50, Val loss 11391148.00\n","Epoch 741, Loss 153489934.12, Val loss 11426370.00\n","Epoch 742, Loss 153254173.25, Val loss 11426336.00\n","Epoch 743, Loss 153366407.50, Val loss 11364843.00\n","Epoch 744, Loss 153578556.12, Val loss 11356075.00\n","Epoch 745, Loss 153442183.62, Val loss 11359590.00\n","Epoch 746, Loss 153796637.75, Val loss 11388532.00\n","Epoch 747, Loss 153437341.88, Val loss 11398640.00\n","Epoch 748, Loss 153173886.00, Val loss 11438739.00\n","Epoch 749, Loss 152931125.62, Val loss 11353688.00\n","Epoch 750, Loss 153351068.25, Val loss 11316610.00\n","Epoch 751, Loss 153263038.25, Val loss 11386037.00\n","Epoch 752, Loss 152850485.50, Val loss 11415736.00\n","Epoch 753, Loss 153459404.25, Val loss 11429502.00\n","Epoch 754, Loss 152906264.88, Val loss 11319282.00\n","Epoch 755, Loss 153108291.25, Val loss 11351783.00\n","Epoch 756, Loss 153538486.75, Val loss 11319515.00\n","Epoch 757, Loss 153130587.75, Val loss 11435843.00\n","Epoch 758, Loss 152663034.00, Val loss 11400696.00\n","Epoch 759, Loss 152910980.88, Val loss 11379119.00\n","Epoch 760, Loss 152831938.88, Val loss 11391527.00\n","Epoch 761, Loss 152719296.12, Val loss 11392374.00\n","Epoch 762, Loss 152994836.12, Val loss 11319727.00\n","Epoch 763, Loss 153154552.62, Val loss 11332809.00\n","Epoch 764, Loss 152679328.88, Val loss 11408563.00\n","Epoch 765, Loss 153045416.75, Val loss 11311711.00\n","Epoch 766, Loss 152743156.62, Val loss 11422670.00\n","Epoch 767, Loss 152276647.88, Val loss 11396062.00\n","Epoch 768, Loss 152457611.88, Val loss 11401985.00\n","Epoch 769, Loss 152306789.88, Val loss 11375460.00\n","Epoch 770, Loss 152264915.62, Val loss 11375951.00\n","Epoch 771, Loss 152617098.38, Val loss 11361480.00\n","Epoch 772, Loss 152213586.62, Val loss 11420966.00\n","Epoch 773, Loss 151968108.00, Val loss 11401355.00\n","Epoch 774, Loss 152173311.38, Val loss 11352265.00\n","Epoch 775, Loss 152580979.88, Val loss 11347657.00\n","Epoch 776, Loss 152099400.25, Val loss 11391707.00\n","Epoch 777, Loss 152741597.25, Val loss 11442129.00\n","Epoch 778, Loss 151865702.88, Val loss 11294040.00\n","Epoch 779, Loss 152515679.75, Val loss 11323871.00\n","Epoch 780, Loss 152417863.88, Val loss 11358444.00\n","Epoch 781, Loss 152125110.25, Val loss 11401504.00\n","Epoch 782, Loss 152190467.00, Val loss 11333669.00\n","Epoch 783, Loss 152024435.50, Val loss 11307180.00\n","Epoch 784, Loss 152059234.50, Val loss 11353244.00\n","Epoch 785, Loss 152562317.25, Val loss 11282888.00\n","Epoch 786, Loss 152057042.00, Val loss 11404182.00\n","Epoch 787, Loss 151916857.00, Val loss 11319543.00\n","Epoch 788, Loss 151630573.12, Val loss 11336617.00\n","Epoch 789, Loss 151790356.62, Val loss 11322737.00\n","Epoch 790, Loss 151959100.38, Val loss 11308107.00\n","Epoch 791, Loss 151619465.88, Val loss 11341439.00\n","Epoch 792, Loss 151802073.62, Val loss 11290675.00\n","Epoch 793, Loss 153470052.00, Val loss 11307239.00\n","Epoch 794, Loss 152486250.12, Val loss 11306585.00\n","Epoch 795, Loss 151780526.00, Val loss 11352565.00\n","Epoch 796, Loss 151835402.75, Val loss 11348077.00\n","Epoch 797, Loss 151621381.25, Val loss 11411747.00\n","Epoch 798, Loss 151321318.25, Val loss 11367692.00\n","Epoch 799, Loss 151251631.12, Val loss 11317692.00\n","Epoch 800, Loss 151388324.88, Val loss 11329961.00\n","Epoch 801, Loss 151471379.88, Val loss 11288210.00\n","Epoch 802, Loss 151667537.25, Val loss 11343657.00\n","Epoch 803, Loss 151530349.00, Val loss 11296483.00\n","Epoch 804, Loss 151395215.12, Val loss 11325306.00\n","Epoch 805, Loss 151489471.00, Val loss 11348035.00\n","Epoch 806, Loss 151276666.88, Val loss 11362900.00\n","Epoch 807, Loss 150757804.50, Val loss 11320971.00\n","Epoch 808, Loss 151193911.00, Val loss 11272397.00\n","Epoch 809, Loss 151190521.12, Val loss 11309817.00\n","Epoch 810, Loss 150924916.12, Val loss 11293788.00\n","Epoch 811, Loss 151154203.62, Val loss 11301669.00\n","Epoch 812, Loss 151327242.62, Val loss 11266901.00\n","Epoch 813, Loss 151244666.75, Val loss 11308401.00\n","Epoch 814, Loss 150916699.00, Val loss 11308990.00\n","Epoch 815, Loss 151336474.62, Val loss 11305251.00\n","Epoch 816, Loss 150873434.00, Val loss 11380950.00\n","Epoch 817, Loss 150671661.25, Val loss 11345781.00\n","Epoch 818, Loss 150577218.38, Val loss 11292321.00\n","Epoch 819, Loss 150743108.38, Val loss 11276149.00\n","Epoch 820, Loss 150840120.75, Val loss 11287463.00\n","Epoch 821, Loss 151062254.75, Val loss 11306987.00\n","Epoch 822, Loss 150832669.88, Val loss 11341877.00\n","Epoch 823, Loss 150529090.00, Val loss 11348354.00\n","Epoch 824, Loss 150434770.75, Val loss 11343859.00\n","Epoch 825, Loss 150753630.38, Val loss 11259645.00\n","Epoch 826, Loss 150771515.62, Val loss 11313835.00\n","Epoch 827, Loss 150364037.38, Val loss 11273346.00\n","Epoch 828, Loss 150355094.00, Val loss 11274571.00\n","Epoch 829, Loss 150641887.62, Val loss 11270819.00\n","Epoch 830, Loss 150841686.12, Val loss 11295577.00\n","Epoch 831, Loss 150506440.75, Val loss 11345919.00\n","Epoch 832, Loss 150293536.88, Val loss 11324547.00\n","Epoch 833, Loss 150112835.88, Val loss 11333014.00\n","Epoch 834, Loss 150179028.62, Val loss 11310448.00\n","Epoch 835, Loss 150549482.88, Val loss 11305828.00\n","Epoch 836, Loss 150110447.88, Val loss 11318118.00\n","Epoch 837, Loss 150018106.00, Val loss 11340061.00\n","Epoch 838, Loss 150099615.12, Val loss 11293162.00\n","Epoch 839, Loss 149714125.50, Val loss 11267474.00\n","Epoch 840, Loss 150023609.50, Val loss 11243071.00\n","Epoch 841, Loss 150319316.00, Val loss 11217138.00\n","Epoch 842, Loss 150605465.62, Val loss 11263064.00\n","Epoch 843, Loss 150343774.25, Val loss 11266796.00\n","Epoch 844, Loss 149978161.88, Val loss 11332692.00\n","Epoch 845, Loss 149655999.88, Val loss 11318076.00\n","Epoch 846, Loss 149672410.38, Val loss 11280313.00\n","Epoch 847, Loss 149830874.25, Val loss 11309173.00\n","Epoch 848, Loss 149804343.88, Val loss 11297973.00\n","Epoch 849, Loss 149685566.50, Val loss 11305526.00\n","Epoch 850, Loss 150055778.50, Val loss 11215050.00\n","Epoch 851, Loss 149862105.00, Val loss 11234794.00\n","Epoch 852, Loss 150061496.00, Val loss 11315669.00\n","Epoch 853, Loss 149866769.00, Val loss 11248132.00\n","Epoch 854, Loss 149557211.25, Val loss 11341638.00\n","Epoch 855, Loss 149361022.00, Val loss 11298347.00\n","Epoch 856, Loss 149171471.25, Val loss 11245974.00\n","Epoch 857, Loss 149491341.62, Val loss 11223166.00\n","Epoch 858, Loss 148725420.88, Val loss 11510335.00\n","Epoch 859, Loss 150346349.25, Val loss 11459529.00\n","==================================================================\n","Saved best model\n","Epoch 860, Loss 143955991.38, Val loss 11119578.00\n","==================================================================\n","Saved best model\n","Epoch 861, Loss 154627051.50, Val loss 10998695.00\n","Epoch 862, Loss 153399374.88, Val loss 11246795.00\n","Epoch 863, Loss 150197245.62, Val loss 11311588.00\n","Epoch 864, Loss 149156303.25, Val loss 11276052.00\n","Epoch 865, Loss 149225090.25, Val loss 11240785.00\n","Epoch 866, Loss 149169854.50, Val loss 11282947.00\n","Epoch 867, Loss 149085966.00, Val loss 11262297.00\n","Epoch 868, Loss 149089631.88, Val loss 11279615.00\n","Epoch 869, Loss 149075920.75, Val loss 11264752.00\n","Epoch 870, Loss 149171490.38, Val loss 11254135.00\n","Epoch 871, Loss 149378742.50, Val loss 11263189.00\n","Epoch 872, Loss 148991249.00, Val loss 11283763.00\n","Epoch 873, Loss 148801238.50, Val loss 11290540.00\n","Epoch 874, Loss 148882747.25, Val loss 11264228.00\n","Epoch 875, Loss 148826134.00, Val loss 11271081.00\n","Epoch 876, Loss 148779049.50, Val loss 11250400.00\n","Epoch 877, Loss 148514260.88, Val loss 11432663.00\n","Epoch 878, Loss 150006868.25, Val loss 11341158.00\n","Epoch 879, Loss 149139582.12, Val loss 11185317.00\n","Epoch 880, Loss 149027096.62, Val loss 11212138.00\n","Epoch 881, Loss 148522062.62, Val loss 11262141.00\n","Epoch 882, Loss 148200366.75, Val loss 11212618.00\n","Epoch 883, Loss 148146863.88, Val loss 11223219.00\n","Epoch 884, Loss 148561461.75, Val loss 11204142.00\n","Epoch 885, Loss 148358924.38, Val loss 11278259.00\n","Epoch 886, Loss 147977391.50, Val loss 11312446.00\n","Epoch 887, Loss 147879238.00, Val loss 11276052.00\n","Epoch 888, Loss 148094479.75, Val loss 11295613.00\n","Epoch 889, Loss 147906327.88, Val loss 11276272.00\n","Epoch 890, Loss 148013526.00, Val loss 11286344.00\n","Epoch 891, Loss 148196495.25, Val loss 11241979.00\n","Epoch 892, Loss 148174316.38, Val loss 11231823.00\n","Epoch 893, Loss 148401671.38, Val loss 11255967.00\n","Epoch 894, Loss 147858364.38, Val loss 11307436.00\n","Epoch 895, Loss 147578618.50, Val loss 11284723.00\n","Epoch 896, Loss 147709863.00, Val loss 11278795.00\n","Epoch 897, Loss 147598187.00, Val loss 11283453.00\n","Epoch 898, Loss 147715882.88, Val loss 11272017.00\n","Epoch 899, Loss 147970204.62, Val loss 11223808.00\n","Epoch 900, Loss 147945722.50, Val loss 11230334.00\n","Epoch 901, Loss 147878966.25, Val loss 11299215.00\n","Epoch 902, Loss 147208601.50, Val loss 11231670.00\n","Epoch 903, Loss 147659594.12, Val loss 11204901.00\n","Epoch 904, Loss 148247500.88, Val loss 11188802.00\n","Epoch 905, Loss 147178169.62, Val loss 11505308.00\n","Epoch 906, Loss 148330241.00, Val loss 11299563.00\n","Epoch 907, Loss 147787994.38, Val loss 11122185.00\n","Epoch 908, Loss 147636491.62, Val loss 11238298.00\n","Epoch 909, Loss 147400567.88, Val loss 11225584.00\n","Epoch 910, Loss 147970405.88, Val loss 11233847.00\n","Epoch 911, Loss 147128050.50, Val loss 11302098.00\n","Epoch 912, Loss 147448819.88, Val loss 11252631.00\n","Epoch 913, Loss 147258219.25, Val loss 11271907.00\n","Epoch 914, Loss 146801116.50, Val loss 11232333.00\n","Epoch 915, Loss 147351040.50, Val loss 11189906.00\n","Epoch 916, Loss 147784174.38, Val loss 11192944.00\n","Epoch 917, Loss 147598641.38, Val loss 11261267.00\n","Epoch 918, Loss 147061616.50, Val loss 11224428.00\n","Epoch 919, Loss 147005109.38, Val loss 11200507.00\n","Epoch 920, Loss 147107710.12, Val loss 11214394.00\n","Epoch 921, Loss 147096309.50, Val loss 11195897.00\n","Epoch 922, Loss 147866338.00, Val loss 11199288.00\n","Epoch 923, Loss 146950076.25, Val loss 11267257.00\n","Epoch 924, Loss 146294378.12, Val loss 11447497.00\n","Epoch 925, Loss 147853923.88, Val loss 11307574.00\n","Epoch 926, Loss 147232478.62, Val loss 11145003.00\n","Epoch 927, Loss 147315081.75, Val loss 11205656.00\n","Epoch 928, Loss 147270154.25, Val loss 11254269.00\n","Epoch 929, Loss 146483998.88, Val loss 11215405.00\n","Epoch 930, Loss 146633624.38, Val loss 11195565.00\n","Epoch 931, Loss 147090785.38, Val loss 11154834.00\n","Epoch 932, Loss 147868563.50, Val loss 11253264.00\n","Epoch 933, Loss 146910050.50, Val loss 11287787.00\n","Epoch 934, Loss 146277272.88, Val loss 11214790.00\n","Epoch 935, Loss 146486698.62, Val loss 11189428.00\n","Epoch 936, Loss 147008514.50, Val loss 11152843.00\n","Epoch 937, Loss 147460077.62, Val loss 11283917.00\n","Epoch 938, Loss 146369093.75, Val loss 11268096.00\n","Epoch 939, Loss 146558645.88, Val loss 11213930.00\n","Epoch 940, Loss 146469287.25, Val loss 11233008.00\n","Epoch 941, Loss 146448374.38, Val loss 11192916.00\n","Epoch 942, Loss 146847560.75, Val loss 11156649.00\n","Epoch 943, Loss 147230225.62, Val loss 11258484.00\n","Epoch 944, Loss 146754194.50, Val loss 11309593.00\n","Epoch 945, Loss 145907271.88, Val loss 11185689.00\n","Epoch 946, Loss 146703443.12, Val loss 11224422.00\n","Epoch 947, Loss 146365482.25, Val loss 11171445.00\n","Epoch 948, Loss 146535958.88, Val loss 11192933.00\n","Epoch 949, Loss 146885708.88, Val loss 11115789.00\n","Epoch 950, Loss 146677970.62, Val loss 11196593.00\n","Epoch 951, Loss 146335572.62, Val loss 11243844.00\n","Epoch 952, Loss 146207729.00, Val loss 11197642.00\n","Epoch 953, Loss 146294790.88, Val loss 11204697.00\n","Epoch 954, Loss 146178933.75, Val loss 11177473.00\n","Epoch 955, Loss 147150157.75, Val loss 11090311.00\n","Epoch 956, Loss 146816602.00, Val loss 11257113.00\n","Epoch 957, Loss 146114366.88, Val loss 11187204.00\n","Epoch 958, Loss 145874936.75, Val loss 11265556.00\n","Epoch 959, Loss 145788685.00, Val loss 11138061.00\n","Epoch 960, Loss 146837178.25, Val loss 11209055.00\n","Epoch 961, Loss 146929540.75, Val loss 11147825.00\n","Epoch 962, Loss 146604960.00, Val loss 11235031.00\n","Epoch 963, Loss 145989908.00, Val loss 11191280.00\n","Epoch 964, Loss 146017661.50, Val loss 11176541.00\n","Epoch 965, Loss 145975733.62, Val loss 11166108.00\n","Epoch 966, Loss 146510049.25, Val loss 11152552.00\n","Epoch 967, Loss 146162359.38, Val loss 11167232.00\n","Epoch 968, Loss 146184702.25, Val loss 11171600.00\n","Epoch 969, Loss 146268574.50, Val loss 11209951.00\n","Epoch 970, Loss 145804266.12, Val loss 11281782.00\n","Epoch 971, Loss 145300269.25, Val loss 11091343.00\n","Epoch 972, Loss 147099601.00, Val loss 11159253.00\n","Epoch 973, Loss 146099868.62, Val loss 11275804.00\n","Epoch 974, Loss 145305829.25, Val loss 11169319.00\n","Epoch 975, Loss 145664138.12, Val loss 11401423.00\n","Epoch 976, Loss 145092418.75, Val loss 11044993.00\n","Epoch 977, Loss 147481341.38, Val loss 11122173.00\n","Epoch 978, Loss 146119974.38, Val loss 11230972.00\n","Epoch 979, Loss 145336063.62, Val loss 11170146.00\n","Epoch 980, Loss 145847187.00, Val loss 11229569.00\n","Epoch 981, Loss 145617191.62, Val loss 11285041.00\n","Epoch 982, Loss 145118953.62, Val loss 11086167.00\n","Epoch 983, Loss 146385956.75, Val loss 11148124.00\n","Epoch 984, Loss 146528522.88, Val loss 11113282.00\n","Epoch 985, Loss 146085323.12, Val loss 11275699.00\n","Epoch 986, Loss 145584289.25, Val loss 11207444.00\n","Epoch 987, Loss 145420208.62, Val loss 11208653.00\n","Epoch 988, Loss 145312522.38, Val loss 11196777.00\n","Epoch 989, Loss 145458277.25, Val loss 11313184.00\n","==================================================================\n","Saved best model\n","Epoch 990, Loss 144478316.88, Val loss 10867967.00\n","Epoch 991, Loss 148149547.38, Val loss 11232697.00\n","Epoch 992, Loss 145655017.62, Val loss 11273049.00\n","Epoch 993, Loss 144892622.88, Val loss 11149118.00\n","Epoch 994, Loss 145585708.50, Val loss 11241259.00\n","Epoch 995, Loss 145248720.50, Val loss 11194090.00\n","Epoch 996, Loss 145318425.50, Val loss 11182859.00\n","Epoch 997, Loss 145600759.12, Val loss 11133758.00\n","Epoch 998, Loss 146015453.50, Val loss 11222563.00\n","Epoch 999, Loss 144667199.00, Val loss 11176859.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"status":"ok","timestamp":1646516146351,"user_tz":360,"elapsed":394,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"8d88e122-eabc-4bf6-b59e-c67fc3dbf5c8"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["261207632.0\n","tensor([2742.0830, 2987.5562, 3213.4766, 3454.1030, 3678.1370, 3885.0942,\n","        4092.6833, 4293.9497, 4509.0425, 4713.8262, 4931.6489, 5136.9985,\n","        5372.6748, 5599.5210, 5901.6528], grad_fn=<SelectBackward0>)\n","tensor([2908., 2871., 3187., 3222., 3421., 3617., 3844., 3908., 3987., 4006.,\n","        4123., 4544., 4193., 4722., 4866.])\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","     # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_number_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_number_infected_tensor':labeled_output # (52, 15)\n","}\n","\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)\n","\n","\n","# print(model_predictions_number_infected['Alabama'])\n","# print(ground_truth_number_infected['Alabama'])"],"metadata":{"id":"s3-8Yge6CAWM","executionInfo":{"status":"ok","timestamp":1646516760855,"user_tz":360,"elapsed":269,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":23,"outputs":[]}]}