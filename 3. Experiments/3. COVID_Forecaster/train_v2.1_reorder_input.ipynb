{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_reorder_input.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN2WkiC3q/ehUeQRuvrO3JX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1646442048629,"user_tz":360,"elapsed":1266,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"42606997-610e-4b5f-dfca-812ff0b04919"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2a25516f-00b9-4386-f3aa-0df65f78ca08","executionInfo":{"status":"ok","timestamp":1646442078090,"user_tz":360,"elapsed":27614,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.1.1)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (4.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.10.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: torch-geometric-temporal in /usr/local/lib/python3.7/dist-packages (0.51.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.12)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.3)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.6.3)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (0.4)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (3.13)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (0.1.8)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric->torch-geometric-temporal) (6.1.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric->torch-geometric-temporal) (4.11.2)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric->torch-geometric-temporal) (0.6.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric->torch-geometric-temporal) (57.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric->torch-geometric-temporal) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.3)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","PyTorch has version 1.10.0+cu111\n"]}],"source":["\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))"]},{"cell_type":"code","source":["\"\"\"\n","Import any other libraries we will use later on\n","\"\"\"\n","! pip install epiweeks\n","! pip install haversine\n","# from preprocess_data import get_preprocessed_data\n","from torch_geometric.data import Data"],"metadata":{"id":"BfNcl5hIj8ci","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646442091096,"user_tz":360,"elapsed":13016,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"ae577b5a-67b4-4f5a-d749-0a2f3bc52ec0"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Set device (CPU or GPU)\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"id":"uPbva_-KTSAt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646442116974,"user_tz":360,"elapsed":241,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"b2952f5f-1ea2-4f04-ab7a-b499ea307bd5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Get preprocessed data\n","\"\"\"\n","\n","import pickle\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open('./data/preprocessed_data.pickle', 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open('./data/preprocessed_data_reorder_input.pickle', 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","\n","\n"],"metadata":{"id":"oWdP4J6ONPLU","executionInfo":{"status":"ok","timestamp":1646442119292,"user_tz":360,"elapsed":570,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Unpack preprocessed data\n","\"\"\"\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_change_in_confirmed_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_change_in_confirmed_smoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_change_in_confirmed_smoothed']"],"metadata":{"id":"gVfNhDK6kzKl","executionInfo":{"status":"ok","timestamp":1646442121617,"user_tz":360,"elapsed":245,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3","executionInfo":{"status":"ok","timestamp":1646442124436,"user_tz":360,"elapsed":281,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","\"\"\"\n","# Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","\n","inputLayer_num_features = 24 \n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window\n","chebyshev_filter_size = 2\n","\n","class GCN(torch.nn.Module):\n","    # def __init__(self):\n","    #     super().__init__()\n","    #     self.conv1 = GCNConv(inputLayer_num_features, hiddenLayer1_num_features)\n","    #     self.conv2 = GCNConv(hiddenLayer1_num_features, outputLayer_num_features)\n","\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","\n","    # def forward(self, data):\n","    #     x, edge_index = data.x, data.edge_index\n","    #     x = self.conv1(x, edge_index)\n","    #     x = F.elu(x)\n","    #     x = self.conv2(x, edge_index)\n","    #     # print(x.shape)\n","    #     return x\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x)\n","      return x\n","\n","model = GCN().to(device)\n","\n","\"\"\"Predicting confirmed cases with UNSMOOTHED DATA\n","# optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 0, Loss 42769390666496.00, Val loss 1144893210624.00\n","# Epoch 1, Loss 43588523467264.00, Val loss 1144893210624.00\n","# Epoch 2, Loss 43588522479872.00, Val loss 1144893210624.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n","# ==================================================================\n","# Saved best model\n","# Epoch 0, Loss 42782691710720.00, Val loss 1145398493184.00\n","# Epoch 1, Loss 43682803331840.00, Val loss 1145398493184.00\n","# Epoch 2, Loss 43682803230208.00, Val loss 1145398493184.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.3)\n","# ==================================================================\n","# Saved best model\n","# Epoch 0, Loss 42842098256896.00, Val loss 1146748010496.00\n","# Epoch 1, Loss 44142974313472.00, Val loss 1146748010496.00\n","# Epoch 2, Loss 44142974349312.00, Val loss 1146748010496.00\n","# Epoch 3, Loss 44142974484992.00, Val loss 1146748010496.00\n","# Epoch 4, Loss 44142974195712.00, Val loss 1146748010496.00\n","# Epoch 5, Loss 44142974426624.00, Val loss 1146748010496.00\n","# Epoch 6, Loss 44142974675712.00, Val loss 1146748010496.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 30, Loss 45311937572864.00, Val loss 1152044630016.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","# Saved best model\n","# Epoch 11, Loss 52422146342912.00, Val loss 1338975584256.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=10) \n","# Saved best model\n","# Epoch 250, Loss 52617727680512.00, Val loss 1434736918528.00\n","# ==================================================================\n","\"\"\"\n","\n","\"\"\" Predicting CHANGE IN CONFIRMED CASES with UNSMOOTHED DATA\n","# optimizer = torch.optim.SGD(model.parameters(), lr=5)\n","# ==================================================================\n","# Saved best model\n","# Epoch 10, Loss 1406912358.97, Val loss 44110352.00\n","# Epoch 11, Loss 1406912362.75, Val loss 44111736.00\n","\n","## CURRENT ATTEMPT\n","# optimizer = torch.optim.SGD(model.parameters(), lr=3)\n","# ==================================================================\n","# Saved best model\n","# Epoch 1, Loss 9978198731645184.00, Val loss 4257833728.00\n","# Epoch 2, Loss 487194958592.00, Val loss 4257833728.00\n","# Epoch 3, Loss 487135717120.00, Val loss 4257833728.00\n","# Epoch 4, Loss 487123448320.00, Val loss 4257833728.00\n","# Epoch 5, Loss 487194958592.00, Val loss 4257833728.00\n","# Epoch 6, Loss 487194958592.00, Val loss 4257833728.00\n","# Epoch 7, Loss 487123672832.00, Val loss 4257833728.00\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 4, Loss 1254756897.00, Val loss 42590060.00\n","# Epoch 5, Loss 1254523141.75, Val loss 42590060.00\n","# Epoch 6, Loss 1254756964.66, Val loss 42590064.00\n","# Epoch 7, Loss 1254756966.41, Val loss 42590060.00\n","# Epoch 8, Loss 1254756963.06, Val loss 42590060.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","# ==================================================================\n","# Saved best model\n","# Epoch 249, Loss 1230884550.44, Val loss 39618176.00\n","# ==================================================================\n","# Saved best model\n","# Epoch 250, Loss 1229793268.94, Val loss 39596024.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 49, Loss 1303417711.12, Val loss 40663960.00\n","# Epoch 50, Loss 1307867300.75, Val loss 40700624.00\n","# Epoch 51, Loss 1307159946.75, Val loss 40732316.00\n","# Epoch 52, Loss 1306538857.00, Val loss 40759704.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 12, Loss 1340716037.00, Val loss 41348480.00\n","# Epoch 13, Loss 1338890136.38, Val loss 41497644.00\n","# Epoch 14, Loss 1339746923.50, Val loss 41397148.00\n","# Epoch 15, Loss 1340006091.38, Val loss 41366280.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=10)\n","# ==================================================================\n","# Saved best model\n","# Epoch 48, Loss 1410565205.31, Val loss 40933468.00\n","# Epoch 49, Loss 1410565144.25, Val loss 40933468.00\n","\"\"\"\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 2, Loss 844733180.70, Val loss 24130472.00\n","# Epoch 3, Loss 844733345.05, Val loss 24130472.00\n","# Epoch 4, Loss 844733344.17, Val loss 24130472.00\n","\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=10)\n","# ==================================================================\n","# Saved best model\n","# Epoch 0, Loss 981986268.34, Val loss 23840134.00\n","# Epoch 1, Loss 1002387839.06, Val loss 23864630.00\n","# Epoch 2, Loss 1001558686.19, Val loss 23866456.00\n","# Epoch 3, Loss 1001384564.81, Val loss 23867010.00\n","# Epoch 4, Loss 1001299320.44, Val loss 23867208.00\n","# Epoch 5, Loss 1001249199.88, Val loss 23867278.00\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=1)\n","# ==================================================================\n","# Saved best model\n","# Epoch 24, Loss 926364984.75, Val loss 24568306.00\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj","executionInfo":{"status":"ok","timestamp":1646442126700,"user_tz":360,"elapsed":251,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646442132256,"user_tz":360,"elapsed":250,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"d3d776bd-a18b-40b7-c59d-372e5b3c7f29"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GCN(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","model_path = './saved_models/smoothed_adam_reordered_input'\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(1000):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, model_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646443686889,"user_tz":360,"elapsed":1551308,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"c44bdb67-351e-4f8c-c511-98d577d56f92"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 1194949399.75, Val loss 32637034.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 1187986435.78, Val loss 32510926.00\n","==================================================================\n","Saved best model\n","Epoch 2, Loss 1181356996.88, Val loss 32387416.00\n","==================================================================\n","Saved best model\n","Epoch 3, Loss 1174874888.86, Val loss 32265838.00\n","==================================================================\n","Saved best model\n","Epoch 4, Loss 1168528898.04, Val loss 32146014.00\n","==================================================================\n","Saved best model\n","Epoch 5, Loss 1162313522.83, Val loss 32027856.00\n","==================================================================\n","Saved best model\n","Epoch 6, Loss 1156224711.57, Val loss 31911304.00\n","==================================================================\n","Saved best model\n","Epoch 7, Loss 1150259227.51, Val loss 31796318.00\n","==================================================================\n","Saved best model\n","Epoch 8, Loss 1144414200.52, Val loss 31682856.00\n","==================================================================\n","Saved best model\n","Epoch 9, Loss 1138686947.64, Val loss 31570888.00\n","==================================================================\n","Saved best model\n","Epoch 10, Loss 1133075064.46, Val loss 31460384.00\n","==================================================================\n","Saved best model\n","Epoch 11, Loss 1127576235.20, Val loss 31351314.00\n","==================================================================\n","Saved best model\n","Epoch 12, Loss 1122188253.90, Val loss 31243664.00\n","==================================================================\n","Saved best model\n","Epoch 13, Loss 1116909039.12, Val loss 31137398.00\n","==================================================================\n","Saved best model\n","Epoch 14, Loss 1111736617.76, Val loss 31032498.00\n","==================================================================\n","Saved best model\n","Epoch 15, Loss 1106669012.26, Val loss 30928944.00\n","==================================================================\n","Saved best model\n","Epoch 16, Loss 1101704455.85, Val loss 30826716.00\n","==================================================================\n","Saved best model\n","Epoch 17, Loss 1096841168.03, Val loss 30725794.00\n","==================================================================\n","Saved best model\n","Epoch 18, Loss 1092086242.72, Val loss 30628510.00\n","==================================================================\n","Saved best model\n","Epoch 19, Loss 1087380061.44, Val loss 30530058.00\n","==================================================================\n","Saved best model\n","Epoch 20, Loss 1082806160.15, Val loss 30435428.00\n","==================================================================\n","Saved best model\n","Epoch 21, Loss 1078256870.40, Val loss 30339372.00\n","==================================================================\n","Saved best model\n","Epoch 22, Loss 1073867976.24, Val loss 30247306.00\n","==================================================================\n","Saved best model\n","Epoch 23, Loss 1069620855.50, Val loss 30150566.00\n","==================================================================\n","Saved best model\n","Epoch 24, Loss 1065295691.09, Val loss 30063336.00\n","==================================================================\n","Saved best model\n","Epoch 25, Loss 1061225346.47, Val loss 29971584.00\n","==================================================================\n","Saved best model\n","Epoch 26, Loss 1057130079.73, Val loss 29877888.00\n","==================================================================\n","Saved best model\n","Epoch 27, Loss 1053009320.30, Val loss 29795418.00\n","==================================================================\n","Saved best model\n","Epoch 28, Loss 1048985769.44, Val loss 29707338.00\n","==================================================================\n","Saved best model\n","Epoch 29, Loss 1045143326.42, Val loss 29620372.00\n","==================================================================\n","Saved best model\n","Epoch 30, Loss 1041374995.03, Val loss 29535372.00\n","==================================================================\n","Saved best model\n","Epoch 31, Loss 1037639764.67, Val loss 29450044.00\n","==================================================================\n","Saved best model\n","Epoch 32, Loss 1033879754.39, Val loss 29366018.00\n","==================================================================\n","Saved best model\n","Epoch 33, Loss 1030365629.61, Val loss 29283118.00\n","==================================================================\n","Saved best model\n","Epoch 34, Loss 1026847669.31, Val loss 29201142.00\n","==================================================================\n","Saved best model\n","Epoch 35, Loss 1023733346.19, Val loss 29119848.00\n","==================================================================\n","Saved best model\n","Epoch 36, Loss 1019881795.12, Val loss 29039572.00\n","==================================================================\n","Saved best model\n","Epoch 37, Loss 1016750987.48, Val loss 28972074.00\n","==================================================================\n","Saved best model\n","Epoch 38, Loss 1013261295.22, Val loss 28894838.00\n","==================================================================\n","Saved best model\n","Epoch 39, Loss 1009826096.12, Val loss 28816946.00\n","==================================================================\n","Saved best model\n","Epoch 40, Loss 1007105800.16, Val loss 28748162.00\n","==================================================================\n","Saved best model\n","Epoch 41, Loss 1003155100.58, Val loss 28663944.00\n","==================================================================\n","Saved best model\n","Epoch 42, Loss 1000953114.69, Val loss 28590284.00\n","==================================================================\n","Saved best model\n","Epoch 43, Loss 996805111.05, Val loss 28520938.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 993096931.67, Val loss 28450842.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 991664999.98, Val loss 28378390.00\n","==================================================================\n","Saved best model\n","Epoch 46, Loss 987180350.89, Val loss 28299548.00\n","==================================================================\n","Saved best model\n","Epoch 47, Loss 984434165.41, Val loss 28228686.00\n","==================================================================\n","Saved best model\n","Epoch 48, Loss 981482513.53, Val loss 28159898.00\n","Epoch 49, Loss 982998755.59, Val loss 28461260.00\n","Epoch 50, Loss 983235501.50, Val loss 28389106.00\n","==================================================================\n","Saved best model\n","Epoch 51, Loss 982844284.56, Val loss 28127298.00\n","==================================================================\n","Saved best model\n","Epoch 52, Loss 974767182.27, Val loss 27877888.00\n","==================================================================\n","Saved best model\n","Epoch 53, Loss 968769644.88, Val loss 27812542.00\n","==================================================================\n","Saved best model\n","Epoch 54, Loss 965169159.28, Val loss 27746504.00\n","==================================================================\n","Saved best model\n","Epoch 55, Loss 965108043.19, Val loss 27680662.00\n","==================================================================\n","Saved best model\n","Epoch 56, Loss 960480981.53, Val loss 27611892.00\n","==================================================================\n","Saved best model\n","Epoch 57, Loss 957870025.19, Val loss 27548048.00\n","==================================================================\n","Saved best model\n","Epoch 58, Loss 955234569.03, Val loss 27483462.00\n","==================================================================\n","Saved best model\n","Epoch 59, Loss 952941112.09, Val loss 27422264.00\n","==================================================================\n","Saved best model\n","Epoch 60, Loss 950511337.19, Val loss 27360308.00\n","==================================================================\n","Saved best model\n","Epoch 61, Loss 948146270.53, Val loss 27295808.00\n","==================================================================\n","Saved best model\n","Epoch 62, Loss 947504389.50, Val loss 27234034.00\n","==================================================================\n","Saved best model\n","Epoch 63, Loss 943922879.78, Val loss 27174818.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 941668732.03, Val loss 27122804.00\n","==================================================================\n","Saved best model\n","Epoch 65, Loss 939616059.88, Val loss 27063190.00\n","==================================================================\n","Saved best model\n","Epoch 66, Loss 937613169.84, Val loss 27007120.00\n","==================================================================\n","Saved best model\n","Epoch 67, Loss 935516085.66, Val loss 26945854.00\n","==================================================================\n","Saved best model\n","Epoch 68, Loss 933654741.97, Val loss 26892340.00\n","==================================================================\n","Saved best model\n","Epoch 69, Loss 931736873.91, Val loss 26835848.00\n","==================================================================\n","Saved best model\n","Epoch 70, Loss 930126673.31, Val loss 26786168.00\n","==================================================================\n","Saved best model\n","Epoch 71, Loss 930524338.09, Val loss 26725080.00\n","==================================================================\n","Saved best model\n","Epoch 72, Loss 926609199.50, Val loss 26679176.00\n","==================================================================\n","Saved best model\n","Epoch 73, Loss 924447917.03, Val loss 26618760.00\n","Epoch 74, Loss 924270933.62, Val loss 26782068.00\n","Epoch 75, Loss 925088530.53, Val loss 26741080.00\n","Epoch 76, Loss 922862556.41, Val loss 26674090.00\n","Epoch 77, Loss 921398830.50, Val loss 26618770.00\n","==================================================================\n","Saved best model\n","Epoch 78, Loss 919520935.41, Val loss 26584726.00\n","==================================================================\n","Saved best model\n","Epoch 79, Loss 917512184.19, Val loss 26518858.00\n","==================================================================\n","Saved best model\n","Epoch 80, Loss 915865334.41, Val loss 26461930.00\n","==================================================================\n","Saved best model\n","Epoch 81, Loss 914493999.94, Val loss 26430590.00\n","==================================================================\n","Saved best model\n","Epoch 82, Loss 912389638.84, Val loss 26360570.00\n","==================================================================\n","Saved best model\n","Epoch 83, Loss 911170156.47, Val loss 26335132.00\n","==================================================================\n","Saved best model\n","Epoch 84, Loss 909499898.66, Val loss 26262134.00\n","==================================================================\n","Saved best model\n","Epoch 85, Loss 910047581.06, Val loss 26210318.00\n","==================================================================\n","Saved best model\n","Epoch 86, Loss 906626926.28, Val loss 26172732.00\n","==================================================================\n","Saved best model\n","Epoch 87, Loss 904598301.88, Val loss 26117836.00\n","==================================================================\n","Saved best model\n","Epoch 88, Loss 906222256.19, Val loss 26082336.00\n","==================================================================\n","Saved best model\n","Epoch 89, Loss 902549503.16, Val loss 26033296.00\n","==================================================================\n","Saved best model\n","Epoch 90, Loss 900701463.03, Val loss 25988732.00\n","==================================================================\n","Saved best model\n","Epoch 91, Loss 899255879.00, Val loss 25945502.00\n","==================================================================\n","Saved best model\n","Epoch 92, Loss 897656281.69, Val loss 25891846.00\n","==================================================================\n","Saved best model\n","Epoch 93, Loss 896532141.72, Val loss 25858792.00\n","==================================================================\n","Saved best model\n","Epoch 94, Loss 895071655.12, Val loss 25802104.00\n","==================================================================\n","Saved best model\n","Epoch 95, Loss 897562717.78, Val loss 25783268.00\n","==================================================================\n","Saved best model\n","Epoch 96, Loss 892965314.41, Val loss 25728110.00\n","==================================================================\n","Saved best model\n","Epoch 97, Loss 891541402.50, Val loss 25678400.00\n","Epoch 98, Loss 895696131.72, Val loss 25897630.00\n","==================================================================\n","Saved best model\n","Epoch 99, Loss 889105352.78, Val loss 25590350.00\n","==================================================================\n","Saved best model\n","Epoch 100, Loss 889093904.75, Val loss 25549046.00\n","==================================================================\n","Saved best model\n","Epoch 101, Loss 887256930.59, Val loss 25520808.00\n","==================================================================\n","Saved best model\n","Epoch 102, Loss 885318690.31, Val loss 25494536.00\n","==================================================================\n","Saved best model\n","Epoch 103, Loss 884344042.41, Val loss 25436832.00\n","==================================================================\n","Saved best model\n","Epoch 104, Loss 883452535.09, Val loss 25405272.00\n","==================================================================\n","Saved best model\n","Epoch 105, Loss 882449020.53, Val loss 25367096.00\n","==================================================================\n","Saved best model\n","Epoch 106, Loss 881228145.06, Val loss 25325098.00\n","Epoch 107, Loss 884188562.19, Val loss 25550188.00\n","Epoch 108, Loss 881208033.44, Val loss 25508596.00\n","Epoch 109, Loss 880835133.34, Val loss 25468894.00\n","Epoch 110, Loss 878918582.28, Val loss 25414152.00\n","Epoch 111, Loss 877393368.22, Val loss 25385458.00\n","Epoch 112, Loss 876412360.22, Val loss 25337276.00\n","==================================================================\n","Saved best model\n","Epoch 113, Loss 879734612.75, Val loss 25293142.00\n","==================================================================\n","Saved best model\n","Epoch 114, Loss 874252691.19, Val loss 25239562.00\n","==================================================================\n","Saved best model\n","Epoch 115, Loss 869568598.03, Val loss 24987444.00\n","==================================================================\n","Saved best model\n","Epoch 116, Loss 873965984.81, Val loss 24956946.00\n","==================================================================\n","Saved best model\n","Epoch 117, Loss 871397584.06, Val loss 24921046.00\n","==================================================================\n","Saved best model\n","Epoch 118, Loss 869605671.12, Val loss 24910386.00\n","==================================================================\n","Saved best model\n","Epoch 119, Loss 868113459.81, Val loss 24859212.00\n","==================================================================\n","Saved best model\n","Epoch 120, Loss 867133477.94, Val loss 24844408.00\n","==================================================================\n","Saved best model\n","Epoch 121, Loss 866349700.44, Val loss 24812156.00\n","==================================================================\n","Saved best model\n","Epoch 122, Loss 865527339.22, Val loss 24783728.00\n","==================================================================\n","Saved best model\n","Epoch 123, Loss 864782424.31, Val loss 24752956.00\n","==================================================================\n","Saved best model\n","Epoch 124, Loss 863884744.06, Val loss 24738974.00\n","==================================================================\n","Saved best model\n","Epoch 125, Loss 862940872.31, Val loss 24686782.00\n","==================================================================\n","Saved best model\n","Epoch 126, Loss 862057556.81, Val loss 24657360.00\n","==================================================================\n","Saved best model\n","Epoch 127, Loss 861346371.06, Val loss 24625082.00\n","Epoch 128, Loss 863658687.91, Val loss 24863962.00\n","Epoch 129, Loss 859977439.75, Val loss 24802362.00\n","Epoch 130, Loss 864596486.84, Val loss 25076604.00\n","Epoch 131, Loss 859826034.72, Val loss 24805702.00\n","Epoch 132, Loss 861217089.06, Val loss 24921186.00\n","Epoch 133, Loss 858266321.81, Val loss 24932626.00\n","Epoch 134, Loss 857149319.22, Val loss 24894982.00\n","Epoch 135, Loss 856167206.25, Val loss 24876260.00\n","Epoch 136, Loss 855028743.16, Val loss 24826352.00\n","Epoch 137, Loss 854191564.31, Val loss 24810410.00\n","Epoch 138, Loss 853023779.28, Val loss 24757526.00\n","Epoch 139, Loss 851752976.62, Val loss 24727722.00\n","Epoch 140, Loss 851020698.06, Val loss 24708408.00\n","Epoch 141, Loss 850429282.19, Val loss 24734276.00\n","==================================================================\n","Saved best model\n","Epoch 142, Loss 854239482.44, Val loss 24622158.00\n","Epoch 143, Loss 852511292.31, Val loss 24703632.00\n","==================================================================\n","Saved best model\n","Epoch 144, Loss 848437063.16, Val loss 24536970.00\n","Epoch 145, Loss 847364179.03, Val loss 24631046.00\n","Epoch 146, Loss 845411951.69, Val loss 24596968.00\n","Epoch 147, Loss 845501367.00, Val loss 24592562.00\n","==================================================================\n","Saved best model\n","Epoch 148, Loss 843779404.53, Val loss 24468080.00\n","==================================================================\n","Saved best model\n","Epoch 149, Loss 843474197.72, Val loss 24417926.00\n","==================================================================\n","Saved best model\n","Epoch 150, Loss 842575396.22, Val loss 24401968.00\n","==================================================================\n","Saved best model\n","Epoch 151, Loss 841681655.94, Val loss 24351800.00\n","==================================================================\n","Saved best model\n","Epoch 152, Loss 840950517.12, Val loss 24337492.00\n","==================================================================\n","Saved best model\n","Epoch 153, Loss 840048645.19, Val loss 24305732.00\n","Epoch 154, Loss 839160411.75, Val loss 24338568.00\n","==================================================================\n","Saved best model\n","Epoch 155, Loss 838228066.16, Val loss 24292346.00\n","==================================================================\n","Saved best model\n","Epoch 156, Loss 837151232.03, Val loss 24271174.00\n","==================================================================\n","Saved best model\n","Epoch 157, Loss 836400283.41, Val loss 24227294.00\n","==================================================================\n","Saved best model\n","Epoch 158, Loss 835132270.38, Val loss 24194492.00\n","==================================================================\n","Saved best model\n","Epoch 159, Loss 834275000.50, Val loss 24161280.00\n","==================================================================\n","Saved best model\n","Epoch 160, Loss 833494939.44, Val loss 24131520.00\n","==================================================================\n","Saved best model\n","Epoch 161, Loss 832573028.81, Val loss 24104802.00\n","==================================================================\n","Saved best model\n","Epoch 162, Loss 831814620.19, Val loss 24079374.00\n","==================================================================\n","Saved best model\n","Epoch 163, Loss 830924230.72, Val loss 23992440.00\n","==================================================================\n","Saved best model\n","Epoch 164, Loss 830383825.47, Val loss 23959458.00\n","Epoch 165, Loss 829988516.16, Val loss 23959558.00\n","Epoch 166, Loss 828871031.88, Val loss 23969748.00\n","Epoch 167, Loss 828064981.84, Val loss 23973240.00\n","==================================================================\n","Saved best model\n","Epoch 168, Loss 826879717.75, Val loss 23930432.00\n","==================================================================\n","Saved best model\n","Epoch 169, Loss 826124528.50, Val loss 23928930.00\n","==================================================================\n","Saved best model\n","Epoch 170, Loss 825367778.56, Val loss 23913696.00\n","==================================================================\n","Saved best model\n","Epoch 171, Loss 824392306.25, Val loss 23871942.00\n","==================================================================\n","Saved best model\n","Epoch 172, Loss 823466616.19, Val loss 23855476.00\n","==================================================================\n","Saved best model\n","Epoch 173, Loss 822713297.53, Val loss 23836428.00\n","==================================================================\n","Saved best model\n","Epoch 174, Loss 822133525.75, Val loss 23797124.00\n","==================================================================\n","Saved best model\n","Epoch 175, Loss 821346653.62, Val loss 23765472.00\n","==================================================================\n","Saved best model\n","Epoch 176, Loss 820716633.19, Val loss 23731828.00\n","==================================================================\n","Saved best model\n","Epoch 177, Loss 819652917.53, Val loss 23711104.00\n","==================================================================\n","Saved best model\n","Epoch 178, Loss 818613078.50, Val loss 23688772.00\n","==================================================================\n","Saved best model\n","Epoch 179, Loss 817717166.09, Val loss 23668150.00\n","==================================================================\n","Saved best model\n","Epoch 180, Loss 816994712.72, Val loss 23646436.00\n","==================================================================\n","Saved best model\n","Epoch 181, Loss 816426542.28, Val loss 23625684.00\n","==================================================================\n","Saved best model\n","Epoch 182, Loss 815503152.03, Val loss 23605820.00\n","Epoch 183, Loss 821304794.19, Val loss 24109568.00\n","Epoch 184, Loss 838948465.91, Val loss 24167984.00\n","Epoch 185, Loss 833849704.28, Val loss 24065962.00\n","Epoch 186, Loss 833147228.56, Val loss 24057688.00\n","Epoch 187, Loss 833037927.25, Val loss 24076526.00\n","Epoch 188, Loss 831828287.69, Val loss 24012730.00\n","Epoch 189, Loss 831296395.22, Val loss 24004850.00\n","Epoch 190, Loss 834858393.12, Val loss 23962608.00\n","Epoch 191, Loss 829919822.66, Val loss 24008108.00\n","Epoch 192, Loss 831602382.91, Val loss 24037910.00\n","Epoch 193, Loss 829921395.16, Val loss 24087014.00\n","Epoch 194, Loss 827060685.91, Val loss 23965404.00\n","Epoch 195, Loss 827135492.00, Val loss 24011920.00\n","Epoch 196, Loss 825575286.72, Val loss 23986162.00\n","Epoch 197, Loss 825035238.66, Val loss 23883594.00\n","Epoch 198, Loss 823626042.94, Val loss 23930266.00\n","Epoch 199, Loss 821484361.62, Val loss 23902954.00\n","Epoch 200, Loss 820651040.75, Val loss 23874512.00\n","Epoch 201, Loss 819888048.22, Val loss 23848270.00\n","Epoch 202, Loss 819117075.62, Val loss 23822270.00\n","Epoch 203, Loss 818359050.81, Val loss 23796512.00\n","Epoch 204, Loss 817607694.38, Val loss 23771030.00\n","Epoch 205, Loss 816859736.94, Val loss 23745530.00\n","Epoch 206, Loss 816109805.81, Val loss 23719720.00\n","Epoch 207, Loss 815370124.97, Val loss 23693774.00\n","Epoch 208, Loss 821352796.56, Val loss 24446554.00\n","Epoch 209, Loss 833311274.19, Val loss 24427258.00\n","Epoch 210, Loss 831735358.41, Val loss 24390926.00\n","Epoch 211, Loss 830412209.56, Val loss 24354446.00\n","Epoch 212, Loss 829280948.94, Val loss 24316342.00\n","Epoch 213, Loss 828237494.44, Val loss 24279736.00\n","Epoch 214, Loss 827214190.50, Val loss 24244004.00\n","Epoch 215, Loss 826213907.66, Val loss 24209140.00\n","Epoch 216, Loss 825233434.94, Val loss 24174960.00\n","Epoch 217, Loss 824274387.53, Val loss 24141210.00\n","Epoch 218, Loss 823339373.16, Val loss 24108404.00\n","Epoch 219, Loss 822418988.56, Val loss 24075886.00\n","Epoch 220, Loss 821508638.19, Val loss 24043072.00\n","Epoch 221, Loss 820618449.72, Val loss 24010578.00\n","Epoch 222, Loss 819743371.41, Val loss 23979034.00\n","Epoch 223, Loss 818877269.81, Val loss 23948272.00\n","Epoch 224, Loss 818020319.59, Val loss 23918108.00\n","Epoch 225, Loss 817175092.72, Val loss 23888386.00\n","Epoch 226, Loss 816337915.94, Val loss 23858878.00\n","Epoch 227, Loss 815510975.44, Val loss 23829736.00\n","Epoch 228, Loss 814685370.44, Val loss 23800352.00\n","Epoch 229, Loss 813889438.72, Val loss 23772138.00\n","Epoch 230, Loss 813084437.81, Val loss 23743486.00\n","Epoch 231, Loss 812309193.53, Val loss 23716660.00\n","Epoch 232, Loss 811490498.25, Val loss 23610536.00\n","==================================================================\n","Saved best model\n","Epoch 233, Loss 811090914.84, Val loss 23584972.00\n","==================================================================\n","Saved best model\n","Epoch 234, Loss 810462483.03, Val loss 23557674.00\n","Epoch 235, Loss 809633432.28, Val loss 23611284.00\n","Epoch 236, Loss 808593326.09, Val loss 23583222.00\n","Epoch 237, Loss 807801896.16, Val loss 23557952.00\n","==================================================================\n","Saved best model\n","Epoch 238, Loss 807210674.44, Val loss 23531614.00\n","==================================================================\n","Saved best model\n","Epoch 239, Loss 806298724.06, Val loss 23505868.00\n","==================================================================\n","Saved best model\n","Epoch 240, Loss 805860200.44, Val loss 23482366.00\n","==================================================================\n","Saved best model\n","Epoch 241, Loss 804810921.59, Val loss 23454786.00\n","==================================================================\n","Saved best model\n","Epoch 242, Loss 804559429.44, Val loss 23423138.00\n","==================================================================\n","Saved best model\n","Epoch 243, Loss 803514936.25, Val loss 23397336.00\n","==================================================================\n","Saved best model\n","Epoch 244, Loss 803487286.03, Val loss 23371356.00\n","==================================================================\n","Saved best model\n","Epoch 245, Loss 802207119.66, Val loss 23349070.00\n","==================================================================\n","Saved best model\n","Epoch 246, Loss 802081324.91, Val loss 23322130.00\n","==================================================================\n","Saved best model\n","Epoch 247, Loss 800952374.62, Val loss 23299262.00\n","==================================================================\n","Saved best model\n","Epoch 248, Loss 800984919.16, Val loss 23274010.00\n","==================================================================\n","Saved best model\n","Epoch 249, Loss 799761723.06, Val loss 23247940.00\n","==================================================================\n","Saved best model\n","Epoch 250, Loss 799823661.06, Val loss 23227678.00\n","==================================================================\n","Saved best model\n","Epoch 251, Loss 798865454.19, Val loss 23204242.00\n","==================================================================\n","Saved best model\n","Epoch 252, Loss 798850652.72, Val loss 23184202.00\n","==================================================================\n","Saved best model\n","Epoch 253, Loss 797412441.59, Val loss 23157860.00\n","==================================================================\n","Saved best model\n","Epoch 254, Loss 797440645.12, Val loss 23137252.00\n","==================================================================\n","Saved best model\n","Epoch 255, Loss 796193091.91, Val loss 23113558.00\n","==================================================================\n","Saved best model\n","Epoch 256, Loss 796284222.28, Val loss 23096632.00\n","==================================================================\n","Saved best model\n","Epoch 257, Loss 795059358.00, Val loss 23073886.00\n","==================================================================\n","Saved best model\n","Epoch 258, Loss 795021931.94, Val loss 23056326.00\n","==================================================================\n","Saved best model\n","Epoch 259, Loss 793971961.81, Val loss 23032804.00\n","Epoch 260, Loss 794351178.56, Val loss 23098628.00\n","==================================================================\n","Saved best model\n","Epoch 261, Loss 793079265.62, Val loss 22990098.00\n","==================================================================\n","Saved best model\n","Epoch 262, Loss 792982087.94, Val loss 22987540.00\n","==================================================================\n","Saved best model\n","Epoch 263, Loss 791681627.66, Val loss 22949996.00\n","==================================================================\n","Saved best model\n","Epoch 264, Loss 791675577.50, Val loss 22934164.00\n","Epoch 265, Loss 790789651.31, Val loss 23007776.00\n","==================================================================\n","Saved best model\n","Epoch 266, Loss 791490429.47, Val loss 22888322.00\n","==================================================================\n","Saved best model\n","Epoch 267, Loss 790371557.09, Val loss 22875798.00\n","==================================================================\n","Saved best model\n","Epoch 268, Loss 789184938.50, Val loss 22852046.00\n","==================================================================\n","Saved best model\n","Epoch 269, Loss 788425702.56, Val loss 22832734.00\n","==================================================================\n","Saved best model\n","Epoch 270, Loss 788190584.62, Val loss 22824320.00\n","Epoch 271, Loss 787702040.38, Val loss 22840666.00\n","==================================================================\n","Saved best model\n","Epoch 272, Loss 787208136.75, Val loss 22768268.00\n","==================================================================\n","Saved best model\n","Epoch 273, Loss 786584830.41, Val loss 22666240.00\n","Epoch 274, Loss 786672791.22, Val loss 22730654.00\n","Epoch 275, Loss 785537824.62, Val loss 22715768.00\n","Epoch 276, Loss 784829649.16, Val loss 22696702.00\n","Epoch 277, Loss 784489072.72, Val loss 22678992.00\n","==================================================================\n","Saved best model\n","Epoch 278, Loss 783909244.97, Val loss 22660616.00\n","==================================================================\n","Saved best model\n","Epoch 279, Loss 783454412.31, Val loss 22640918.00\n","==================================================================\n","Saved best model\n","Epoch 280, Loss 783042118.56, Val loss 22625834.00\n","==================================================================\n","Saved best model\n","Epoch 281, Loss 782472769.62, Val loss 22610482.00\n","==================================================================\n","Saved best model\n","Epoch 282, Loss 781988197.62, Val loss 22592016.00\n","==================================================================\n","Saved best model\n","Epoch 283, Loss 781591266.91, Val loss 22573666.00\n","==================================================================\n","Saved best model\n","Epoch 284, Loss 781161644.62, Val loss 22554992.00\n","==================================================================\n","Saved best model\n","Epoch 285, Loss 780761834.19, Val loss 22537416.00\n","==================================================================\n","Saved best model\n","Epoch 286, Loss 780384105.41, Val loss 22529434.00\n","==================================================================\n","Saved best model\n","Epoch 287, Loss 779931119.28, Val loss 22520968.00\n","==================================================================\n","Saved best model\n","Epoch 288, Loss 779569352.62, Val loss 22491712.00\n","Epoch 289, Loss 783221312.66, Val loss 22905992.00\n","Epoch 290, Loss 777632801.38, Val loss 22883530.00\n","Epoch 291, Loss 777002457.69, Val loss 22856402.00\n","Epoch 292, Loss 776430029.91, Val loss 22738724.00\n","Epoch 293, Loss 776184559.28, Val loss 22798612.00\n","Epoch 294, Loss 775138614.97, Val loss 22772886.00\n","Epoch 295, Loss 774510249.97, Val loss 22745030.00\n","Epoch 296, Loss 773950904.88, Val loss 22717172.00\n","Epoch 297, Loss 773333211.03, Val loss 22691892.00\n","Epoch 298, Loss 772629067.03, Val loss 22667742.00\n","Epoch 299, Loss 772298293.38, Val loss 22640562.00\n","Epoch 300, Loss 771433123.12, Val loss 22618498.00\n","Epoch 301, Loss 771091763.03, Val loss 22566280.00\n","Epoch 302, Loss 770680971.47, Val loss 22570892.00\n","Epoch 303, Loss 770028026.75, Val loss 22546750.00\n","Epoch 304, Loss 769557023.19, Val loss 22523906.00\n","Epoch 305, Loss 768970468.31, Val loss 22502234.00\n","==================================================================\n","Saved best model\n","Epoch 306, Loss 768284914.72, Val loss 22481990.00\n","==================================================================\n","Saved best model\n","Epoch 307, Loss 767785586.19, Val loss 22459870.00\n","==================================================================\n","Saved best model\n","Epoch 308, Loss 767179211.97, Val loss 22440388.00\n","==================================================================\n","Saved best model\n","Epoch 309, Loss 766704948.16, Val loss 22421154.00\n","==================================================================\n","Saved best model\n","Epoch 310, Loss 766277813.72, Val loss 22373626.00\n","Epoch 311, Loss 765910570.41, Val loss 22383034.00\n","==================================================================\n","Saved best model\n","Epoch 312, Loss 765205213.44, Val loss 22366572.00\n","Epoch 313, Loss 771889647.06, Val loss 23047640.00\n","Epoch 314, Loss 789847169.28, Val loss 23418458.00\n","Epoch 315, Loss 785041029.19, Val loss 23427778.00\n","Epoch 316, Loss 783480067.44, Val loss 23341056.00\n","Epoch 317, Loss 782757343.41, Val loss 23326272.00\n","Epoch 318, Loss 782085436.31, Val loss 23280882.00\n","Epoch 319, Loss 780970557.38, Val loss 23253188.00\n","Epoch 320, Loss 780230376.06, Val loss 23216446.00\n","Epoch 321, Loss 779243754.88, Val loss 23182834.00\n","Epoch 322, Loss 778607164.22, Val loss 23154798.00\n","Epoch 323, Loss 777619990.03, Val loss 23122844.00\n","Epoch 324, Loss 791312867.08, Val loss 23644270.00\n","Epoch 325, Loss 787076121.78, Val loss 23610438.00\n","Epoch 326, Loss 783970276.81, Val loss 23001636.00\n","Epoch 327, Loss 774573375.56, Val loss 22974554.00\n","Epoch 328, Loss 773578695.38, Val loss 22950570.00\n","Epoch 329, Loss 772979390.44, Val loss 22928418.00\n","Epoch 330, Loss 772088251.91, Val loss 22920900.00\n","Epoch 331, Loss 771268840.66, Val loss 22901196.00\n","Epoch 332, Loss 770485225.34, Val loss 22877984.00\n","Epoch 333, Loss 770309306.00, Val loss 22849626.00\n","Epoch 334, Loss 769306725.72, Val loss 22829688.00\n","Epoch 335, Loss 768257319.16, Val loss 22802768.00\n","Epoch 336, Loss 767722802.41, Val loss 22778866.00\n","Epoch 337, Loss 766885242.88, Val loss 22751730.00\n","Epoch 338, Loss 766266722.69, Val loss 22727898.00\n","Epoch 339, Loss 765596172.41, Val loss 22705972.00\n","Epoch 340, Loss 764958788.31, Val loss 22682232.00\n","Epoch 341, Loss 764790078.06, Val loss 22659054.00\n","Epoch 342, Loss 764126606.62, Val loss 22635900.00\n","Epoch 343, Loss 762861784.38, Val loss 22612110.00\n","Epoch 344, Loss 762236435.44, Val loss 22592202.00\n","Epoch 345, Loss 765488252.47, Val loss 22566340.00\n","Epoch 346, Loss 761225262.62, Val loss 22542476.00\n","Epoch 347, Loss 760646628.72, Val loss 22525164.00\n","Epoch 348, Loss 760999866.34, Val loss 22558342.00\n","Epoch 349, Loss 759372638.47, Val loss 22477302.00\n","Epoch 350, Loss 760037364.81, Val loss 22460804.00\n","Epoch 351, Loss 758247043.06, Val loss 22441706.00\n","Epoch 352, Loss 757745910.19, Val loss 22418278.00\n","Epoch 353, Loss 757233781.03, Val loss 22393254.00\n","Epoch 354, Loss 756121950.06, Val loss 22428226.00\n","==================================================================\n","Saved best model\n","Epoch 355, Loss 755112943.38, Val loss 22348542.00\n","==================================================================\n","Saved best model\n","Epoch 356, Loss 754628114.78, Val loss 22324608.00\n","==================================================================\n","Saved best model\n","Epoch 357, Loss 753776814.53, Val loss 22310358.00\n","==================================================================\n","Saved best model\n","Epoch 358, Loss 753874096.06, Val loss 22293960.00\n","==================================================================\n","Saved best model\n","Epoch 359, Loss 753180025.31, Val loss 22275526.00\n","==================================================================\n","Saved best model\n","Epoch 360, Loss 752493037.19, Val loss 22253032.00\n","Epoch 361, Loss 753472438.69, Val loss 22264076.00\n","==================================================================\n","Saved best model\n","Epoch 362, Loss 751525562.00, Val loss 22223090.00\n","==================================================================\n","Saved best model\n","Epoch 363, Loss 750836692.41, Val loss 22190374.00\n","==================================================================\n","Saved best model\n","Epoch 364, Loss 750260869.91, Val loss 22166206.00\n","==================================================================\n","Saved best model\n","Epoch 365, Loss 756045324.38, Val loss 22163718.00\n","==================================================================\n","Saved best model\n","Epoch 366, Loss 758883620.59, Val loss 22106532.00\n","==================================================================\n","Saved best model\n","Epoch 367, Loss 748687530.22, Val loss 22099400.00\n","==================================================================\n","Saved best model\n","Epoch 368, Loss 748101696.44, Val loss 22081908.00\n","==================================================================\n","Saved best model\n","Epoch 369, Loss 747786498.50, Val loss 22064420.00\n","==================================================================\n","Saved best model\n","Epoch 370, Loss 746925998.06, Val loss 22042414.00\n","==================================================================\n","Saved best model\n","Epoch 371, Loss 746540410.12, Val loss 22026828.00\n","==================================================================\n","Saved best model\n","Epoch 372, Loss 745857557.06, Val loss 22018738.00\n","==================================================================\n","Saved best model\n","Epoch 373, Loss 746726688.62, Val loss 21998342.00\n","==================================================================\n","Saved best model\n","Epoch 374, Loss 745021817.81, Val loss 21997594.00\n","==================================================================\n","Saved best model\n","Epoch 375, Loss 744514216.62, Val loss 21960518.00\n","==================================================================\n","Saved best model\n","Epoch 376, Loss 744025927.62, Val loss 21948754.00\n","==================================================================\n","Saved best model\n","Epoch 377, Loss 743544074.88, Val loss 21916302.00\n","==================================================================\n","Saved best model\n","Epoch 378, Loss 743153314.38, Val loss 21907632.00\n","==================================================================\n","Saved best model\n","Epoch 379, Loss 742778582.22, Val loss 21881220.00\n","==================================================================\n","Saved best model\n","Epoch 380, Loss 742118343.16, Val loss 21867244.00\n","==================================================================\n","Saved best model\n","Epoch 381, Loss 742068186.19, Val loss 21849548.00\n","==================================================================\n","Saved best model\n","Epoch 382, Loss 741265763.69, Val loss 21835962.00\n","==================================================================\n","Saved best model\n","Epoch 383, Loss 740784869.78, Val loss 21821782.00\n","==================================================================\n","Saved best model\n","Epoch 384, Loss 740156202.66, Val loss 21805304.00\n","==================================================================\n","Saved best model\n","Epoch 385, Loss 740268833.94, Val loss 21790032.00\n","==================================================================\n","Saved best model\n","Epoch 386, Loss 740193932.69, Val loss 21789052.00\n","==================================================================\n","Saved best model\n","Epoch 387, Loss 739183350.81, Val loss 21665292.00\n","Epoch 388, Loss 739835569.78, Val loss 21746082.00\n","Epoch 389, Loss 738292435.31, Val loss 21728912.00\n","Epoch 390, Loss 738322201.25, Val loss 21699674.00\n","Epoch 391, Loss 737806583.50, Val loss 21695512.00\n","Epoch 392, Loss 737574783.00, Val loss 21666826.00\n","==================================================================\n","Saved best model\n","Epoch 393, Loss 737147429.94, Val loss 21654228.00\n","Epoch 394, Loss 736639476.25, Val loss 21673410.00\n","==================================================================\n","Saved best model\n","Epoch 395, Loss 735548407.03, Val loss 21633346.00\n","==================================================================\n","Saved best model\n","Epoch 396, Loss 735441632.78, Val loss 21622720.00\n","==================================================================\n","Saved best model\n","Epoch 397, Loss 734886977.41, Val loss 21591298.00\n","==================================================================\n","Saved best model\n","Epoch 398, Loss 734287187.47, Val loss 21577164.00\n","==================================================================\n","Saved best model\n","Epoch 399, Loss 734392396.19, Val loss 21555836.00\n","==================================================================\n","Saved best model\n","Epoch 400, Loss 734058728.31, Val loss 21544492.00\n","==================================================================\n","Saved best model\n","Epoch 401, Loss 733329408.09, Val loss 21535954.00\n","==================================================================\n","Saved best model\n","Epoch 402, Loss 732980811.16, Val loss 21511694.00\n","==================================================================\n","Saved best model\n","Epoch 403, Loss 732843075.16, Val loss 21494668.00\n","Epoch 404, Loss 731934333.44, Val loss 21494932.00\n","==================================================================\n","Saved best model\n","Epoch 405, Loss 731235758.28, Val loss 21470192.00\n","==================================================================\n","Saved best model\n","Epoch 406, Loss 730358969.00, Val loss 21465894.00\n","Epoch 407, Loss 730344764.03, Val loss 21470100.00\n","==================================================================\n","Saved best model\n","Epoch 408, Loss 726554328.94, Val loss 20987784.00\n","Epoch 409, Loss 720093370.56, Val loss 21067032.00\n","Epoch 410, Loss 718507296.28, Val loss 21040064.00\n","Epoch 411, Loss 718231569.66, Val loss 21019764.00\n","Epoch 412, Loss 718192986.12, Val loss 20992202.00\n","Epoch 413, Loss 720127886.50, Val loss 21003052.00\n","Epoch 414, Loss 719539070.84, Val loss 21280450.00\n","Epoch 415, Loss 716804725.88, Val loss 21286356.00\n","Epoch 416, Loss 716835008.03, Val loss 21260262.00\n","Epoch 417, Loss 716040443.03, Val loss 21252930.00\n","Epoch 418, Loss 716872097.84, Val loss 21249180.00\n","Epoch 419, Loss 715081240.97, Val loss 21225932.00\n","Epoch 420, Loss 716497037.12, Val loss 21227602.00\n","Epoch 421, Loss 714588109.38, Val loss 21213156.00\n","Epoch 422, Loss 714452128.84, Val loss 21204852.00\n","Epoch 423, Loss 715412304.19, Val loss 21193544.00\n","Epoch 424, Loss 713567829.41, Val loss 21188276.00\n","Epoch 425, Loss 713677502.19, Val loss 21177758.00\n","Epoch 426, Loss 712913052.59, Val loss 21168664.00\n","Epoch 427, Loss 712754301.16, Val loss 21157012.00\n","Epoch 428, Loss 711784991.88, Val loss 21146140.00\n","Epoch 429, Loss 711149191.19, Val loss 21133766.00\n","Epoch 430, Loss 711473474.62, Val loss 21116404.00\n","Epoch 431, Loss 715940447.41, Val loss 21497580.00\n","Epoch 432, Loss 712968884.12, Val loss 21563302.00\n","Epoch 433, Loss 712302174.88, Val loss 21647114.00\n","Epoch 434, Loss 710805464.03, Val loss 21636618.00\n","Epoch 435, Loss 710499421.00, Val loss 21619652.00\n","Epoch 436, Loss 709756030.38, Val loss 21593792.00\n","Epoch 437, Loss 709018427.50, Val loss 21572862.00\n","Epoch 438, Loss 708433063.03, Val loss 21550480.00\n","Epoch 439, Loss 707703919.22, Val loss 21529080.00\n","Epoch 440, Loss 707427683.94, Val loss 21505936.00\n","Epoch 441, Loss 706709799.91, Val loss 21483640.00\n","Epoch 442, Loss 706533280.69, Val loss 21456800.00\n","Epoch 443, Loss 705679882.91, Val loss 21424694.00\n","Epoch 444, Loss 706533683.44, Val loss 21426580.00\n","Epoch 445, Loss 705438841.59, Val loss 21402772.00\n","Epoch 446, Loss 704457726.72, Val loss 21100384.00\n","Epoch 447, Loss 704397848.19, Val loss 21230770.00\n","Epoch 448, Loss 704739174.38, Val loss 21234010.00\n","Epoch 449, Loss 703834286.12, Val loss 21210350.00\n","Epoch 450, Loss 703334696.62, Val loss 21204960.00\n","Epoch 451, Loss 702227638.78, Val loss 21171798.00\n","Epoch 452, Loss 702195215.59, Val loss 21165906.00\n","Epoch 453, Loss 701360390.56, Val loss 21139600.00\n","Epoch 454, Loss 702719849.75, Val loss 21125818.00\n","Epoch 455, Loss 700989349.28, Val loss 21098096.00\n","Epoch 456, Loss 700077464.44, Val loss 21084708.00\n","Epoch 457, Loss 699489149.38, Val loss 21069822.00\n","Epoch 458, Loss 698921082.50, Val loss 21053976.00\n","Epoch 459, Loss 698455067.88, Val loss 21036722.00\n","Epoch 460, Loss 698047987.53, Val loss 21020108.00\n","Epoch 461, Loss 697635769.28, Val loss 21003822.00\n","==================================================================\n","Saved best model\n","Epoch 462, Loss 697234847.53, Val loss 20987070.00\n","==================================================================\n","Saved best model\n","Epoch 463, Loss 696816412.28, Val loss 20970832.00\n","==================================================================\n","Saved best model\n","Epoch 464, Loss 696367555.91, Val loss 20954538.00\n","==================================================================\n","Saved best model\n","Epoch 465, Loss 696136833.22, Val loss 20938080.00\n","==================================================================\n","Saved best model\n","Epoch 466, Loss 695536135.59, Val loss 20922374.00\n","==================================================================\n","Saved best model\n","Epoch 467, Loss 695128174.88, Val loss 20907652.00\n","Epoch 468, Loss 694883954.66, Val loss 21006242.00\n","==================================================================\n","Saved best model\n","Epoch 469, Loss 693999897.56, Val loss 20875634.00\n","==================================================================\n","Saved best model\n","Epoch 470, Loss 693851869.91, Val loss 20862684.00\n","Epoch 471, Loss 694682217.03, Val loss 21011618.00\n","Epoch 472, Loss 692522342.84, Val loss 20991424.00\n","Epoch 473, Loss 692722468.03, Val loss 20979738.00\n","Epoch 474, Loss 693031348.53, Val loss 20961514.00\n","Epoch 475, Loss 691628440.62, Val loss 20945390.00\n","Epoch 476, Loss 690728567.19, Val loss 20936140.00\n","Epoch 477, Loss 690768712.19, Val loss 20908896.00\n","Epoch 478, Loss 689996045.28, Val loss 20896664.00\n","Epoch 479, Loss 689518632.09, Val loss 20880840.00\n","Epoch 480, Loss 689047876.62, Val loss 20865588.00\n","==================================================================\n","Saved best model\n","Epoch 481, Loss 688600187.59, Val loss 20852342.00\n","==================================================================\n","Saved best model\n","Epoch 482, Loss 688509921.25, Val loss 20837542.00\n","==================================================================\n","Saved best model\n","Epoch 483, Loss 687822435.31, Val loss 20820364.00\n","==================================================================\n","Saved best model\n","Epoch 484, Loss 687432266.25, Val loss 20805310.00\n","==================================================================\n","Saved best model\n","Epoch 485, Loss 687600656.25, Val loss 20789456.00\n","==================================================================\n","Saved best model\n","Epoch 486, Loss 687440712.50, Val loss 20774460.00\n","==================================================================\n","Saved best model\n","Epoch 487, Loss 686287093.44, Val loss 20574796.00\n","==================================================================\n","Saved best model\n","Epoch 488, Loss 685509367.56, Val loss 20561994.00\n","==================================================================\n","Saved best model\n","Epoch 489, Loss 685105421.00, Val loss 20551090.00\n","==================================================================\n","Saved best model\n","Epoch 490, Loss 684789879.03, Val loss 20531242.00\n","==================================================================\n","Saved best model\n","Epoch 491, Loss 685048277.66, Val loss 20515012.00\n","==================================================================\n","Saved best model\n","Epoch 492, Loss 684184486.59, Val loss 20505758.00\n","==================================================================\n","Saved best model\n","Epoch 493, Loss 683823014.62, Val loss 20492312.00\n","==================================================================\n","Saved best model\n","Epoch 494, Loss 683497810.38, Val loss 20478340.00\n","==================================================================\n","Saved best model\n","Epoch 495, Loss 683175423.25, Val loss 20464692.00\n","==================================================================\n","Saved best model\n","Epoch 496, Loss 682883731.97, Val loss 20454112.00\n","==================================================================\n","Saved best model\n","Epoch 497, Loss 682619821.31, Val loss 20437988.00\n","==================================================================\n","Saved best model\n","Epoch 498, Loss 682315007.03, Val loss 20425392.00\n","Epoch 499, Loss 682926401.78, Val loss 20604580.00\n","==================================================================\n","Saved best model\n","Epoch 500, Loss 681869449.62, Val loss 20398876.00\n","==================================================================\n","Saved best model\n","Epoch 501, Loss 681591866.44, Val loss 20385514.00\n","==================================================================\n","Saved best model\n","Epoch 502, Loss 681236947.97, Val loss 20377880.00\n","Epoch 503, Loss 682412572.19, Val loss 20392182.00\n","Epoch 504, Loss 681965754.34, Val loss 20378896.00\n","==================================================================\n","Saved best model\n","Epoch 505, Loss 681159883.91, Val loss 20322824.00\n","==================================================================\n","Saved best model\n","Epoch 506, Loss 680397931.06, Val loss 20317000.00\n","==================================================================\n","Saved best model\n","Epoch 507, Loss 679880957.50, Val loss 20309078.00\n","==================================================================\n","Saved best model\n","Epoch 508, Loss 679518692.69, Val loss 20297218.00\n","==================================================================\n","Saved best model\n","Epoch 509, Loss 679228641.19, Val loss 20285798.00\n","==================================================================\n","Saved best model\n","Epoch 510, Loss 678957774.38, Val loss 20276226.00\n","==================================================================\n","Saved best model\n","Epoch 511, Loss 678644677.62, Val loss 20261886.00\n","Epoch 512, Loss 679324513.56, Val loss 20449206.00\n","Epoch 513, Loss 678512238.19, Val loss 20434952.00\n","Epoch 514, Loss 677900426.41, Val loss 20423126.00\n","Epoch 515, Loss 677324071.44, Val loss 20409838.00\n","Epoch 516, Loss 677038695.50, Val loss 20401462.00\n","Epoch 517, Loss 676831044.62, Val loss 20386750.00\n","Epoch 518, Loss 676516552.91, Val loss 20368184.00\n","Epoch 519, Loss 676169708.25, Val loss 20363648.00\n","Epoch 520, Loss 675821661.75, Val loss 20347226.00\n","Epoch 521, Loss 675596855.75, Val loss 20335444.00\n","Epoch 522, Loss 675265263.12, Val loss 20320352.00\n","Epoch 523, Loss 674964790.31, Val loss 20309744.00\n","Epoch 524, Loss 674725021.31, Val loss 20301322.00\n","Epoch 525, Loss 674340470.50, Val loss 20287838.00\n","Epoch 526, Loss 674333382.75, Val loss 20274282.00\n","Epoch 527, Loss 673801069.81, Val loss 20264926.00\n","==================================================================\n","Saved best model\n","Epoch 528, Loss 673496785.25, Val loss 20255774.00\n","==================================================================\n","Saved best model\n","Epoch 529, Loss 673239668.25, Val loss 20240702.00\n","==================================================================\n","Saved best model\n","Epoch 530, Loss 672932217.06, Val loss 20225966.00\n","==================================================================\n","Saved best model\n","Epoch 531, Loss 672623908.50, Val loss 20218096.00\n","==================================================================\n","Saved best model\n","Epoch 532, Loss 672334246.00, Val loss 20211278.00\n","==================================================================\n","Saved best model\n","Epoch 533, Loss 672006359.75, Val loss 20198310.00\n","==================================================================\n","Saved best model\n","Epoch 534, Loss 671666264.88, Val loss 20182538.00\n","==================================================================\n","Saved best model\n","Epoch 535, Loss 671819570.19, Val loss 20173190.00\n","==================================================================\n","Saved best model\n","Epoch 536, Loss 671149597.62, Val loss 20155122.00\n","==================================================================\n","Saved best model\n","Epoch 537, Loss 670884708.94, Val loss 20150690.00\n","==================================================================\n","Saved best model\n","Epoch 538, Loss 670587361.56, Val loss 20139998.00\n","==================================================================\n","Saved best model\n","Epoch 539, Loss 670331036.50, Val loss 20129470.00\n","==================================================================\n","Saved best model\n","Epoch 540, Loss 670044491.25, Val loss 20118780.00\n","==================================================================\n","Saved best model\n","Epoch 541, Loss 669805796.88, Val loss 20114492.00\n","==================================================================\n","Saved best model\n","Epoch 542, Loss 669503831.88, Val loss 20096226.00\n","==================================================================\n","Saved best model\n","Epoch 543, Loss 669274066.94, Val loss 20087134.00\n","==================================================================\n","Saved best model\n","Epoch 544, Loss 669001430.06, Val loss 20082282.00\n","==================================================================\n","Saved best model\n","Epoch 545, Loss 668679479.00, Val loss 20066366.00\n","Epoch 546, Loss 680576266.44, Val loss 21406314.00\n","Epoch 547, Loss 690615804.50, Val loss 21062264.00\n","Epoch 548, Loss 686094992.81, Val loss 20995798.00\n","Epoch 549, Loss 684628910.81, Val loss 20978056.00\n","Epoch 550, Loss 684208076.97, Val loss 20955280.00\n","Epoch 551, Loss 683693240.66, Val loss 20932858.00\n","Epoch 552, Loss 678679808.75, Val loss 20392694.00\n","Epoch 553, Loss 696808021.38, Val loss 21023450.00\n","Epoch 554, Loss 692301081.97, Val loss 20868482.00\n","Epoch 555, Loss 685683645.25, Val loss 20817116.00\n","Epoch 556, Loss 687302417.16, Val loss 20792950.00\n","Epoch 557, Loss 685280610.97, Val loss 20786460.00\n","Epoch 558, Loss 690235068.66, Val loss 21061382.00\n","Epoch 559, Loss 686491572.91, Val loss 21025330.00\n","Epoch 560, Loss 686114120.00, Val loss 21011156.00\n","Epoch 561, Loss 685734664.41, Val loss 20994928.00\n","Epoch 562, Loss 685366180.12, Val loss 20977328.00\n","Epoch 563, Loss 684998723.19, Val loss 20959580.00\n","Epoch 564, Loss 684604365.59, Val loss 20944080.00\n","Epoch 565, Loss 684203905.00, Val loss 20929904.00\n","Epoch 566, Loss 683840458.56, Val loss 20915698.00\n","Epoch 567, Loss 683357432.47, Val loss 20899326.00\n","Epoch 568, Loss 682988968.78, Val loss 20883210.00\n","Epoch 569, Loss 702969419.00, Val loss 21372128.00\n","Epoch 570, Loss 698082376.72, Val loss 21259510.00\n","Epoch 571, Loss 699084015.19, Val loss 21241022.00\n","Epoch 572, Loss 698642499.88, Val loss 21227508.00\n","Epoch 573, Loss 698021874.41, Val loss 21208266.00\n","Epoch 574, Loss 697682775.34, Val loss 21185492.00\n","Epoch 575, Loss 697177603.97, Val loss 21169832.00\n","Epoch 576, Loss 696790502.12, Val loss 21154964.00\n","Epoch 577, Loss 696334691.03, Val loss 21141502.00\n","Epoch 578, Loss 695916711.88, Val loss 21127218.00\n","Epoch 579, Loss 695517965.97, Val loss 21112626.00\n","Epoch 580, Loss 695149203.56, Val loss 21098238.00\n","Epoch 581, Loss 694956476.94, Val loss 21075332.00\n","Epoch 582, Loss 694497139.94, Val loss 21063950.00\n","Epoch 583, Loss 694347867.72, Val loss 21044588.00\n","Epoch 584, Loss 693847767.62, Val loss 21033592.00\n","Epoch 585, Loss 693442175.94, Val loss 21018316.00\n","Epoch 586, Loss 693076215.50, Val loss 21004516.00\n","Epoch 587, Loss 692746219.91, Val loss 20985716.00\n","Epoch 588, Loss 692399608.91, Val loss 20972568.00\n","Epoch 589, Loss 692082442.12, Val loss 20954828.00\n","Epoch 590, Loss 691924459.94, Val loss 20937234.00\n","Epoch 591, Loss 691395730.28, Val loss 20925812.00\n","Epoch 592, Loss 691238532.84, Val loss 20904034.00\n","Epoch 593, Loss 690717156.97, Val loss 20892504.00\n","Epoch 594, Loss 700910162.88, Val loss 20873200.00\n","Epoch 595, Loss 690264116.50, Val loss 20854756.00\n","Epoch 596, Loss 689570301.06, Val loss 20834094.00\n","Epoch 597, Loss 689346677.81, Val loss 20827230.00\n","Epoch 598, Loss 688977291.97, Val loss 20811954.00\n","Epoch 599, Loss 701307504.56, Val loss 21118030.00\n","Epoch 600, Loss 699667773.22, Val loss 21105030.00\n","Epoch 601, Loss 699119892.22, Val loss 21092986.00\n","Epoch 602, Loss 698736372.91, Val loss 21078368.00\n","Epoch 603, Loss 698612118.44, Val loss 21142622.00\n","Epoch 604, Loss 697803008.78, Val loss 21129550.00\n","Epoch 605, Loss 697418800.50, Val loss 21120300.00\n","Epoch 606, Loss 697122758.88, Val loss 21103612.00\n","Epoch 607, Loss 696610045.28, Val loss 21092716.00\n","Epoch 608, Loss 696248999.84, Val loss 21081566.00\n","Epoch 609, Loss 695917426.47, Val loss 21067404.00\n","Epoch 610, Loss 695504441.16, Val loss 21054882.00\n","Epoch 611, Loss 695157626.94, Val loss 21041004.00\n","Epoch 612, Loss 694801725.84, Val loss 21027674.00\n","Epoch 613, Loss 694461451.09, Val loss 21014620.00\n","Epoch 614, Loss 694090706.75, Val loss 21001526.00\n","Epoch 615, Loss 693755328.75, Val loss 20988122.00\n","Epoch 616, Loss 693430601.84, Val loss 20974572.00\n","Epoch 617, Loss 693117740.59, Val loss 20961548.00\n","Epoch 618, Loss 692801337.06, Val loss 20948660.00\n","Epoch 619, Loss 692474046.31, Val loss 20935766.00\n","Epoch 620, Loss 692145591.62, Val loss 20922978.00\n","Epoch 621, Loss 691819790.81, Val loss 20910216.00\n","Epoch 622, Loss 691495733.81, Val loss 20897468.00\n","Epoch 623, Loss 691175591.38, Val loss 20884764.00\n","Epoch 624, Loss 690859036.25, Val loss 20872126.00\n","Epoch 625, Loss 690545461.50, Val loss 20859550.00\n","Epoch 626, Loss 690234279.12, Val loss 20847030.00\n","Epoch 627, Loss 689925379.47, Val loss 20834536.00\n","Epoch 628, Loss 689618341.06, Val loss 20822048.00\n","Epoch 629, Loss 689311642.91, Val loss 20809516.00\n","Epoch 630, Loss 689002322.59, Val loss 20796244.00\n","Epoch 631, Loss 688681725.84, Val loss 20784374.00\n","Epoch 632, Loss 688428270.03, Val loss 20770274.00\n","Epoch 633, Loss 688221518.03, Val loss 20759554.00\n","Epoch 634, Loss 687908365.19, Val loss 20743800.00\n","Epoch 635, Loss 687469799.06, Val loss 20738324.00\n","Epoch 636, Loss 687276949.53, Val loss 20727232.00\n","Epoch 637, Loss 686885977.75, Val loss 20710676.00\n","Epoch 638, Loss 686542581.22, Val loss 20702736.00\n","Epoch 639, Loss 686283330.59, Val loss 20690248.00\n","Epoch 640, Loss 685998623.03, Val loss 20677018.00\n","Epoch 641, Loss 685736331.50, Val loss 20663804.00\n","Epoch 642, Loss 686740821.47, Val loss 21294280.00\n","Epoch 643, Loss 689321513.56, Val loss 20690280.00\n","Epoch 644, Loss 691026391.06, Val loss 21318760.00\n","Epoch 645, Loss 689819528.41, Val loss 21383692.00\n","Epoch 646, Loss 688381351.69, Val loss 21344966.00\n","Epoch 647, Loss 687267852.69, Val loss 21319442.00\n","Epoch 648, Loss 686965184.66, Val loss 20899264.00\n","Epoch 649, Loss 686914154.25, Val loss 20988588.00\n","Epoch 650, Loss 686975436.59, Val loss 21076736.00\n","Epoch 651, Loss 685186733.38, Val loss 21092360.00\n","Epoch 652, Loss 680497839.56, Val loss 20824872.00\n","Epoch 653, Loss 679766778.16, Val loss 20872942.00\n","Epoch 654, Loss 677042063.75, Val loss 20147102.00\n","Epoch 655, Loss 678713761.69, Val loss 20594186.00\n","Epoch 656, Loss 674282125.66, Val loss 20579600.00\n","Epoch 657, Loss 673758232.78, Val loss 20562634.00\n","Epoch 658, Loss 673439278.84, Val loss 20514414.00\n","Epoch 659, Loss 673292496.28, Val loss 20499646.00\n","Epoch 660, Loss 673009055.66, Val loss 20487160.00\n","Epoch 661, Loss 672245637.62, Val loss 20480048.00\n","Epoch 662, Loss 671883040.69, Val loss 20484280.00\n","Epoch 663, Loss 671522269.66, Val loss 20484334.00\n","Epoch 664, Loss 671189760.59, Val loss 20472078.00\n","Epoch 665, Loss 670905697.12, Val loss 20468134.00\n","Epoch 666, Loss 670506367.94, Val loss 20450904.00\n","Epoch 667, Loss 670271732.56, Val loss 20442172.00\n","Epoch 668, Loss 669904329.72, Val loss 20421934.00\n","Epoch 669, Loss 669693333.16, Val loss 20409996.00\n","Epoch 670, Loss 669322552.59, Val loss 20385236.00\n","Epoch 671, Loss 669093009.97, Val loss 20370672.00\n","Epoch 672, Loss 668734576.22, Val loss 20354148.00\n","Epoch 673, Loss 668514463.97, Val loss 20343928.00\n","Epoch 674, Loss 668142545.84, Val loss 20330434.00\n","Epoch 675, Loss 667923534.72, Val loss 20323502.00\n","Epoch 676, Loss 667537803.00, Val loss 20310836.00\n","Epoch 677, Loss 667329867.72, Val loss 20304286.00\n","Epoch 678, Loss 669806792.88, Val loss 20456458.00\n","Epoch 679, Loss 674311283.84, Val loss 20447622.00\n","Epoch 680, Loss 671134804.28, Val loss 20431804.00\n","Epoch 681, Loss 669858847.00, Val loss 20423516.00\n","Epoch 682, Loss 666820240.69, Val loss 20236688.00\n","Epoch 683, Loss 666260113.25, Val loss 20230120.00\n","Epoch 684, Loss 665351086.38, Val loss 20214752.00\n","Epoch 685, Loss 665151279.19, Val loss 20205838.00\n","Epoch 686, Loss 664810169.06, Val loss 20191678.00\n","Epoch 687, Loss 664639483.56, Val loss 20182946.00\n","Epoch 688, Loss 664306092.75, Val loss 20169288.00\n","Epoch 689, Loss 664262469.69, Val loss 20160502.00\n","Epoch 690, Loss 663836832.88, Val loss 20146796.00\n","Epoch 691, Loss 663698531.12, Val loss 20138070.00\n","Epoch 692, Loss 663372993.00, Val loss 20124650.00\n","Epoch 693, Loss 663246300.06, Val loss 20116236.00\n","Epoch 694, Loss 662915004.69, Val loss 20103470.00\n","Epoch 695, Loss 662850295.19, Val loss 20094882.00\n","Epoch 696, Loss 662472257.12, Val loss 20082198.00\n","Epoch 697, Loss 662345166.75, Val loss 20074306.00\n","==================================================================\n","Saved best model\n","Epoch 698, Loss 662008884.56, Val loss 20062324.00\n","==================================================================\n","Saved best model\n","Epoch 699, Loss 661879366.56, Val loss 20054408.00\n","==================================================================\n","Saved best model\n","Epoch 700, Loss 661549378.56, Val loss 20042524.00\n","==================================================================\n","Saved best model\n","Epoch 701, Loss 661516438.50, Val loss 20034416.00\n","==================================================================\n","Saved best model\n","Epoch 702, Loss 661091258.00, Val loss 20022836.00\n","==================================================================\n","Saved best model\n","Epoch 703, Loss 660963406.19, Val loss 20014486.00\n","==================================================================\n","Saved best model\n","Epoch 704, Loss 660634256.44, Val loss 20003388.00\n","==================================================================\n","Saved best model\n","Epoch 705, Loss 660512575.56, Val loss 19996200.00\n","==================================================================\n","Saved best model\n","Epoch 706, Loss 660163986.75, Val loss 19988168.00\n","==================================================================\n","Saved best model\n","Epoch 707, Loss 660100228.62, Val loss 19983440.00\n","==================================================================\n","Saved best model\n","Epoch 708, Loss 659611708.81, Val loss 19964864.00\n","==================================================================\n","Saved best model\n","Epoch 709, Loss 659480336.12, Val loss 19956294.00\n","==================================================================\n","Saved best model\n","Epoch 710, Loss 659121811.62, Val loss 19944610.00\n","==================================================================\n","Saved best model\n","Epoch 711, Loss 658996808.38, Val loss 19937056.00\n","==================================================================\n","Saved best model\n","Epoch 712, Loss 658614262.81, Val loss 19929204.00\n","==================================================================\n","Saved best model\n","Epoch 713, Loss 658437591.56, Val loss 19924320.00\n","==================================================================\n","Saved best model\n","Epoch 714, Loss 658089005.25, Val loss 19914198.00\n","==================================================================\n","Saved best model\n","Epoch 715, Loss 657977567.56, Val loss 19908530.00\n","==================================================================\n","Saved best model\n","Epoch 716, Loss 657650796.56, Val loss 19897824.00\n","==================================================================\n","Saved best model\n","Epoch 717, Loss 657538046.62, Val loss 19891856.00\n","==================================================================\n","Saved best model\n","Epoch 718, Loss 657207756.06, Val loss 19880266.00\n","==================================================================\n","Saved best model\n","Epoch 719, Loss 657224379.69, Val loss 19875540.00\n","==================================================================\n","Saved best model\n","Epoch 720, Loss 656808155.12, Val loss 19863154.00\n","==================================================================\n","Saved best model\n","Epoch 721, Loss 656689487.56, Val loss 19856952.00\n","==================================================================\n","Saved best model\n","Epoch 722, Loss 656390781.94, Val loss 19845718.00\n","==================================================================\n","Saved best model\n","Epoch 723, Loss 656263786.62, Val loss 19841058.00\n","==================================================================\n","Saved best model\n","Epoch 724, Loss 655965603.12, Val loss 19832050.00\n","==================================================================\n","Saved best model\n","Epoch 725, Loss 656224056.25, Val loss 19824256.00\n","==================================================================\n","Saved best model\n","Epoch 726, Loss 655691227.25, Val loss 19815488.00\n","==================================================================\n","Saved best model\n","Epoch 727, Loss 655828410.88, Val loss 19808770.00\n","==================================================================\n","Saved best model\n","Epoch 728, Loss 655718903.62, Val loss 19798644.00\n","==================================================================\n","Saved best model\n","Epoch 729, Loss 655320835.00, Val loss 19793578.00\n","==================================================================\n","Saved best model\n","Epoch 730, Loss 655032826.50, Val loss 19783886.00\n","==================================================================\n","Saved best model\n","Epoch 731, Loss 654950791.38, Val loss 19777538.00\n","==================================================================\n","Saved best model\n","Epoch 732, Loss 654832835.69, Val loss 19768464.00\n","==================================================================\n","Saved best model\n","Epoch 733, Loss 654862898.19, Val loss 19761842.00\n","==================================================================\n","Saved best model\n","Epoch 734, Loss 654872159.19, Val loss 19751954.00\n","==================================================================\n","Saved best model\n","Epoch 735, Loss 654633145.94, Val loss 19747292.00\n","==================================================================\n","Saved best model\n","Epoch 736, Loss 654414456.25, Val loss 19739470.00\n","==================================================================\n","Saved best model\n","Epoch 737, Loss 654563475.44, Val loss 19732738.00\n","==================================================================\n","Saved best model\n","Epoch 738, Loss 654041111.69, Val loss 19724600.00\n","==================================================================\n","Saved best model\n","Epoch 739, Loss 653874564.56, Val loss 19717498.00\n","==================================================================\n","Saved best model\n","Epoch 740, Loss 653675916.88, Val loss 19709192.00\n","==================================================================\n","Saved best model\n","Epoch 741, Loss 653510633.81, Val loss 19701570.00\n","==================================================================\n","Saved best model\n","Epoch 742, Loss 653319607.50, Val loss 19693292.00\n","==================================================================\n","Saved best model\n","Epoch 743, Loss 653169531.75, Val loss 19684552.00\n","==================================================================\n","Saved best model\n","Epoch 744, Loss 652991576.00, Val loss 19676288.00\n","==================================================================\n","Saved best model\n","Epoch 745, Loss 652848244.56, Val loss 19668090.00\n","==================================================================\n","Saved best model\n","Epoch 746, Loss 652711564.56, Val loss 19659224.00\n","==================================================================\n","Saved best model\n","Epoch 747, Loss 652756402.44, Val loss 19650104.00\n","==================================================================\n","Saved best model\n","Epoch 748, Loss 654110480.12, Val loss 19641334.00\n","==================================================================\n","Saved best model\n","Epoch 749, Loss 652598510.00, Val loss 19632544.00\n","==================================================================\n","Saved best model\n","Epoch 750, Loss 652326201.56, Val loss 19624906.00\n","==================================================================\n","Saved best model\n","Epoch 751, Loss 651742471.62, Val loss 19616930.00\n","==================================================================\n","Saved best model\n","Epoch 752, Loss 651544978.62, Val loss 19607812.00\n","==================================================================\n","Saved best model\n","Epoch 753, Loss 651400398.25, Val loss 19599064.00\n","==================================================================\n","Saved best model\n","Epoch 754, Loss 651255184.69, Val loss 19590442.00\n","==================================================================\n","Saved best model\n","Epoch 755, Loss 651126276.00, Val loss 19581480.00\n","==================================================================\n","Saved best model\n","Epoch 756, Loss 651003138.50, Val loss 19571604.00\n","==================================================================\n","Saved best model\n","Epoch 757, Loss 650909368.50, Val loss 19561476.00\n","==================================================================\n","Saved best model\n","Epoch 758, Loss 650829058.81, Val loss 19549602.00\n","==================================================================\n","Saved best model\n","Epoch 759, Loss 650751875.38, Val loss 19533578.00\n","==================================================================\n","Saved best model\n","Epoch 760, Loss 650618965.94, Val loss 19523824.00\n","==================================================================\n","Saved best model\n","Epoch 761, Loss 650341714.00, Val loss 19518786.00\n","==================================================================\n","Saved best model\n","Epoch 762, Loss 650148459.62, Val loss 19513736.00\n","==================================================================\n","Saved best model\n","Epoch 763, Loss 649925884.25, Val loss 19505376.00\n","==================================================================\n","Saved best model\n","Epoch 764, Loss 649738275.88, Val loss 19496274.00\n","==================================================================\n","Saved best model\n","Epoch 765, Loss 649578741.12, Val loss 19487688.00\n","==================================================================\n","Saved best model\n","Epoch 766, Loss 649798844.88, Val loss 19478728.00\n","==================================================================\n","Saved best model\n","Epoch 767, Loss 649247369.56, Val loss 19470320.00\n","==================================================================\n","Saved best model\n","Epoch 768, Loss 649074606.75, Val loss 19461354.00\n","==================================================================\n","Saved best model\n","Epoch 769, Loss 648925785.12, Val loss 19452888.00\n","==================================================================\n","Saved best model\n","Epoch 770, Loss 648763028.56, Val loss 19442990.00\n","==================================================================\n","Saved best model\n","Epoch 771, Loss 648610337.19, Val loss 19435544.00\n","==================================================================\n","Saved best model\n","Epoch 772, Loss 648456651.50, Val loss 19425980.00\n","Epoch 773, Loss 650868653.12, Val loss 19560262.00\n","Epoch 774, Loss 650716360.69, Val loss 19553046.00\n","Epoch 775, Loss 650527978.19, Val loss 19546166.00\n","==================================================================\n","Saved best model\n","Epoch 776, Loss 648253194.06, Val loss 19394026.00\n","==================================================================\n","Saved best model\n","Epoch 777, Loss 647705812.06, Val loss 19385578.00\n","==================================================================\n","Saved best model\n","Epoch 778, Loss 647546900.69, Val loss 19377986.00\n","==================================================================\n","Saved best model\n","Epoch 779, Loss 647384837.31, Val loss 19368258.00\n","==================================================================\n","Saved best model\n","Epoch 780, Loss 647252261.00, Val loss 19363566.00\n","==================================================================\n","Saved best model\n","Epoch 781, Loss 647092452.50, Val loss 19354814.00\n","==================================================================\n","Saved best model\n","Epoch 782, Loss 646947788.38, Val loss 19346592.00\n","==================================================================\n","Saved best model\n","Epoch 783, Loss 646819129.88, Val loss 19341400.00\n","==================================================================\n","Saved best model\n","Epoch 784, Loss 646666016.81, Val loss 19333148.00\n","==================================================================\n","Saved best model\n","Epoch 785, Loss 646523826.25, Val loss 19324832.00\n","==================================================================\n","Saved best model\n","Epoch 786, Loss 646372883.44, Val loss 19319278.00\n","==================================================================\n","Saved best model\n","Epoch 787, Loss 646246474.56, Val loss 19311026.00\n","==================================================================\n","Saved best model\n","Epoch 788, Loss 646066647.62, Val loss 19302532.00\n","==================================================================\n","Saved best model\n","Epoch 789, Loss 645952898.12, Val loss 19297584.00\n","==================================================================\n","Saved best model\n","Epoch 790, Loss 645794137.31, Val loss 19288662.00\n","==================================================================\n","Saved best model\n","Epoch 791, Loss 645668678.12, Val loss 19282530.00\n","==================================================================\n","Saved best model\n","Epoch 792, Loss 642242516.62, Val loss 18864436.00\n","==================================================================\n","Saved best model\n","Epoch 793, Loss 642238234.12, Val loss 18860292.00\n","==================================================================\n","Saved best model\n","Epoch 794, Loss 639433233.25, Val loss 18854492.00\n","Epoch 795, Loss 647418986.88, Val loss 19267988.00\n","Epoch 796, Loss 644994425.50, Val loss 19261474.00\n","Epoch 797, Loss 644831356.44, Val loss 19253356.00\n","Epoch 798, Loss 644686060.25, Val loss 19245252.00\n","Epoch 799, Loss 644570392.19, Val loss 19239502.00\n","Epoch 800, Loss 644416297.38, Val loss 19231260.00\n","Epoch 801, Loss 644304011.00, Val loss 19225404.00\n","Epoch 802, Loss 644149518.19, Val loss 19217086.00\n","Epoch 803, Loss 655994520.12, Val loss 19701348.00\n","Epoch 804, Loss 652407001.06, Val loss 19691602.00\n","Epoch 805, Loss 652430407.81, Val loss 19680178.00\n","Epoch 806, Loss 652133535.50, Val loss 19675152.00\n","Epoch 807, Loss 651918657.31, Val loss 19665648.00\n","Epoch 808, Loss 652212559.94, Val loss 19644426.00\n","Epoch 809, Loss 651968017.69, Val loss 19660558.00\n","Epoch 810, Loss 651290247.69, Val loss 19652556.00\n","Epoch 811, Loss 651164694.38, Val loss 19644544.00\n","Epoch 812, Loss 658639328.31, Val loss 20051818.00\n","Epoch 813, Loss 687967004.66, Val loss 20706658.00\n","Epoch 814, Loss 683826121.00, Val loss 20707938.00\n","Epoch 815, Loss 683479406.09, Val loss 20694956.00\n","Epoch 816, Loss 683287223.84, Val loss 20684600.00\n","Epoch 817, Loss 683011613.59, Val loss 20672662.00\n","Epoch 818, Loss 682743323.22, Val loss 20660690.00\n","Epoch 819, Loss 682478723.47, Val loss 20648778.00\n","Epoch 820, Loss 682216619.97, Val loss 20636838.00\n","Epoch 821, Loss 681959269.34, Val loss 20624684.00\n","Epoch 822, Loss 679031321.59, Val loss 20158994.00\n","Epoch 823, Loss 671027409.53, Val loss 20127232.00\n","Epoch 824, Loss 670269184.56, Val loss 20122432.00\n","Epoch 825, Loss 669996994.97, Val loss 20112188.00\n","Epoch 826, Loss 691013895.19, Val loss 20901344.00\n","Epoch 827, Loss 680209951.97, Val loss 20103606.00\n","Epoch 828, Loss 669792154.09, Val loss 20058030.00\n","Epoch 829, Loss 669437076.12, Val loss 20051934.00\n","Epoch 830, Loss 669197425.94, Val loss 20044126.00\n","Epoch 831, Loss 668909702.94, Val loss 20034682.00\n","Epoch 832, Loss 668635445.19, Val loss 20024628.00\n","Epoch 833, Loss 668385547.31, Val loss 20014382.00\n","Epoch 834, Loss 668159813.81, Val loss 20004114.00\n","Epoch 835, Loss 667942618.88, Val loss 19994002.00\n","Epoch 836, Loss 667729328.06, Val loss 19984064.00\n","Epoch 837, Loss 687732088.81, Val loss 20726628.00\n","Epoch 838, Loss 686306488.78, Val loss 20726718.00\n","Epoch 839, Loss 691522600.72, Val loss 20298138.00\n","Epoch 840, Loss 692436261.31, Val loss 20471738.00\n","Epoch 841, Loss 690341093.81, Val loss 20543410.00\n","Epoch 842, Loss 688956030.25, Val loss 20557476.00\n","Epoch 843, Loss 688065069.09, Val loss 20492622.00\n","Epoch 844, Loss 688170939.19, Val loss 20502874.00\n","Epoch 845, Loss 687549601.97, Val loss 20564650.00\n","Epoch 846, Loss 692732210.34, Val loss 20914462.00\n","Epoch 847, Loss 692192818.00, Val loss 20882590.00\n","Epoch 848, Loss 691968500.97, Val loss 20854154.00\n","Epoch 849, Loss 690248155.16, Val loss 20603494.00\n","Epoch 850, Loss 687114949.09, Val loss 20628200.00\n","Epoch 851, Loss 686912944.66, Val loss 20628870.00\n","Epoch 852, Loss 686865921.12, Val loss 20619732.00\n","Epoch 853, Loss 685987847.34, Val loss 20612692.00\n","Epoch 854, Loss 685679921.12, Val loss 20609132.00\n","Epoch 855, Loss 685238907.09, Val loss 20598422.00\n","Epoch 856, Loss 685473645.31, Val loss 20578632.00\n","Epoch 857, Loss 684733395.88, Val loss 20591996.00\n","Epoch 858, Loss 684808340.44, Val loss 20568860.00\n","Epoch 859, Loss 685794360.12, Val loss 20620100.00\n","Epoch 860, Loss 683454348.50, Val loss 20586366.00\n","Epoch 861, Loss 685109360.62, Val loss 20604578.00\n","Epoch 862, Loss 682838805.31, Val loss 20587074.00\n","Epoch 863, Loss 682610862.22, Val loss 20588846.00\n","Epoch 864, Loss 682233896.97, Val loss 20581572.00\n","Epoch 865, Loss 681722992.56, Val loss 20571434.00\n","Epoch 866, Loss 681451186.28, Val loss 20556848.00\n","Epoch 867, Loss 681345020.47, Val loss 20534828.00\n","Epoch 868, Loss 681192433.44, Val loss 20541486.00\n","Epoch 869, Loss 680851574.94, Val loss 20521168.00\n","Epoch 870, Loss 680212252.41, Val loss 20514280.00\n","Epoch 871, Loss 679928111.78, Val loss 20494324.00\n","Epoch 872, Loss 680623741.84, Val loss 20460670.00\n","Epoch 873, Loss 679701581.78, Val loss 20453942.00\n","Epoch 874, Loss 679264551.59, Val loss 20449054.00\n","Epoch 875, Loss 678962488.84, Val loss 20436330.00\n","Epoch 876, Loss 678532573.62, Val loss 20428068.00\n","Epoch 877, Loss 679171538.00, Val loss 20397654.00\n","Epoch 878, Loss 678129561.22, Val loss 20396352.00\n","Epoch 879, Loss 677778297.97, Val loss 20385752.00\n","Epoch 880, Loss 677479780.22, Val loss 20386352.00\n","Epoch 881, Loss 677102217.72, Val loss 20377136.00\n","Epoch 882, Loss 676620213.03, Val loss 20375532.00\n","Epoch 883, Loss 676237060.97, Val loss 20369272.00\n","Epoch 884, Loss 675868631.75, Val loss 20360068.00\n","Epoch 885, Loss 675491580.53, Val loss 20348566.00\n","Epoch 886, Loss 675205009.59, Val loss 20337544.00\n","Epoch 887, Loss 674939660.00, Val loss 20321428.00\n","Epoch 888, Loss 674890985.75, Val loss 20277544.00\n","Epoch 889, Loss 675005172.06, Val loss 20245422.00\n","Epoch 890, Loss 674798243.44, Val loss 20273808.00\n","Epoch 891, Loss 675249233.69, Val loss 20280726.00\n","Epoch 892, Loss 673923636.00, Val loss 20284472.00\n","Epoch 893, Loss 673177028.06, Val loss 20273498.00\n","Epoch 894, Loss 673070900.31, Val loss 20266892.00\n","Epoch 895, Loss 672682345.44, Val loss 20253804.00\n","Epoch 896, Loss 672486412.00, Val loss 20244322.00\n","Epoch 897, Loss 672182334.00, Val loss 20224120.00\n","Epoch 898, Loss 672015876.38, Val loss 20204318.00\n","Epoch 899, Loss 671822512.25, Val loss 20181122.00\n","Epoch 900, Loss 671705142.81, Val loss 20173006.00\n","Epoch 901, Loss 671319351.94, Val loss 20155648.00\n","Epoch 902, Loss 671043139.88, Val loss 20149498.00\n","Epoch 903, Loss 670958388.44, Val loss 20133528.00\n","Epoch 904, Loss 670601938.31, Val loss 20139686.00\n","Epoch 905, Loss 670473428.31, Val loss 20114072.00\n","Epoch 906, Loss 670149716.38, Val loss 20122256.00\n","Epoch 907, Loss 669894813.38, Val loss 20103704.00\n","Epoch 908, Loss 669543722.00, Val loss 20111014.00\n","Epoch 909, Loss 669309627.06, Val loss 20088810.00\n","Epoch 910, Loss 669083280.50, Val loss 20103250.00\n","Epoch 911, Loss 668791076.69, Val loss 20081914.00\n","Epoch 912, Loss 670060423.69, Val loss 20064086.00\n","Epoch 913, Loss 668503062.38, Val loss 20067790.00\n","Epoch 914, Loss 668080866.31, Val loss 20067414.00\n","Epoch 915, Loss 668007606.56, Val loss 20036830.00\n","Epoch 916, Loss 667762097.12, Val loss 20045838.00\n","Epoch 917, Loss 667451714.75, Val loss 20048588.00\n","Epoch 918, Loss 667279337.94, Val loss 20023524.00\n","Epoch 919, Loss 666904726.12, Val loss 20029480.00\n","Epoch 920, Loss 666754879.12, Val loss 20023604.00\n","Epoch 921, Loss 666619862.88, Val loss 20009414.00\n","Epoch 922, Loss 666398440.44, Val loss 20012730.00\n","Epoch 923, Loss 666236747.00, Val loss 20005146.00\n","Epoch 924, Loss 666759503.06, Val loss 19985662.00\n","Epoch 925, Loss 665702619.75, Val loss 19993414.00\n","Epoch 926, Loss 665434639.69, Val loss 19976216.00\n","Epoch 927, Loss 705718415.44, Val loss 21116240.00\n","Epoch 928, Loss 702871894.28, Val loss 21091092.00\n","Epoch 929, Loss 701621466.34, Val loss 21104614.00\n","Epoch 930, Loss 701834967.81, Val loss 21107334.00\n","Epoch 931, Loss 700581902.81, Val loss 21070258.00\n","Epoch 932, Loss 700345503.62, Val loss 21079510.00\n","Epoch 933, Loss 700146443.97, Val loss 21067156.00\n","Epoch 934, Loss 699787006.69, Val loss 21052402.00\n","Epoch 935, Loss 699471655.09, Val loss 21039038.00\n","Epoch 936, Loss 699193766.06, Val loss 21024750.00\n","Epoch 937, Loss 700436201.88, Val loss 21233276.00\n","Epoch 938, Loss 700437870.62, Val loss 21206380.00\n","Epoch 939, Loss 699639955.00, Val loss 21192310.00\n","Epoch 940, Loss 699842255.97, Val loss 21183980.00\n","Epoch 941, Loss 699264304.91, Val loss 21190036.00\n","Epoch 942, Loss 698245368.41, Val loss 21171726.00\n","Epoch 943, Loss 697945336.16, Val loss 21130562.00\n","Epoch 944, Loss 698099035.62, Val loss 21144228.00\n","Epoch 945, Loss 698152346.53, Val loss 21135416.00\n","Epoch 946, Loss 697158871.06, Val loss 21120802.00\n","Epoch 947, Loss 697092774.59, Val loss 21081216.00\n","Epoch 948, Loss 697196243.44, Val loss 21117948.00\n","Epoch 949, Loss 696752724.72, Val loss 21081666.00\n","Epoch 950, Loss 702204664.78, Val loss 21125268.00\n","Epoch 951, Loss 712970943.62, Val loss 21493002.00\n","Epoch 952, Loss 703359837.91, Val loss 21258704.00\n","Epoch 953, Loss 702830281.00, Val loss 21292538.00\n","Epoch 954, Loss 702179496.25, Val loss 21327402.00\n","Epoch 955, Loss 700265290.47, Val loss 21222140.00\n","Epoch 956, Loss 700125736.69, Val loss 21193986.00\n","Epoch 957, Loss 699009460.59, Val loss 21132776.00\n","Epoch 958, Loss 699667472.72, Val loss 21105678.00\n","Epoch 959, Loss 697173852.66, Val loss 21092960.00\n","Epoch 960, Loss 698243855.59, Val loss 21071908.00\n","Epoch 961, Loss 697444314.97, Val loss 21050042.00\n","Epoch 962, Loss 697139900.72, Val loss 21071480.00\n","Epoch 963, Loss 696690003.94, Val loss 21062730.00\n","Epoch 964, Loss 691295977.19, Val loss 20582036.00\n","Epoch 965, Loss 680931754.81, Val loss 20570466.00\n","Epoch 966, Loss 682187455.28, Val loss 21000444.00\n","Epoch 967, Loss 680532481.44, Val loss 20824600.00\n","Epoch 968, Loss 680459714.81, Val loss 20682628.00\n","Epoch 969, Loss 680294354.59, Val loss 20632146.00\n","Epoch 970, Loss 680118681.88, Val loss 20622568.00\n","Epoch 971, Loss 679712745.31, Val loss 20615196.00\n","Epoch 972, Loss 678542385.41, Val loss 20605080.00\n","Epoch 973, Loss 678129773.06, Val loss 20594790.00\n","Epoch 974, Loss 677735097.12, Val loss 20581200.00\n","Epoch 975, Loss 677364124.50, Val loss 20569710.00\n","Epoch 976, Loss 677003423.28, Val loss 20556186.00\n","Epoch 977, Loss 676683200.22, Val loss 20545550.00\n","Epoch 978, Loss 676314102.06, Val loss 20530116.00\n","Epoch 979, Loss 676071846.34, Val loss 20518582.00\n","Epoch 980, Loss 675716026.78, Val loss 20508654.00\n","Epoch 981, Loss 675303169.91, Val loss 20494422.00\n","Epoch 982, Loss 675019353.12, Val loss 20483068.00\n","Epoch 983, Loss 674692993.22, Val loss 20472854.00\n","Epoch 984, Loss 674328080.59, Val loss 20459978.00\n","Epoch 985, Loss 673854721.66, Val loss 20193602.00\n","Epoch 986, Loss 672767967.62, Val loss 20187880.00\n","Epoch 987, Loss 672404042.66, Val loss 20179858.00\n","Epoch 988, Loss 672082652.72, Val loss 20169930.00\n","Epoch 989, Loss 671782284.03, Val loss 20159442.00\n","Epoch 990, Loss 671507899.50, Val loss 20143026.00\n","Epoch 991, Loss 671261322.56, Val loss 20128268.00\n","Epoch 992, Loss 670964584.31, Val loss 20111258.00\n","Epoch 993, Loss 670703823.38, Val loss 20091872.00\n","Epoch 994, Loss 670461883.06, Val loss 20084226.00\n","Epoch 995, Loss 670165659.81, Val loss 20073978.00\n","Epoch 996, Loss 670106562.69, Val loss 20067192.00\n","Epoch 997, Loss 669573911.25, Val loss 20065346.00\n","Epoch 998, Loss 669257546.72, Val loss 20070892.00\n","Epoch 999, Loss 668920690.09, Val loss 20078988.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","import matplotlib.pyplot as plt\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 17\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"status":"ok","timestamp":1646443687200,"user_tz":360,"elapsed":388,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"1bf2aaa4-9c2c-4055-f55e-2f5c17188740"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["505495712.0\n","tensor([775.9082, 777.5519, 780.5554, 788.0889, 787.7352, 786.8952, 787.6926,\n","        790.0599, 797.1009, 799.2278, 798.0536, 799.2029, 802.5173, 810.4515,\n","        813.1431], grad_fn=<SelectBackward0>)\n","tensor([ 7467.0000,  9550.8750,  7866.1250,  8410.3750,  8329.6250,  8803.7500,\n","         7433.1250,  7433.1250,  7433.1250, 10268.8750, 10635.3750, 10933.0000,\n","        11784.6250, 10117.2500, 10117.2500])\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]}]}