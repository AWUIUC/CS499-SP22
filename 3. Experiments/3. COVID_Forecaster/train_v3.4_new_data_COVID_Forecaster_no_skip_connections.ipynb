{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.4_new_data_COVID_Forecaster_no_skip_connections.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNEaxMHMaFexIHz6hpkbEO7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1646782115052,"user_tz":360,"elapsed":19716,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"368c554e-f9ff-47f0-f431-d7c737927b4e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"58ac2a2e-f634-4c64-fc00-536a637d7526","executionInfo":{"status":"ok","timestamp":1646782164635,"user_tz":360,"elapsed":49591,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 7.2 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 9.2 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.12\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 8.2 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.5.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (747 kB)\n","\u001b[K     |████████████████████████████████| 747 kB 7.8 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.3.tar.gz (370 kB)\n","\u001b[K     |████████████████████████████████| 370 kB 8.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Collecting rdflib\n","  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n","\u001b[K     |████████████████████████████████| 482 kB 41.8 MB/s \n","\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Collecting isodate\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 459 kB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (4.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.10.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.3-py3-none-any.whl size=581968 sha256=9d2b9a502d5327175b1fb1df59dc4aea886c95a7e9d0981f2bcd87b71887cb8f\n","  Stored in directory: /root/.cache/pip/wheels/c3/2a/58/87ce0508964d4def1aafb92750c4f3ac77038efd1b9a89dcf5\n","Successfully built torch-geometric\n","Installing collected packages: isodate, yacs, rdflib, torch-geometric\n","Successfully installed isodate-0.6.1 rdflib-6.1.1 torch-geometric-2.0.3 yacs-0.1.8\n","Collecting torch-geometric-temporal\n","  Downloading torch_geometric_temporal-0.51.0.tar.gz (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: torch_sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.12)\n","Requirement already satisfied: torch_scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.1.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.13)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.6.3)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (6.1.1)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (57.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (4.11.2)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (0.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch_geometric->torch-geometric-temporal) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.1.0)\n","Building wheels for collected packages: torch-geometric-temporal\n","  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.51.0-py3-none-any.whl size=83569 sha256=0bb34bafab7dfd3bf289044f625bd26609d6f8985bd63a39467cce335f3d5bbc\n","  Stored in directory: /root/.cache/pip/wheels/a5/26/64/465700aa43b21fccca9ae446b407de2389f0ba16114e84db8d\n","Successfully built torch-geometric-temporal\n","Installing collected packages: torch-geometric-temporal\n","Successfully installed torch-geometric-temporal-0.51.0\n","Collecting ogb\n","  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Collecting outdated>=0.2.0\n","  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Collecting littleutils\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=85120024142314aad20238ca3932710c95bc5f71b9fd5d272e7854682559318b\n","  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n","PyTorch has version 1.10.0+cu111\n","Collecting epiweeks\n","  Downloading epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n","Installing collected packages: epiweeks\n","Successfully installed epiweeks-2.1.4\n","Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 24\n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3.3_to_3.5_new_data.pickle'\n","save_model_relative_path = './saved_models/v3.4_new_data_COVID_Forecaster_no_skip_connections'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3.4_archived_output.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1646782164637,"user_tz":360,"elapsed":42,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"866b8231-94e0-47e9-b9d8-f67f3987ee7d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_confirmed_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_confirmed_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_confirmed_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU","executionInfo":{"status":"ok","timestamp":1646782165268,"user_tz":360,"elapsed":657,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3","executionInfo":{"status":"ok","timestamp":1646782165270,"user_tz":360,"elapsed":14,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class COVID_forecaster_no_skip(torch.nn.Module):\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","      # self.linear2 = Linear(history_window, outputLayer_num_features)\n","      self.linear3 = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x) # + self.linear2(data.x[:, 0:history_window])\n","\n","      x = F.elu(x)\n","      x = self.linear3(x)\n","\n","      return x\n","\n","model = COVID_forecaster_no_skip().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj","executionInfo":{"status":"ok","timestamp":1646782165796,"user_tz":360,"elapsed":538,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646782165796,"user_tz":360,"elapsed":8,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"de348c13-e2ae-4356-a4b0-15eecd9b6cca"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["COVID_forecaster_no_skip(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n","  (linear3): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646783624901,"user_tz":360,"elapsed":1459108,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"958afad7-778c-4804-9215-ca840538f733"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 94081294595584.00, Val loss 4626474598400.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 93959983809536.00, Val loss 4622463795200.00\n","==================================================================\n","Saved best model\n","Epoch 2, Loss 93736735208960.00, Val loss 4615939031040.00\n","==================================================================\n","Saved best model\n","Epoch 3, Loss 93411794087168.00, Val loss 4606981046272.00\n","==================================================================\n","Saved best model\n","Epoch 4, Loss 92991189546496.00, Val loss 4595729825792.00\n","==================================================================\n","Saved best model\n","Epoch 5, Loss 92482570312448.00, Val loss 4582344228864.00\n","==================================================================\n","Saved best model\n","Epoch 6, Loss 91894166732544.00, Val loss 4566988881920.00\n","==================================================================\n","Saved best model\n","Epoch 7, Loss 91234410955776.00, Val loss 4549827362816.00\n","==================================================================\n","Saved best model\n","Epoch 8, Loss 90511716087552.00, Val loss 4531020627968.00\n","==================================================================\n","Saved best model\n","Epoch 9, Loss 89734373169152.00, Val loss 4510727012352.00\n","==================================================================\n","Saved best model\n","Epoch 10, Loss 88910445570560.00, Val loss 4489097510912.00\n","==================================================================\n","Saved best model\n","Epoch 11, Loss 88047727154944.00, Val loss 4466281021440.00\n","==================================================================\n","Saved best model\n","Epoch 12, Loss 87153688922880.00, Val loss 4442417528832.00\n","==================================================================\n","Saved best model\n","Epoch 13, Loss 86234155930368.00, Val loss 4417635483648.00\n","==================================================================\n","Saved best model\n","Epoch 14, Loss 85292932668416.00, Val loss 4392049442816.00\n","==================================================================\n","Saved best model\n","Epoch 15, Loss 84308234526720.00, Val loss 4365897695232.00\n","==================================================================\n","Saved best model\n","Epoch 16, Loss 83335823036416.00, Val loss 4338868289536.00\n","==================================================================\n","Saved best model\n","Epoch 17, Loss 82358331696128.00, Val loss 4311244865536.00\n","==================================================================\n","Saved best model\n","Epoch 18, Loss 81372948441600.00, Val loss 4282782842880.00\n","==================================================================\n","Saved best model\n","Epoch 19, Loss 80418731889664.00, Val loss 4255056920576.00\n","==================================================================\n","Saved best model\n","Epoch 20, Loss 79418108680192.00, Val loss 4226225012736.00\n","==================================================================\n","Saved best model\n","Epoch 21, Loss 78438374861824.00, Val loss 4197125980160.00\n","==================================================================\n","Saved best model\n","Epoch 22, Loss 77457817957376.00, Val loss 4168847982592.00\n","==================================================================\n","Saved best model\n","Epoch 23, Loss 76472921526272.00, Val loss 4138796318720.00\n","==================================================================\n","Saved best model\n","Epoch 24, Loss 75626922689536.00, Val loss 4109448773632.00\n","==================================================================\n","Saved best model\n","Epoch 25, Loss 74696386924544.00, Val loss 4079956000768.00\n","==================================================================\n","Saved best model\n","Epoch 26, Loss 73766659223552.00, Val loss 4052576894976.00\n","==================================================================\n","Saved best model\n","Epoch 27, Loss 72970255249408.00, Val loss 4026492256256.00\n","==================================================================\n","Saved best model\n","Epoch 28, Loss 72013236373504.00, Val loss 3996050522112.00\n","Epoch 29, Loss 72591478837248.00, Val loss 4014262976512.00\n","==================================================================\n","Saved best model\n","Epoch 30, Loss 71728420925440.00, Val loss 3986691194880.00\n","==================================================================\n","Saved best model\n","Epoch 31, Loss 70956250335232.00, Val loss 3957526364160.00\n","==================================================================\n","Saved best model\n","Epoch 32, Loss 70111766923264.00, Val loss 3931154939904.00\n","==================================================================\n","Saved best model\n","Epoch 33, Loss 69380765315072.00, Val loss 3905735884800.00\n","==================================================================\n","Saved best model\n","Epoch 34, Loss 68706201720832.00, Val loss 3876433428480.00\n","==================================================================\n","Saved best model\n","Epoch 35, Loss 67889684094976.00, Val loss 3848548122624.00\n","==================================================================\n","Saved best model\n","Epoch 36, Loss 67155806525440.00, Val loss 3824294559744.00\n","Epoch 37, Loss 68171743258624.00, Val loss 3857739677696.00\n","Epoch 38, Loss 67368809494528.00, Val loss 3831555162112.00\n","==================================================================\n","Saved best model\n","Epoch 39, Loss 66658211969024.00, Val loss 3805450076160.00\n","==================================================================\n","Saved best model\n","Epoch 40, Loss 66109132273664.00, Val loss 3776881623040.00\n","==================================================================\n","Saved best model\n","Epoch 41, Loss 65398270556160.00, Val loss 3755305074688.00\n","==================================================================\n","Saved best model\n","Epoch 42, Loss 65144783060992.00, Val loss 3726838857728.00\n","==================================================================\n","Saved best model\n","Epoch 43, Loss 64313773522944.00, Val loss 3703406329856.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 62901362339840.00, Val loss 3619164520448.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 63551969736704.00, Val loss 3616638763008.00\n","Epoch 46, Loss 63759996170240.00, Val loss 3688152432640.00\n","Epoch 47, Loss 62849063927808.00, Val loss 3663786409984.00\n","Epoch 48, Loss 62255212273664.00, Val loss 3624860385280.00\n","==================================================================\n","Saved best model\n","Epoch 49, Loss 61164822921216.00, Val loss 3569590468608.00\n","==================================================================\n","Saved best model\n","Epoch 50, Loss 61466281992192.00, Val loss 3546231341056.00\n","Epoch 51, Loss 60964693958656.00, Val loss 3558228623360.00\n","==================================================================\n","Saved best model\n","Epoch 52, Loss 60532997902336.00, Val loss 3540771143680.00\n","==================================================================\n","Saved best model\n","Epoch 53, Loss 59681585729536.00, Val loss 3480395186176.00\n","==================================================================\n","Saved best model\n","Epoch 54, Loss 59406542868480.00, Val loss 3462647250944.00\n","Epoch 55, Loss 59297238863872.00, Val loss 3478037200896.00\n","Epoch 56, Loss 59999951179776.00, Val loss 3509506277376.00\n","Epoch 57, Loss 59328484298752.00, Val loss 3513646055424.00\n","==================================================================\n","Saved best model\n","Epoch 58, Loss 58388722503680.00, Val loss 3453167599616.00\n","Epoch 59, Loss 58829081246720.00, Val loss 3470260174848.00\n","Epoch 60, Loss 59616340475904.00, Val loss 3490269364224.00\n","Epoch 61, Loss 59718083106816.00, Val loss 3508799012864.00\n","Epoch 62, Loss 59737167679488.00, Val loss 3482257719296.00\n","Epoch 63, Loss 59188316188672.00, Val loss 3462384058368.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 58597110374400.00, Val loss 3442428608512.00\n","==================================================================\n","Saved best model\n","Epoch 65, Loss 58369337053184.00, Val loss 3419730608128.00\n","Epoch 66, Loss 60039010095104.00, Val loss 3478893887488.00\n","Epoch 67, Loss 59604126572544.00, Val loss 3454336761856.00\n","Epoch 68, Loss 61503506276352.00, Val loss 3545299943424.00\n","Epoch 69, Loss 60638646878208.00, Val loss 3524768038912.00\n","==================================================================\n","Saved best model\n","Epoch 70, Loss 58715497631744.00, Val loss 3418640089088.00\n","==================================================================\n","Saved best model\n","Epoch 71, Loss 58558408704000.00, Val loss 3395981672448.00\n","==================================================================\n","Saved best model\n","Epoch 72, Loss 58076001001472.00, Val loss 3375467069440.00\n","Epoch 73, Loss 57964386021376.00, Val loss 3400861220864.00\n","Epoch 74, Loss 57309918541824.00, Val loss 3376554704896.00\n","==================================================================\n","Saved best model\n","Epoch 75, Loss 57430417309696.00, Val loss 3364471177216.00\n","==================================================================\n","Saved best model\n","Epoch 76, Loss 57309490847744.00, Val loss 3360022331392.00\n","==================================================================\n","Saved best model\n","Epoch 77, Loss 56550629613568.00, Val loss 3339261313024.00\n","Epoch 78, Loss 56051329937408.00, Val loss 3353695485952.00\n","==================================================================\n","Saved best model\n","Epoch 79, Loss 55373579108352.00, Val loss 3334739853312.00\n","==================================================================\n","Saved best model\n","Epoch 80, Loss 55489470435328.00, Val loss 3305498738688.00\n","==================================================================\n","Saved best model\n","Epoch 81, Loss 55921120663552.00, Val loss 3298031828992.00\n","Epoch 82, Loss 55080794517504.00, Val loss 3300155195392.00\n","Epoch 83, Loss 54447667412992.00, Val loss 3317563392000.00\n","==================================================================\n","Saved best model\n","Epoch 84, Loss 53520556937216.00, Val loss 3294301519872.00\n","==================================================================\n","Saved best model\n","Epoch 85, Loss 54159657140224.00, Val loss 3285639757824.00\n","==================================================================\n","Saved best model\n","Epoch 86, Loss 52852442816512.00, Val loss 3256870502400.00\n","==================================================================\n","Saved best model\n","Epoch 87, Loss 52899595481088.00, Val loss 3237225693184.00\n","==================================================================\n","Saved best model\n","Epoch 88, Loss 52715441315840.00, Val loss 3226767982592.00\n","Epoch 89, Loss 53460215156736.00, Val loss 3252955119616.00\n","Epoch 90, Loss 52641844512768.00, Val loss 3243955453952.00\n","Epoch 91, Loss 53253992953856.00, Val loss 3286913253376.00\n","==================================================================\n","Saved best model\n","Epoch 92, Loss 52128949522432.00, Val loss 3190390259712.00\n","Epoch 93, Loss 52729915828224.00, Val loss 3252589428736.00\n","Epoch 94, Loss 51550289668096.00, Val loss 3203010396160.00\n","==================================================================\n","Saved best model\n","Epoch 95, Loss 52290397749248.00, Val loss 3136460423168.00\n","Epoch 96, Loss 51756062048256.00, Val loss 3184707239936.00\n","==================================================================\n","Saved best model\n","Epoch 97, Loss 52919837976576.00, Val loss 3130123616256.00\n","Epoch 98, Loss 50992557404160.00, Val loss 3173875974144.00\n","Epoch 99, Loss 50692697796608.00, Val loss 3136860979200.00\n","Epoch 100, Loss 49650878267392.00, Val loss 3145798254592.00\n","==================================================================\n","Saved best model\n","Epoch 101, Loss 48983510536192.00, Val loss 3118728478720.00\n","==================================================================\n","Saved best model\n","Epoch 102, Loss 48820100612096.00, Val loss 3110084018176.00\n","==================================================================\n","Saved best model\n","Epoch 103, Loss 48397096452096.00, Val loss 3093461991424.00\n","==================================================================\n","Saved best model\n","Epoch 104, Loss 48372119425024.00, Val loss 3072691535872.00\n","==================================================================\n","Saved best model\n","Epoch 105, Loss 48027940003840.00, Val loss 3061409644544.00\n","==================================================================\n","Saved best model\n","Epoch 106, Loss 47716605460480.00, Val loss 3045689917440.00\n","==================================================================\n","Saved best model\n","Epoch 107, Loss 47402650099712.00, Val loss 3029000781824.00\n","==================================================================\n","Saved best model\n","Epoch 108, Loss 47233605214208.00, Val loss 3015764606976.00\n","==================================================================\n","Saved best model\n","Epoch 109, Loss 46982144847872.00, Val loss 3005811785728.00\n","==================================================================\n","Saved best model\n","Epoch 110, Loss 46812542775296.00, Val loss 2986343661568.00\n","==================================================================\n","Saved best model\n","Epoch 111, Loss 46500472942592.00, Val loss 2973259268096.00\n","==================================================================\n","Saved best model\n","Epoch 112, Loss 46364959748096.00, Val loss 2966656122880.00\n","==================================================================\n","Saved best model\n","Epoch 113, Loss 46087129792512.00, Val loss 2943861129216.00\n","==================================================================\n","Saved best model\n","Epoch 114, Loss 45932226539520.00, Val loss 2940257173504.00\n","==================================================================\n","Saved best model\n","Epoch 115, Loss 45661256048640.00, Val loss 2920094105600.00\n","==================================================================\n","Saved best model\n","Epoch 116, Loss 45526861983744.00, Val loss 2919541506048.00\n","Epoch 117, Loss 46316345645056.00, Val loss 2928512335872.00\n","Epoch 118, Loss 45284223451136.00, Val loss 2981866504192.00\n","Epoch 119, Loss 44901055782912.00, Val loss 2965573992448.00\n","Epoch 120, Loss 43891863824384.00, Val loss 2927921987584.00\n","Epoch 121, Loss 43893072054272.00, Val loss 2928764256256.00\n","==================================================================\n","Saved best model\n","Epoch 122, Loss 43660662872064.00, Val loss 2917875843072.00\n","==================================================================\n","Saved best model\n","Epoch 123, Loss 43283631325184.00, Val loss 2795796168704.00\n","Epoch 124, Loss 45235386470400.00, Val loss 2894551318528.00\n","Epoch 125, Loss 45604539183104.00, Val loss 2860730810368.00\n","Epoch 126, Loss 42939823656960.00, Val loss 2844616818688.00\n","Epoch 127, Loss 43750004666368.00, Val loss 2828627869696.00\n","Epoch 128, Loss 43183676577792.00, Val loss 2832199057408.00\n","Epoch 129, Loss 42458520313856.00, Val loss 2834150195200.00\n","Epoch 130, Loss 43089749700608.00, Val loss 2897176166400.00\n","Epoch 131, Loss 41916968914944.00, Val loss 2843128102912.00\n","Epoch 132, Loss 41867328641024.00, Val loss 2801067622400.00\n","==================================================================\n","Saved best model\n","Epoch 133, Loss 41759285757952.00, Val loss 2783441846272.00\n","==================================================================\n","Saved best model\n","Epoch 134, Loss 41535940225024.00, Val loss 2766445477888.00\n","Epoch 135, Loss 41477364512768.00, Val loss 2792174649344.00\n","Epoch 136, Loss 40943298301952.00, Val loss 2772752400384.00\n","Epoch 137, Loss 40576938936320.00, Val loss 2786336702464.00\n","==================================================================\n","Saved best model\n","Epoch 138, Loss 40169279365120.00, Val loss 2750252056576.00\n","Epoch 139, Loss 43770596428800.00, Val loss 2800724213760.00\n","Epoch 140, Loss 42779638734848.00, Val loss 2868267712512.00\n","Epoch 141, Loss 41765573865472.00, Val loss 2823594180608.00\n","Epoch 142, Loss 41134413842432.00, Val loss 2797587136512.00\n","Epoch 143, Loss 41279118336000.00, Val loss 2798889730048.00\n","==================================================================\n","Saved best model\n","Epoch 144, Loss 41698040172544.00, Val loss 2731204935680.00\n","Epoch 145, Loss 42494425571328.00, Val loss 2776205623296.00\n","Epoch 146, Loss 40968925138944.00, Val loss 2759273218048.00\n","Epoch 147, Loss 40491658815488.00, Val loss 2739673497600.00\n","Epoch 148, Loss 39821676240896.00, Val loss 2731438243840.00\n","==================================================================\n","Saved best model\n","Epoch 149, Loss 39667334883328.00, Val loss 2718725570560.00\n","==================================================================\n","Saved best model\n","Epoch 150, Loss 39405509302272.00, Val loss 2701402832896.00\n","==================================================================\n","Saved best model\n","Epoch 151, Loss 40141763248128.00, Val loss 2693074780160.00\n","==================================================================\n","Saved best model\n","Epoch 152, Loss 39408326985728.00, Val loss 2683058520064.00\n","==================================================================\n","Saved best model\n","Epoch 153, Loss 39057866366976.00, Val loss 2678659219456.00\n","==================================================================\n","Saved best model\n","Epoch 154, Loss 38900534210560.00, Val loss 2672224370688.00\n","==================================================================\n","Saved best model\n","Epoch 155, Loss 38724679895040.00, Val loss 2660699471872.00\n","==================================================================\n","Saved best model\n","Epoch 156, Loss 38562295425024.00, Val loss 2649393725440.00\n","==================================================================\n","Saved best model\n","Epoch 157, Loss 38347996954624.00, Val loss 2627983900672.00\n","==================================================================\n","Saved best model\n","Epoch 158, Loss 38279473971200.00, Val loss 2626838855680.00\n","==================================================================\n","Saved best model\n","Epoch 159, Loss 38100516868096.00, Val loss 2616355454976.00\n","==================================================================\n","Saved best model\n","Epoch 160, Loss 37948830783488.00, Val loss 2605691699200.00\n","==================================================================\n","Saved best model\n","Epoch 161, Loss 37826824009728.00, Val loss 2594964504576.00\n","==================================================================\n","Saved best model\n","Epoch 162, Loss 37663144120320.00, Val loss 2584506793984.00\n","==================================================================\n","Saved best model\n","Epoch 163, Loss 37574400176128.00, Val loss 2577488674816.00\n","Epoch 164, Loss 39656470429696.00, Val loss 2660522524672.00\n","Epoch 165, Loss 39103138621440.00, Val loss 2651202519040.00\n","Epoch 166, Loss 38943642388480.00, Val loss 2641886707712.00\n","Epoch 167, Loss 38849837897728.00, Val loss 2632531836928.00\n","Epoch 168, Loss 38770985084928.00, Val loss 2622750720000.00\n","Epoch 169, Loss 38618829299712.00, Val loss 2614297362432.00\n","Epoch 170, Loss 38448517808128.00, Val loss 2604919422976.00\n","Epoch 171, Loss 38320635703296.00, Val loss 2596030644224.00\n","Epoch 172, Loss 38194077044736.00, Val loss 2586222526464.00\n","Epoch 173, Loss 38063940935680.00, Val loss 2578155307008.00\n","==================================================================\n","Saved best model\n","Epoch 174, Loss 37929970941952.00, Val loss 2567679770624.00\n","==================================================================\n","Saved best model\n","Epoch 175, Loss 37839446519808.00, Val loss 2561250689024.00\n","==================================================================\n","Saved best model\n","Epoch 176, Loss 37710016770048.00, Val loss 2548980776960.00\n","==================================================================\n","Saved best model\n","Epoch 177, Loss 37657553252352.00, Val loss 2546423562240.00\n","==================================================================\n","Saved best model\n","Epoch 178, Loss 37511150514176.00, Val loss 2530480226304.00\n","Epoch 179, Loss 37508537155584.00, Val loss 2534230720512.00\n","==================================================================\n","Saved best model\n","Epoch 180, Loss 37327680724992.00, Val loss 2514473713664.00\n","Epoch 181, Loss 37339876515840.00, Val loss 2519313940480.00\n","Epoch 182, Loss 40977681686528.00, Val loss 2624204832768.00\n","Epoch 183, Loss 40319713669120.00, Val loss 2652809199616.00\n","Epoch 184, Loss 40118264786944.00, Val loss 2636197658624.00\n","Epoch 185, Loss 40058040504320.00, Val loss 2639125807104.00\n","Epoch 186, Loss 39900684713984.00, Val loss 2623742148608.00\n","Epoch 187, Loss 39811479089152.00, Val loss 2620133212160.00\n","Epoch 188, Loss 39690907701248.00, Val loss 2605281705984.00\n","Epoch 189, Loss 39601111625728.00, Val loss 2605484867584.00\n","Epoch 190, Loss 39512164319232.00, Val loss 2599029833728.00\n","Epoch 191, Loss 39448500776960.00, Val loss 2593639366656.00\n","Epoch 192, Loss 39370445123584.00, Val loss 2588324397056.00\n","Epoch 193, Loss 39316387536896.00, Val loss 2584091820032.00\n","Epoch 194, Loss 39244082950144.00, Val loss 2579900923904.00\n","Epoch 195, Loss 39174281875456.00, Val loss 2576309551104.00\n","Epoch 196, Loss 39077151096832.00, Val loss 2572193890304.00\n","Epoch 197, Loss 38969185857536.00, Val loss 2568320450560.00\n","Epoch 198, Loss 38855916523520.00, Val loss 2559030853632.00\n","Epoch 199, Loss 38781785997312.00, Val loss 2558682464256.00\n","Epoch 200, Loss 38702345089024.00, Val loss 2554066370560.00\n","Epoch 201, Loss 38632223469568.00, Val loss 2551271129088.00\n","Epoch 202, Loss 38502790397952.00, Val loss 2548755857408.00\n","Epoch 203, Loss 38290729177088.00, Val loss 2545971888128.00\n","Epoch 204, Loss 38166177140736.00, Val loss 2541453312000.00\n","==================================================================\n","Saved best model\n","Epoch 205, Loss 37921370644480.00, Val loss 2476474630144.00\n","Epoch 206, Loss 38010334437376.00, Val loss 2481879777280.00\n","==================================================================\n","Saved best model\n","Epoch 207, Loss 37765983629312.00, Val loss 2471018364928.00\n","Epoch 208, Loss 38125498060800.00, Val loss 2502582337536.00\n","Epoch 209, Loss 37154713686016.00, Val loss 2503188414464.00\n","Epoch 210, Loss 36723574415360.00, Val loss 2486486695936.00\n","Epoch 211, Loss 36957665824768.00, Val loss 2472706310144.00\n","Epoch 212, Loss 37773342904320.00, Val loss 2482786795520.00\n","==================================================================\n","Saved best model\n","Epoch 213, Loss 36150222802944.00, Val loss 2465238614016.00\n","==================================================================\n","Saved best model\n","Epoch 214, Loss 36112762368000.00, Val loss 2460114223104.00\n","Epoch 215, Loss 36225532006400.00, Val loss 2461521149952.00\n","==================================================================\n","Saved best model\n","Epoch 216, Loss 35756596285440.00, Val loss 2445238337536.00\n","Epoch 217, Loss 36211940712448.00, Val loss 2448055861248.00\n","Epoch 218, Loss 35969202294784.00, Val loss 2448472408064.00\n","==================================================================\n","Saved best model\n","Epoch 219, Loss 35670451605504.00, Val loss 2429539057664.00\n","==================================================================\n","Saved best model\n","Epoch 220, Loss 36348827467776.00, Val loss 2395293876224.00\n","Epoch 221, Loss 42344373997568.00, Val loss 2529812545536.00\n","Epoch 222, Loss 40149083918336.00, Val loss 2596058693632.00\n","Epoch 223, Loss 38820285755392.00, Val loss 2544107782144.00\n","Epoch 224, Loss 36967678703616.00, Val loss 2509951991808.00\n","Epoch 225, Loss 38930405222400.00, Val loss 2572818579456.00\n","Epoch 226, Loss 38540370098176.00, Val loss 2609663705088.00\n","Epoch 227, Loss 38426411491328.00, Val loss 2611964280832.00\n","Epoch 228, Loss 37973311520768.00, Val loss 2557832069120.00\n","Epoch 229, Loss 38182911504384.00, Val loss 2599235092480.00\n","Epoch 230, Loss 37798923182080.00, Val loss 2589267853312.00\n","Epoch 231, Loss 37235731720192.00, Val loss 2520945524736.00\n","Epoch 232, Loss 37871018500096.00, Val loss 2580711735296.00\n","Epoch 233, Loss 37852386820096.00, Val loss 2568276934656.00\n","Epoch 234, Loss 37152333443072.00, Val loss 2559640338432.00\n","Epoch 235, Loss 36744050092032.00, Val loss 2551800659968.00\n","Epoch 236, Loss 36688476100608.00, Val loss 2528441270272.00\n","Epoch 237, Loss 36871480750080.00, Val loss 2507183751168.00\n","Epoch 238, Loss 36941083121664.00, Val loss 2527882903552.00\n","Epoch 239, Loss 36255834595328.00, Val loss 2515202473984.00\n","Epoch 240, Loss 36374377291776.00, Val loss 2514465062912.00\n","Epoch 241, Loss 36560485529600.00, Val loss 2497408663552.00\n","Epoch 242, Loss 36120192407552.00, Val loss 2480496705536.00\n","Epoch 243, Loss 36018180923392.00, Val loss 2489244450816.00\n","Epoch 244, Loss 35919027689472.00, Val loss 2436945936384.00\n","Epoch 245, Loss 36331658586112.00, Val loss 2432906297344.00\n","Epoch 246, Loss 36823100825600.00, Val loss 2430736007168.00\n","Epoch 247, Loss 35942737870848.00, Val loss 2430812028928.00\n","Epoch 248, Loss 36681715505152.00, Val loss 2438603997184.00\n","Epoch 249, Loss 35733589116928.00, Val loss 2428131606528.00\n","Epoch 250, Loss 35527194075136.00, Val loss 2463884640256.00\n","Epoch 251, Loss 35275128010752.00, Val loss 2433740963840.00\n","Epoch 252, Loss 34767922403328.00, Val loss 2401657159680.00\n","Epoch 253, Loss 35807529478144.00, Val loss 2446638186496.00\n","Epoch 254, Loss 34721469513728.00, Val loss 2395593768960.00\n","Epoch 255, Loss 35137033414656.00, Val loss 2433000144896.00\n","==================================================================\n","Saved best model\n","Epoch 256, Loss 34697283112960.00, Val loss 2394958331904.00\n","==================================================================\n","Saved best model\n","Epoch 257, Loss 34728825423872.00, Val loss 2379396677632.00\n","Epoch 258, Loss 34940830924800.00, Val loss 2409572335616.00\n","==================================================================\n","Saved best model\n","Epoch 259, Loss 34354606057472.00, Val loss 2349142900736.00\n","Epoch 260, Loss 34774716303360.00, Val loss 2418419433472.00\n","==================================================================\n","Saved best model\n","Epoch 261, Loss 34745161617408.00, Val loss 2346991484928.00\n","Epoch 262, Loss 34947644854272.00, Val loss 2356580974592.00\n","Epoch 263, Loss 34575239479296.00, Val loss 2355222020096.00\n","==================================================================\n","Saved best model\n","Epoch 264, Loss 34682053279744.00, Val loss 2333210836992.00\n","Epoch 265, Loss 34599141840896.00, Val loss 2349833650176.00\n","==================================================================\n","Saved best model\n","Epoch 266, Loss 34534224631808.00, Val loss 2329337921536.00\n","Epoch 267, Loss 34398296227840.00, Val loss 2330895056896.00\n","==================================================================\n","Saved best model\n","Epoch 268, Loss 34390279999488.00, Val loss 2315460018176.00\n","Epoch 269, Loss 34246203312128.00, Val loss 2333503127552.00\n","Epoch 270, Loss 34297809360896.00, Val loss 2323364970496.00\n","==================================================================\n","Saved best model\n","Epoch 271, Loss 34007483643904.00, Val loss 2308474929152.00\n","Epoch 272, Loss 34268201590784.00, Val loss 2314279845888.00\n","Epoch 273, Loss 34168192038912.00, Val loss 2326278701056.00\n","==================================================================\n","Saved best model\n","Epoch 274, Loss 33809235689472.00, Val loss 2279717732352.00\n","Epoch 275, Loss 34028528949248.00, Val loss 2293453815808.00\n","Epoch 276, Loss 34200786864128.00, Val loss 2310930956288.00\n","Epoch 277, Loss 33778641438720.00, Val loss 2292455309312.00\n","==================================================================\n","Saved best model\n","Epoch 278, Loss 33927152828416.00, Val loss 2271043387392.00\n","Epoch 279, Loss 33992784162816.00, Val loss 2286144716800.00\n","==================================================================\n","Saved best model\n","Epoch 280, Loss 33846869581824.00, Val loss 2257668800512.00\n","Epoch 281, Loss 33986551617536.00, Val loss 2279878688768.00\n","==================================================================\n","Saved best model\n","Epoch 282, Loss 33868483905536.00, Val loss 2256985653248.00\n","Epoch 283, Loss 33772026359808.00, Val loss 2270019977216.00\n","Epoch 284, Loss 33927416057856.00, Val loss 2257090772992.00\n","Epoch 285, Loss 33674799742976.00, Val loss 2265796575232.00\n","==================================================================\n","Saved best model\n","Epoch 286, Loss 33738881654784.00, Val loss 2242522120192.00\n","Epoch 287, Loss 33602915975168.00, Val loss 2269761241088.00\n","==================================================================\n","Saved best model\n","Epoch 288, Loss 33444618186752.00, Val loss 2229171912704.00\n","Epoch 289, Loss 33569015674880.00, Val loss 2263207903232.00\n","==================================================================\n","Saved best model\n","Epoch 290, Loss 33404828139520.00, Val loss 2222377140224.00\n","Epoch 291, Loss 33504300087296.00, Val loss 2255881240576.00\n","==================================================================\n","Saved best model\n","Epoch 292, Loss 33424395089920.00, Val loss 2209434828800.00\n","Epoch 293, Loss 33735943540736.00, Val loss 2239552290816.00\n","Epoch 294, Loss 33917435330560.00, Val loss 2257610342400.00\n","Epoch 295, Loss 33271927531520.00, Val loss 2247535886336.00\n","==================================================================\n","Saved best model\n","Epoch 296, Loss 33466188173312.00, Val loss 2203891269632.00\n","Epoch 297, Loss 33197688692736.00, Val loss 2234527514624.00\n","==================================================================\n","Saved best model\n","Epoch 298, Loss 33357519470592.00, Val loss 2203616542720.00\n","Epoch 299, Loss 33232946526208.00, Val loss 2234218708992.00\n","==================================================================\n","Saved best model\n","Epoch 300, Loss 33379449626624.00, Val loss 2197744386048.00\n","Epoch 301, Loss 32995426910208.00, Val loss 2215646068736.00\n","==================================================================\n","Saved best model\n","Epoch 302, Loss 33110415130624.00, Val loss 2188924026880.00\n","Epoch 303, Loss 33115044577280.00, Val loss 2221672759296.00\n","Epoch 304, Loss 33294452916224.00, Val loss 2200932188160.00\n","Epoch 305, Loss 36043415046144.00, Val loss 2389633138688.00\n","Epoch 306, Loss 35137727537152.00, Val loss 2386344280064.00\n","Epoch 307, Loss 34585140709376.00, Val loss 2394323943424.00\n","Epoch 308, Loss 34208332548096.00, Val loss 2385791942656.00\n","Epoch 309, Loss 34011757627392.00, Val loss 2424528961536.00\n","Epoch 310, Loss 33314387052544.00, Val loss 2396538798080.00\n","Epoch 311, Loss 33408214112256.00, Val loss 2409562112000.00\n","Epoch 312, Loss 33116420669440.00, Val loss 2393978961920.00\n","Epoch 313, Loss 33063589681152.00, Val loss 2376944582656.00\n","Epoch 314, Loss 33025468405760.00, Val loss 2331631943680.00\n","Epoch 315, Loss 33251583842304.00, Val loss 2318186053632.00\n","Epoch 316, Loss 34760842809344.00, Val loss 2194748997632.00\n","Epoch 317, Loss 38155733065728.00, Val loss 2424011489280.00\n","Epoch 318, Loss 34980746878976.00, Val loss 2288336240640.00\n","Epoch 319, Loss 40465691742208.00, Val loss 2350333820928.00\n","Epoch 320, Loss 36838802546688.00, Val loss 2418819727360.00\n","Epoch 321, Loss 35653828050944.00, Val loss 2528753483776.00\n","Epoch 322, Loss 33986556342272.00, Val loss 2461630201856.00\n","Epoch 323, Loss 34649762021376.00, Val loss 2448885284864.00\n","Epoch 324, Loss 34090988601344.00, Val loss 2431152291840.00\n","Epoch 325, Loss 34044280401920.00, Val loss 2421803974656.00\n","Epoch 326, Loss 33991407429632.00, Val loss 2419885342720.00\n","Epoch 327, Loss 34078014433280.00, Val loss 2447038742528.00\n","Epoch 328, Loss 33629759528960.00, Val loss 2446286651392.00\n","Epoch 329, Loss 33018582740992.00, Val loss 2427839315968.00\n","Epoch 330, Loss 32794629836800.00, Val loss 2390703472640.00\n","Epoch 331, Loss 33601330370560.00, Val loss 2449580752896.00\n","Epoch 332, Loss 32335059728384.00, Val loss 2351049736192.00\n","Epoch 333, Loss 33839837829120.00, Val loss 2425405571072.00\n","Epoch 334, Loss 32259064991744.00, Val loss 2359297572864.00\n","Epoch 335, Loss 32681064132608.00, Val loss 2394486996992.00\n","Epoch 336, Loss 32750749982720.00, Val loss 2396641558528.00\n","Epoch 337, Loss 32263861293056.00, Val loss 2356799078400.00\n","Epoch 338, Loss 32970821128192.00, Val loss 2404226170880.00\n","Epoch 339, Loss 32022168956928.00, Val loss 2328026415104.00\n","Epoch 340, Loss 32285392769024.00, Val loss 2350815117312.00\n","Epoch 341, Loss 33428201330688.00, Val loss 2379914674176.00\n","Epoch 342, Loss 31766482231296.00, Val loss 2337470152704.00\n","Epoch 343, Loss 31941892968448.00, Val loss 2334500847616.00\n","Epoch 344, Loss 32100541693952.00, Val loss 2311315259392.00\n","Epoch 345, Loss 31672272437248.00, Val loss 2287288975360.00\n","Epoch 346, Loss 32087484606464.00, Val loss 2310880362496.00\n","Epoch 347, Loss 32026538131456.00, Val loss 2289977524224.00\n","Epoch 348, Loss 31661446307840.00, Val loss 2284264882176.00\n","Epoch 349, Loss 31824249573376.00, Val loss 2290018680832.00\n","Epoch 350, Loss 35861595330560.00, Val loss 2380794691584.00\n","Epoch 351, Loss 38480209596416.00, Val loss 2406256214016.00\n","Epoch 352, Loss 36801606404096.00, Val loss 2466879635456.00\n","Epoch 353, Loss 39430020739072.00, Val loss 2682201833472.00\n","Epoch 354, Loss 36713298012160.00, Val loss 2606741585920.00\n","Epoch 355, Loss 35933464576000.00, Val loss 2580272906240.00\n","Epoch 356, Loss 34868290883584.00, Val loss 2522433191936.00\n","Epoch 357, Loss 36163265865728.00, Val loss 2591780765696.00\n","Epoch 358, Loss 35053546444800.00, Val loss 2575108669440.00\n","Epoch 359, Loss 34759249182720.00, Val loss 2505895837696.00\n","Epoch 360, Loss 34819002132480.00, Val loss 2490196295680.00\n","Epoch 361, Loss 34966414641152.00, Val loss 2503791345664.00\n","Epoch 362, Loss 34182885871616.00, Val loss 2490193149952.00\n","Epoch 363, Loss 34230837547008.00, Val loss 2492272476160.00\n","Epoch 364, Loss 33495892736000.00, Val loss 2473353019392.00\n","Epoch 365, Loss 33291585839104.00, Val loss 2460764340224.00\n","Epoch 366, Loss 33077052137472.00, Val loss 2447345451008.00\n","Epoch 367, Loss 32945370277888.00, Val loss 2438475022336.00\n","Epoch 368, Loss 32836465532928.00, Val loss 2427811528704.00\n","Epoch 369, Loss 32738652444672.00, Val loss 2416674078720.00\n","Epoch 370, Loss 32666423300096.00, Val loss 2407483047936.00\n","Epoch 371, Loss 32596662341632.00, Val loss 2397419864064.00\n","Epoch 372, Loss 32543364935680.00, Val loss 2387987922944.00\n","Epoch 373, Loss 32492489654272.00, Val loss 2378817601536.00\n","Epoch 374, Loss 32446934478848.00, Val loss 2370072477696.00\n","Epoch 375, Loss 32400639369216.00, Val loss 2361423560704.00\n","Epoch 376, Loss 32352216285184.00, Val loss 2353288708096.00\n","Epoch 377, Loss 32290881171456.00, Val loss 2344935227392.00\n","Epoch 378, Loss 32225365245952.00, Val loss 2337311817728.00\n","Epoch 379, Loss 35027863867392.00, Val loss 2399915999232.00\n","Epoch 380, Loss 37539753709568.00, Val loss 2398242734080.00\n","Epoch 381, Loss 36854750758912.00, Val loss 2467520053248.00\n","Epoch 382, Loss 38780828553216.00, Val loss 2643074220032.00\n","Epoch 383, Loss 36851455946752.00, Val loss 2592799981568.00\n","Epoch 384, Loss 36759038009344.00, Val loss 2569392619520.00\n","Epoch 385, Loss 36637930022912.00, Val loss 2557011296256.00\n","Epoch 386, Loss 36294532972544.00, Val loss 2548495286272.00\n","Epoch 387, Loss 36345953888256.00, Val loss 2536484896768.00\n","Epoch 388, Loss 36100731211776.00, Val loss 2528369704960.00\n","Epoch 389, Loss 36186068606976.00, Val loss 2519820140544.00\n","Epoch 390, Loss 35924101009408.00, Val loss 2509290602496.00\n","Epoch 391, Loss 35975133351936.00, Val loss 2499788406784.00\n","Epoch 392, Loss 35842631880704.00, Val loss 2502685622272.00\n","Epoch 393, Loss 35610721435648.00, Val loss 2491749236736.00\n","Epoch 394, Loss 35769480413184.00, Val loss 2485484257280.00\n","Epoch 395, Loss 35462203834368.00, Val loss 2477894926336.00\n","Epoch 396, Loss 35077852487680.00, Val loss 2470607585280.00\n","Epoch 397, Loss 35876462213120.00, Val loss 2518740107264.00\n","Epoch 398, Loss 34475106975744.00, Val loss 2449150836736.00\n","Epoch 399, Loss 35672934436864.00, Val loss 2506462068736.00\n","Epoch 400, Loss 35180829007872.00, Val loss 2496779517952.00\n","Epoch 401, Loss 35099774832640.00, Val loss 2487842242560.00\n","Epoch 402, Loss 35009626628096.00, Val loss 2476383141888.00\n","Epoch 403, Loss 34953184333824.00, Val loss 2467165110272.00\n","Epoch 404, Loss 34917533470720.00, Val loss 2463180521472.00\n","Epoch 405, Loss 34805297635328.00, Val loss 2441120841728.00\n","Epoch 406, Loss 34993649139712.00, Val loss 2438962610176.00\n","Epoch 407, Loss 33996712267776.00, Val loss 2377574776832.00\n","Epoch 408, Loss 34977787154432.00, Val loss 2437561712640.00\n","Epoch 409, Loss 34631719231488.00, Val loss 2430545690624.00\n","Epoch 410, Loss 34631728988160.00, Val loss 2424220942336.00\n","Epoch 411, Loss 34552347000832.00, Val loss 2416828481536.00\n","Epoch 412, Loss 34494375579648.00, Val loss 2411390042112.00\n","Epoch 413, Loss 34444096487424.00, Val loss 2405327699968.00\n","Epoch 414, Loss 34397274042368.00, Val loss 2398056873984.00\n","Epoch 415, Loss 34346828709888.00, Val loss 2393222938624.00\n","Epoch 416, Loss 34301222023168.00, Val loss 2385072095232.00\n","Epoch 417, Loss 33494675943424.00, Val loss 2320671703040.00\n","Epoch 418, Loss 34381007544320.00, Val loss 2335666864128.00\n","Epoch 419, Loss 34647201751040.00, Val loss 2378374840320.00\n","Epoch 420, Loss 33592953671680.00, Val loss 2304475660288.00\n","Epoch 421, Loss 34945238863872.00, Val loss 2418314313728.00\n","Epoch 422, Loss 37512931180544.00, Val loss 2509608845312.00\n","Epoch 423, Loss 38824561221632.00, Val loss 2458280263680.00\n","Epoch 424, Loss 37627640266752.00, Val loss 2510793211904.00\n","Epoch 425, Loss 36712035594240.00, Val loss 2508222365696.00\n","Epoch 426, Loss 36808907677696.00, Val loss 2500572741632.00\n","Epoch 427, Loss 36172118749184.00, Val loss 2495118311424.00\n","Epoch 428, Loss 35813278633984.00, Val loss 2489705562112.00\n","Epoch 429, Loss 35689433690112.00, Val loss 2484638318592.00\n","Epoch 430, Loss 35632298364928.00, Val loss 2479124381696.00\n","Epoch 431, Loss 35573566517248.00, Val loss 2474056089600.00\n","Epoch 432, Loss 35521759309824.00, Val loss 2468320116736.00\n","Epoch 433, Loss 35477272031232.00, Val loss 2462590173184.00\n","Epoch 434, Loss 35427742248960.00, Val loss 2457497763840.00\n","Epoch 435, Loss 35382774169600.00, Val loss 2452137705472.00\n","Epoch 436, Loss 35342543736832.00, Val loss 2446964031488.00\n","Epoch 437, Loss 35303526739968.00, Val loss 2442231021568.00\n","Epoch 438, Loss 35264193318912.00, Val loss 2437316870144.00\n","Epoch 439, Loss 35229645815808.00, Val loss 2432912326656.00\n","Epoch 440, Loss 35198171209728.00, Val loss 2428475277312.00\n","Epoch 441, Loss 35169679265792.00, Val loss 2424121851904.00\n","Epoch 442, Loss 35144916488192.00, Val loss 2420469399552.00\n","Epoch 443, Loss 35119832440832.00, Val loss 2416925999104.00\n","Epoch 444, Loss 35096566001664.00, Val loss 2413765591040.00\n","Epoch 445, Loss 35072340930560.00, Val loss 2410450255872.00\n","Epoch 446, Loss 35048953229312.00, Val loss 2407870496768.00\n","Epoch 447, Loss 35024191299584.00, Val loss 2404913774592.00\n","Epoch 448, Loss 34999063379968.00, Val loss 2401855602688.00\n","Epoch 449, Loss 34972310056960.00, Val loss 2398644862976.00\n","Epoch 450, Loss 34943129698304.00, Val loss 2395212873728.00\n","Epoch 451, Loss 34898201632768.00, Val loss 2391982473216.00\n","Epoch 452, Loss 34817154240512.00, Val loss 2388144422912.00\n","Epoch 453, Loss 34847460605952.00, Val loss 2384823058432.00\n","Epoch 454, Loss 34789820964864.00, Val loss 2381414400000.00\n","Epoch 455, Loss 34564614737920.00, Val loss 2274545106944.00\n","Epoch 456, Loss 33520081416192.00, Val loss 2361654771712.00\n","Epoch 457, Loss 34533400272896.00, Val loss 2358159605760.00\n","Epoch 458, Loss 36621455425536.00, Val loss 2279584825344.00\n","Epoch 459, Loss 34950346506240.00, Val loss 2358355165184.00\n","Epoch 460, Loss 37603224772608.00, Val loss 2475175968768.00\n","Epoch 461, Loss 38122153414656.00, Val loss 2574417395712.00\n","Epoch 462, Loss 37702408630272.00, Val loss 2563284402176.00\n","Epoch 463, Loss 37587384344576.00, Val loss 2559849267200.00\n","Epoch 464, Loss 37377775493120.00, Val loss 2551044636672.00\n","Epoch 465, Loss 37220581138432.00, Val loss 2541701038080.00\n","Epoch 466, Loss 36938475278336.00, Val loss 2532113383424.00\n","Epoch 467, Loss 36937633337344.00, Val loss 2516398374912.00\n","Epoch 468, Loss 36765734752256.00, Val loss 2507841208320.00\n","Epoch 469, Loss 36648510119936.00, Val loss 2502749585408.00\n","Epoch 470, Loss 36239111671808.00, Val loss 2499932585984.00\n","Epoch 471, Loss 36397993984000.00, Val loss 2459657568256.00\n","Epoch 472, Loss 36454147928064.00, Val loss 2523467874304.00\n","Epoch 473, Loss 35674888417280.00, Val loss 2502524928000.00\n","Epoch 474, Loss 36510349758464.00, Val loss 2479660728320.00\n","Epoch 475, Loss 35595066789888.00, Val loss 2480859774976.00\n","Epoch 476, Loss 35413010984960.00, Val loss 2473437954048.00\n","Epoch 477, Loss 35151381127168.00, Val loss 2460202041344.00\n","Epoch 478, Loss 35010421727232.00, Val loss 2455013163008.00\n","Epoch 479, Loss 34921736609792.00, Val loss 2446524678144.00\n","Epoch 480, Loss 34832650579968.00, Val loss 2439358971904.00\n","Epoch 481, Loss 34786301620224.00, Val loss 2431536594944.00\n","Epoch 482, Loss 34742751027200.00, Val loss 2423888019456.00\n","Epoch 483, Loss 34716745605120.00, Val loss 2416762421248.00\n","Epoch 484, Loss 34702310363136.00, Val loss 2410403332096.00\n","Epoch 485, Loss 34706077528064.00, Val loss 2405160976384.00\n","Epoch 486, Loss 34779947696128.00, Val loss 2400659701760.00\n","Epoch 487, Loss 34773933203456.00, Val loss 2396680093696.00\n","Epoch 488, Loss 34831026118656.00, Val loss 2393907920896.00\n","Epoch 489, Loss 34943280508928.00, Val loss 2391608918016.00\n","Epoch 490, Loss 35122648670208.00, Val loss 2399164694528.00\n","Epoch 491, Loss 35136864092160.00, Val loss 2420832731136.00\n","Epoch 492, Loss 34147041304576.00, Val loss 2373881167872.00\n","Epoch 493, Loss 34734952931328.00, Val loss 2378932944896.00\n","Epoch 494, Loss 34980128432128.00, Val loss 2430227447808.00\n","Epoch 495, Loss 33685515505664.00, Val loss 2358716661760.00\n","Epoch 496, Loss 34445344493568.00, Val loss 2363895054336.00\n","Epoch 497, Loss 34582392283136.00, Val loss 2360979750912.00\n","Epoch 498, Loss 34541677760512.00, Val loss 2372902584320.00\n","Epoch 499, Loss 34266355949568.00, Val loss 2330162626560.00\n","Epoch 500, Loss 34631857889280.00, Val loss 2347388633088.00\n","Epoch 501, Loss 33590420336640.00, Val loss 2294227402752.00\n","==================================================================\n","Saved best model\n","Epoch 502, Loss 30696444792832.00, Val loss 2120561065984.00\n","==================================================================\n","Saved best model\n","Epoch 503, Loss 33108592922624.00, Val loss 2009365348352.00\n","Epoch 504, Loss 31208057819136.00, Val loss 2015309987840.00\n","==================================================================\n","Saved best model\n","Epoch 505, Loss 30907468206080.00, Val loss 2008510889984.00\n","Epoch 506, Loss 30611550351360.00, Val loss 2009352765440.00\n","Epoch 507, Loss 30571829432320.00, Val loss 2044322643968.00\n","Epoch 508, Loss 33559270952960.00, Val loss 2184343584768.00\n","Epoch 509, Loss 35526714636288.00, Val loss 2165773041664.00\n","Epoch 510, Loss 31205855809536.00, Val loss 2059125391360.00\n","Epoch 511, Loss 30307026305024.00, Val loss 2063763505152.00\n","Epoch 512, Loss 30497780703232.00, Val loss 2116408573952.00\n","Epoch 513, Loss 29835507654656.00, Val loss 2040654200832.00\n","Epoch 514, Loss 30213995196416.00, Val loss 2094818656256.00\n","Epoch 515, Loss 29579283128320.00, Val loss 2023126073344.00\n","Epoch 516, Loss 30074458836992.00, Val loss 2077347151872.00\n","==================================================================\n","Saved best model\n","Epoch 517, Loss 29291156480000.00, Val loss 2006845095936.00\n","Epoch 518, Loss 30047029510144.00, Val loss 2063987113984.00\n","==================================================================\n","Saved best model\n","Epoch 519, Loss 29061215571968.00, Val loss 1994903912448.00\n","Epoch 520, Loss 30021707206656.00, Val loss 2051710910464.00\n","==================================================================\n","Saved best model\n","Epoch 521, Loss 28978689040384.00, Val loss 1984518029312.00\n","Epoch 522, Loss 29877797302272.00, Val loss 2042529972224.00\n","==================================================================\n","Saved best model\n","Epoch 523, Loss 28857958477824.00, Val loss 1977861799936.00\n","Epoch 524, Loss 29545226825728.00, Val loss 2033922867200.00\n","==================================================================\n","Saved best model\n","Epoch 525, Loss 28729651150848.00, Val loss 1970767396864.00\n","Epoch 526, Loss 29424336138240.00, Val loss 2026310336512.00\n","==================================================================\n","Saved best model\n","Epoch 527, Loss 28739125137408.00, Val loss 1964390481920.00\n","Epoch 528, Loss 29769423368192.00, Val loss 2037340831744.00\n","Epoch 529, Loss 29081524989952.00, Val loss 2015438176256.00\n","Epoch 530, Loss 29520682651648.00, Val loss 2038333440000.00\n","Epoch 531, Loss 29049566429184.00, Val loss 2034337972224.00\n","Epoch 532, Loss 28847524945920.00, Val loss 1974367158272.00\n","Epoch 533, Loss 29240311951360.00, Val loss 2034253561856.00\n","Epoch 534, Loss 28932645623808.00, Val loss 1975346659328.00\n","Epoch 535, Loss 29206050840576.00, Val loss 2029765001216.00\n","Epoch 536, Loss 28974637217792.00, Val loss 2024084733952.00\n","Epoch 537, Loss 28885669142528.00, Val loss 1967308013568.00\n","Epoch 538, Loss 29032647817216.00, Val loss 2021860311040.00\n","Epoch 539, Loss 28980673914880.00, Val loss 2019582410752.00\n","Epoch 540, Loss 28970218860544.00, Val loss 2015713165312.00\n","Epoch 541, Loss 28923682787328.00, Val loss 1975442341888.00\n","==================================================================\n","Saved best model\n","Epoch 542, Loss 29038590730240.00, Val loss 1954879373312.00\n","Epoch 543, Loss 29214123223040.00, Val loss 2011198390272.00\n","Epoch 544, Loss 29000166443008.00, Val loss 2006291709952.00\n","Epoch 545, Loss 29011185326080.00, Val loss 2002674122752.00\n","Epoch 546, Loss 29026615164928.00, Val loss 2001705238528.00\n","Epoch 547, Loss 29033083938816.00, Val loss 2001194057728.00\n","Epoch 548, Loss 29003215845376.00, Val loss 1955328950272.00\n","Epoch 549, Loss 29364675037184.00, Val loss 1996379783168.00\n","==================================================================\n","Saved best model\n","Epoch 550, Loss 28931996772352.00, Val loss 1934607122432.00\n","Epoch 551, Loss 29263994451968.00, Val loss 1963067441152.00\n","Epoch 552, Loss 29519518427136.00, Val loss 1988244144128.00\n","Epoch 553, Loss 29259838871552.00, Val loss 1985989836800.00\n","Epoch 554, Loss 29335444746240.00, Val loss 1982135009280.00\n","==================================================================\n","Saved best model\n","Epoch 555, Loss 29282870382592.00, Val loss 1927160135680.00\n","Epoch 556, Loss 29559390599168.00, Val loss 1984416055296.00\n","Epoch 557, Loss 29540540368896.00, Val loss 1977417859072.00\n","Epoch 558, Loss 29689660426240.00, Val loss 1983182405632.00\n","==================================================================\n","Saved best model\n","Epoch 559, Loss 29627692048384.00, Val loss 1926677921792.00\n","Epoch 560, Loss 29774193688576.00, Val loss 1981229563904.00\n","Epoch 561, Loss 29669591087104.00, Val loss 1981013688320.00\n","Epoch 562, Loss 29708810770432.00, Val loss 1979821064192.00\n","Epoch 563, Loss 29693274554368.00, Val loss 1980024619008.00\n","Epoch 564, Loss 29654504013824.00, Val loss 1970870288384.00\n","Epoch 565, Loss 29627970758656.00, Val loss 1971555139584.00\n","Epoch 566, Loss 29604996939776.00, Val loss 1974547513344.00\n","Epoch 567, Loss 29553379731456.00, Val loss 1975231053824.00\n","Epoch 568, Loss 29524370790400.00, Val loss 1973429731328.00\n","Epoch 569, Loss 29496120426496.00, Val loss 1974209740800.00\n","Epoch 570, Loss 29474699966464.00, Val loss 1966820950016.00\n","Epoch 571, Loss 29471679227904.00, Val loss 1973110702080.00\n","Epoch 572, Loss 29418874484736.00, Val loss 1971113820160.00\n","Epoch 573, Loss 29443460128768.00, Val loss 1971149471744.00\n","Epoch 574, Loss 29427627481088.00, Val loss 1968012394496.00\n","==================================================================\n","Saved best model\n","Epoch 575, Loss 29298347558912.00, Val loss 1909467381760.00\n","Epoch 576, Loss 29206040950784.00, Val loss 1957650104320.00\n","Epoch 577, Loss 29394617747456.00, Val loss 1965982220288.00\n","Epoch 578, Loss 29424856567808.00, Val loss 1966372945920.00\n","==================================================================\n","Saved best model\n","Epoch 579, Loss 29162848593920.00, Val loss 1903773351936.00\n","Epoch 580, Loss 29377137295360.00, Val loss 1966097170432.00\n","Epoch 581, Loss 29145234991104.00, Val loss 1960797143040.00\n","Epoch 582, Loss 29239250108416.00, Val loss 1956014456832.00\n","Epoch 583, Loss 29289495138304.00, Val loss 1960995061760.00\n","Epoch 584, Loss 29271090642944.00, Val loss 1959030292480.00\n","Epoch 585, Loss 29317786249216.00, Val loss 1952973979648.00\n","Epoch 586, Loss 29341167368192.00, Val loss 1948610854912.00\n","Epoch 587, Loss 29396448804864.00, Val loss 1948337438720.00\n","Epoch 588, Loss 29402049949696.00, Val loss 1946323779584.00\n","Epoch 589, Loss 29431075866624.00, Val loss 1945213337600.00\n","Epoch 590, Loss 29457513076736.00, Val loss 1943578345472.00\n","Epoch 591, Loss 29477055637504.00, Val loss 1943320526848.00\n","Epoch 592, Loss 29468284303360.00, Val loss 1930673651712.00\n","Epoch 593, Loss 29447850033152.00, Val loss 1941721448448.00\n","Epoch 594, Loss 29486521274368.00, Val loss 1937531994112.00\n","Epoch 595, Loss 29508412669952.00, Val loss 1940018036736.00\n","Epoch 596, Loss 29463559122944.00, Val loss 1933140557824.00\n","Epoch 597, Loss 29443088527360.00, Val loss 1938881249280.00\n","Epoch 598, Loss 29406245060608.00, Val loss 1937152540672.00\n","Epoch 599, Loss 30016365584384.00, Val loss 1923800891392.00\n","Epoch 600, Loss 29245119600640.00, Val loss 1931287986176.00\n","Epoch 601, Loss 29519705894912.00, Val loss 1983433277440.00\n","Epoch 602, Loss 29134110396416.00, Val loss 1979823030272.00\n","Epoch 603, Loss 29090564964352.00, Val loss 1922676162560.00\n","Epoch 604, Loss 29148215449600.00, Val loss 1975920885760.00\n","Epoch 605, Loss 29048215562240.00, Val loss 1972932444160.00\n","Epoch 606, Loss 29030158809088.00, Val loss 1970948800512.00\n","Epoch 607, Loss 28981608554496.00, Val loss 1968904601600.00\n","Epoch 608, Loss 28957312778240.00, Val loss 1966905098240.00\n","Epoch 609, Loss 28955457912832.00, Val loss 1965045972992.00\n","Epoch 610, Loss 28955975219200.00, Val loss 1963287248896.00\n","Epoch 611, Loss 28888577769472.00, Val loss 1942500540416.00\n","Epoch 612, Loss 28938906523648.00, Val loss 1941373452288.00\n","Epoch 613, Loss 28914432374784.00, Val loss 1940097597440.00\n","Epoch 614, Loss 28916094875648.00, Val loss 1938982436864.00\n","Epoch 615, Loss 28921209327616.00, Val loss 1937902534656.00\n","Epoch 616, Loss 28926535710720.00, Val loss 1936724459520.00\n","==================================================================\n","Saved best model\n","Epoch 617, Loss 28763929882624.00, Val loss 1871646031872.00\n","Epoch 618, Loss 28827667144704.00, Val loss 1935903162368.00\n","Epoch 619, Loss 28671210823680.00, Val loss 1874886000640.00\n","Epoch 620, Loss 28887612684288.00, Val loss 1935937634304.00\n","Epoch 621, Loss 28859212496896.00, Val loss 1934425063424.00\n","Epoch 622, Loss 28838302740480.00, Val loss 1933840744448.00\n","Epoch 623, Loss 28814553839616.00, Val loss 1932991004672.00\n","Epoch 624, Loss 28805119778816.00, Val loss 1932200247296.00\n","Epoch 625, Loss 28799016349696.00, Val loss 1931355226112.00\n","Epoch 626, Loss 28794043002880.00, Val loss 1930583998464.00\n","Epoch 627, Loss 28787825709056.00, Val loss 1929790750720.00\n","Epoch 628, Loss 28781099094016.00, Val loss 1929016770560.00\n","Epoch 629, Loss 28773819340800.00, Val loss 1928218148864.00\n","Epoch 630, Loss 28766125082624.00, Val loss 1927254376448.00\n","==================================================================\n","Saved best model\n","Epoch 631, Loss 28560458301440.00, Val loss 1861548113920.00\n","Epoch 632, Loss 28664953640960.00, Val loss 1877968420864.00\n","Epoch 633, Loss 29030324699136.00, Val loss 1928431534080.00\n","Epoch 634, Loss 28667955898368.00, Val loss 1925417533440.00\n","Epoch 635, Loss 28743236593664.00, Val loss 1925280038912.00\n","Epoch 636, Loss 28725185073152.00, Val loss 1924194762752.00\n","Epoch 637, Loss 28727090132992.00, Val loss 1923701407744.00\n","Epoch 638, Loss 28722338633728.00, Val loss 1922923233280.00\n","Epoch 639, Loss 28726406451200.00, Val loss 1922394095616.00\n","Epoch 640, Loss 28726670213120.00, Val loss 1921781202944.00\n","Epoch 641, Loss 28726165198848.00, Val loss 1921256783872.00\n","Epoch 642, Loss 28721366704128.00, Val loss 1920761331712.00\n","Epoch 643, Loss 28678643040256.00, Val loss 1916183773184.00\n","Epoch 644, Loss 28713513908224.00, Val loss 1919971229696.00\n","Epoch 645, Loss 28695927113728.00, Val loss 1919633063936.00\n","Epoch 646, Loss 28694946574336.00, Val loss 1919303155712.00\n","Epoch 647, Loss 28693761073152.00, Val loss 1919030919168.00\n","Epoch 648, Loss 28691854673920.00, Val loss 1918783193088.00\n","Epoch 649, Loss 28688161040384.00, Val loss 1918551195648.00\n","Epoch 650, Loss 28683889573888.00, Val loss 1918274502656.00\n","Epoch 651, Loss 28658178400256.00, Val loss 1885012230144.00\n","Epoch 652, Loss 28770253234176.00, Val loss 1919583387648.00\n","Epoch 653, Loss 28635373608960.00, Val loss 1917668294656.00\n","Epoch 654, Loss 28683310061568.00, Val loss 1917828464640.00\n","Epoch 655, Loss 28666789646336.00, Val loss 1917438787584.00\n","Epoch 656, Loss 28659838588928.00, Val loss 1917224484864.00\n","Epoch 657, Loss 28654140172288.00, Val loss 1916886712320.00\n","Epoch 658, Loss 28650492190720.00, Val loss 1916571746304.00\n","Epoch 659, Loss 28643375226880.00, Val loss 1916160835584.00\n","Epoch 660, Loss 28638801168384.00, Val loss 1915674951680.00\n","Epoch 661, Loss 28636745250816.00, Val loss 1915167440896.00\n","Epoch 662, Loss 28634404587520.00, Val loss 1914669891584.00\n","Epoch 663, Loss 28630497036288.00, Val loss 1914180206592.00\n","Epoch 664, Loss 28625822851072.00, Val loss 1913672957952.00\n","Epoch 665, Loss 28620911790080.00, Val loss 1913047089152.00\n","==================================================================\n","Saved best model\n","Epoch 666, Loss 28416394668032.00, Val loss 1832890400768.00\n","Epoch 667, Loss 28996582215680.00, Val loss 1913777422336.00\n","Epoch 668, Loss 28496347113472.00, Val loss 1911302782976.00\n","Epoch 669, Loss 28601901752320.00, Val loss 1910854647808.00\n","Epoch 670, Loss 28597488465920.00, Val loss 1910305456128.00\n","Epoch 671, Loss 28591158259712.00, Val loss 1909757575168.00\n","Epoch 672, Loss 28595518957568.00, Val loss 1909171421184.00\n","Epoch 673, Loss 28602137157632.00, Val loss 1908597981184.00\n","Epoch 674, Loss 28603851538432.00, Val loss 1908057571328.00\n","Epoch 675, Loss 28601231362048.00, Val loss 1907538526208.00\n","Epoch 676, Loss 28596808306688.00, Val loss 1907037437952.00\n","Epoch 677, Loss 28591932690432.00, Val loss 1906552340480.00\n","Epoch 678, Loss 28587958122496.00, Val loss 1906072223744.00\n","Epoch 679, Loss 28584630747136.00, Val loss 1905597349888.00\n","Epoch 680, Loss 28581408491520.00, Val loss 1905125490688.00\n","Epoch 681, Loss 28578171985920.00, Val loss 1904655859712.00\n","Epoch 682, Loss 28574816036864.00, Val loss 1904198156288.00\n","Epoch 683, Loss 28571946862592.00, Val loss 1903749627904.00\n","Epoch 684, Loss 28563492614144.00, Val loss 1903318532096.00\n","Epoch 685, Loss 28565362425856.00, Val loss 1902880489472.00\n","Epoch 686, Loss 28560139497472.00, Val loss 1902446510080.00\n","Epoch 687, Loss 28556997566464.00, Val loss 1902041759744.00\n","Epoch 688, Loss 28552904935424.00, Val loss 1901624295424.00\n","Epoch 689, Loss 28549733144576.00, Val loss 1901234880512.00\n","Epoch 690, Loss 28545645029376.00, Val loss 1900834848768.00\n","Epoch 691, Loss 28542318516224.00, Val loss 1900458803200.00\n","Epoch 692, Loss 28538068244480.00, Val loss 1900037799936.00\n","==================================================================\n","Saved best model\n","Epoch 693, Loss 28353137618944.00, Val loss 1816694226944.00\n","Epoch 694, Loss 28901090398208.00, Val loss 1901680263168.00\n","Epoch 695, Loss 28341450602496.00, Val loss 1899095654400.00\n","Epoch 696, Loss 28526402484224.00, Val loss 1898561011712.00\n","Epoch 697, Loss 28497063581696.00, Val loss 1898880565248.00\n","Epoch 698, Loss 28404629739520.00, Val loss 1836035211264.00\n","Epoch 699, Loss 28532988751872.00, Val loss 1898783047680.00\n","Epoch 700, Loss 28461613733888.00, Val loss 1897059581952.00\n","Epoch 701, Loss 28536052064256.00, Val loss 1897267331072.00\n","Epoch 702, Loss 28496764698624.00, Val loss 1894971736064.00\n","Epoch 703, Loss 28510314688512.00, Val loss 1896795340800.00\n","Epoch 704, Loss 28254107289600.00, Val loss 1818624917504.00\n","Epoch 705, Loss 28691873153024.00, Val loss 1898650927104.00\n","Epoch 706, Loss 28343645054976.00, Val loss 1896159248384.00\n","Epoch 707, Loss 28469677398016.00, Val loss 1894389645312.00\n","Epoch 708, Loss 28454686617600.00, Val loss 1895261405184.00\n","Epoch 709, Loss 28459982600192.00, Val loss 1895430619136.00\n","Epoch 710, Loss 28148168615936.00, Val loss 1818137067520.00\n","Epoch 711, Loss 28448874577920.00, Val loss 1896495448064.00\n","Epoch 712, Loss 28363549833216.00, Val loss 1895001751552.00\n","Epoch 713, Loss 28480584376320.00, Val loss 1895354204160.00\n","Epoch 714, Loss 28435452411904.00, Val loss 1894334595072.00\n","Epoch 715, Loss 28447862302720.00, Val loss 1894860980224.00\n","Epoch 716, Loss 28395176278016.00, Val loss 1893861556224.00\n","Epoch 717, Loss 28343829815296.00, Val loss 1823546802176.00\n","Epoch 718, Loss 28670913691648.00, Val loss 1895691845632.00\n","Epoch 719, Loss 28417334831104.00, Val loss 1893285232640.00\n","Epoch 720, Loss 28455410700288.00, Val loss 1894201556992.00\n","Epoch 721, Loss 28500322803712.00, Val loss 1892618076160.00\n","Epoch 722, Loss 28465744019456.00, Val loss 1892887035904.00\n","Epoch 723, Loss 28497449353216.00, Val loss 1892448337920.00\n","Epoch 724, Loss 28399944540160.00, Val loss 1892501422080.00\n","==================================================================\n","Saved best model\n","Epoch 725, Loss 28213842235392.00, Val loss 1809424580608.00\n","Epoch 726, Loss 28468984768512.00, Val loss 1893750013952.00\n","Epoch 727, Loss 28463050907648.00, Val loss 1892868685824.00\n","Epoch 728, Loss 28409475969024.00, Val loss 1893141053440.00\n","Epoch 729, Loss 28488642555904.00, Val loss 1892203626496.00\n","Epoch 730, Loss 28417693255680.00, Val loss 1892788994048.00\n","Epoch 731, Loss 28482648367104.00, Val loss 1891766763520.00\n","Epoch 732, Loss 28416855506944.00, Val loss 1892465770496.00\n","Epoch 733, Loss 28493575356416.00, Val loss 1891261480960.00\n","Epoch 734, Loss 28432023885824.00, Val loss 1892093132800.00\n","Epoch 735, Loss 28506462736384.00, Val loss 1890703114240.00\n","Epoch 736, Loss 28444518387712.00, Val loss 1891744481280.00\n","Epoch 737, Loss 28516439957504.00, Val loss 1890061254656.00\n","==================================================================\n","Saved best model\n","Epoch 738, Loss 28288565342208.00, Val loss 1807900344320.00\n","Epoch 739, Loss 28536143478784.00, Val loss 1890011316224.00\n","Epoch 740, Loss 28435849584640.00, Val loss 1890715697152.00\n","==================================================================\n","Saved best model\n","Epoch 741, Loss 28231619096576.00, Val loss 1806839578624.00\n","Epoch 742, Loss 28984546779136.00, Val loss 1829469159424.00\n","Epoch 743, Loss 28765101350912.00, Val loss 1891207741440.00\n","Epoch 744, Loss 28460454408192.00, Val loss 1892788862976.00\n","Epoch 745, Loss 28541650100224.00, Val loss 1890803253248.00\n","Epoch 746, Loss 28463068672000.00, Val loss 1892279779328.00\n","Epoch 747, Loss 28556317061120.00, Val loss 1890464956416.00\n","Epoch 748, Loss 28472851075072.00, Val loss 1891933224960.00\n","Epoch 749, Loss 28556489631744.00, Val loss 1890261925888.00\n","Epoch 750, Loss 28467663187968.00, Val loss 1891727835136.00\n","Epoch 751, Loss 28548035065856.00, Val loss 1890133606400.00\n","Epoch 752, Loss 28460370419712.00, Val loss 1891556655104.00\n","Epoch 753, Loss 28539827218432.00, Val loss 1890021539840.00\n","Epoch 754, Loss 28455541768192.00, Val loss 1891373809664.00\n","Epoch 755, Loss 28537301231616.00, Val loss 1889919827968.00\n","Epoch 756, Loss 28452066189312.00, Val loss 1891164618752.00\n","Epoch 757, Loss 28545583343616.00, Val loss 1889777221632.00\n","Epoch 758, Loss 28461523705856.00, Val loss 1890930655232.00\n","Epoch 759, Loss 28548802256896.00, Val loss 1889755856896.00\n","Epoch 760, Loss 28448580460544.00, Val loss 1890863022080.00\n","Epoch 761, Loss 28530463234048.00, Val loss 1889752973312.00\n","Epoch 762, Loss 28431429920768.00, Val loss 1890765897728.00\n","Epoch 763, Loss 28534755682304.00, Val loss 1889525563392.00\n","Epoch 764, Loss 28429809721344.00, Val loss 1890601533440.00\n","Epoch 765, Loss 28559075733504.00, Val loss 1889145323520.00\n","Epoch 766, Loss 28438188384256.00, Val loss 1890431533056.00\n","Epoch 767, Loss 28615909699584.00, Val loss 1888687620096.00\n","Epoch 768, Loss 28454823706624.00, Val loss 1890279751680.00\n","Epoch 769, Loss 28592629125120.00, Val loss 1889193033728.00\n","Epoch 770, Loss 28426607968256.00, Val loss 1890144092160.00\n","Epoch 771, Loss 28524988022784.00, Val loss 1889926250496.00\n","Epoch 772, Loss 28393325174784.00, Val loss 1889925857280.00\n","Epoch 773, Loss 28514534649856.00, Val loss 1890126004224.00\n","Epoch 774, Loss 28388189057024.00, Val loss 1889886666752.00\n","Epoch 775, Loss 28515243233280.00, Val loss 1890236497920.00\n","Epoch 776, Loss 28399839948800.00, Val loss 1889839087616.00\n","Epoch 777, Loss 28513681399808.00, Val loss 1890345680896.00\n","Epoch 778, Loss 28398026309632.00, Val loss 1889789411328.00\n","Epoch 779, Loss 28513998307328.00, Val loss 1890401648640.00\n","Epoch 780, Loss 28401815146496.00, Val loss 1889691631616.00\n","Epoch 781, Loss 28517328363520.00, Val loss 1890424979456.00\n","Epoch 782, Loss 28407001579520.00, Val loss 1889590181888.00\n","Epoch 783, Loss 28519172984832.00, Val loss 1890450800640.00\n","Epoch 784, Loss 28409198116864.00, Val loss 1889457930240.00\n","Epoch 785, Loss 28221733924864.00, Val loss 1808807624704.00\n","Epoch 786, Loss 28402101682176.00, Val loss 1890463907840.00\n","Epoch 787, Loss 28511933521920.00, Val loss 1891782754304.00\n","Epoch 788, Loss 28402550906880.00, Val loss 1890400468992.00\n","Epoch 789, Loss 28504716177408.00, Val loss 1891614195712.00\n","Epoch 790, Loss 28400319565824.00, Val loss 1890231648256.00\n","Epoch 791, Loss 28492869623808.00, Val loss 1891744350208.00\n","Epoch 792, Loss 28390405824512.00, Val loss 1890148024320.00\n","Epoch 793, Loss 28473146339328.00, Val loss 1892099424256.00\n","Epoch 794, Loss 28376996876288.00, Val loss 1890111324160.00\n","Epoch 795, Loss 28455573872640.00, Val loss 1892362616832.00\n","Epoch 796, Loss 28023334244352.00, Val loss 1810265538560.00\n","Epoch 797, Loss 28503308845056.00, Val loss 1894011240448.00\n","Epoch 798, Loss 28342404763648.00, Val loss 1891400679424.00\n","Epoch 799, Loss 28415878660096.00, Val loss 1894326992896.00\n","Epoch 800, Loss 28343451299840.00, Val loss 1890852798464.00\n","Epoch 801, Loss 28407033106432.00, Val loss 1894156599296.00\n","Epoch 802, Loss 28086446534656.00, Val loss 1807040380928.00\n","Epoch 803, Loss 28434289360896.00, Val loss 1895226408960.00\n","Epoch 804, Loss 28362981793792.00, Val loss 1888020856832.00\n","Epoch 805, Loss 28416275566592.00, Val loss 1895613988864.00\n","Epoch 806, Loss 28391699795968.00, Val loss 1891508944896.00\n","Epoch 807, Loss 28390186668032.00, Val loss 1895664189440.00\n","Epoch 808, Loss 28407261024256.00, Val loss 1891464380416.00\n","Epoch 809, Loss 28383273721856.00, Val loss 1895817674752.00\n","Epoch 810, Loss 28403517206528.00, Val loss 1891473817600.00\n","Epoch 811, Loss 28363381628928.00, Val loss 1895797751808.00\n","Epoch 812, Loss 28362620809216.00, Val loss 1891642114048.00\n","Epoch 813, Loss 27883021291520.00, Val loss 1816272306176.00\n","Epoch 814, Loss 28453535121408.00, Val loss 1893543313408.00\n","Epoch 815, Loss 28200048148480.00, Val loss 1897573384192.00\n","Epoch 816, Loss 28250832033792.00, Val loss 1893001854976.00\n","Epoch 817, Loss 28287509258240.00, Val loss 1896686944256.00\n","Epoch 818, Loss 28267316981760.00, Val loss 1892626333696.00\n","Epoch 819, Loss 28374838087680.00, Val loss 1895163756544.00\n","Epoch 820, Loss 28315996379136.00, Val loss 1892361043968.00\n","Epoch 821, Loss 28441168674816.00, Val loss 1893896814592.00\n","Epoch 822, Loss 28390557241344.00, Val loss 1892104536064.00\n","Epoch 823, Loss 28494920142848.00, Val loss 1893422858240.00\n","Epoch 824, Loss 28418703888384.00, Val loss 1892019339264.00\n","Epoch 825, Loss 28391491624960.00, Val loss 1894830440448.00\n","Epoch 826, Loss 28328228249600.00, Val loss 1891911729152.00\n","Epoch 827, Loss 28315726819328.00, Val loss 1895554613248.00\n","Epoch 828, Loss 28384375422976.00, Val loss 1891984343040.00\n","Epoch 829, Loss 28326293983232.00, Val loss 1895618183168.00\n","Epoch 830, Loss 28387698040832.00, Val loss 1892289609728.00\n","Epoch 831, Loss 28310592974848.00, Val loss 1895833665536.00\n","Epoch 832, Loss 28355834779648.00, Val loss 1892503126016.00\n","Epoch 833, Loss 28298167652352.00, Val loss 1896043642880.00\n","Epoch 834, Loss 28339823525888.00, Val loss 1892606935040.00\n","Epoch 835, Loss 28293410127872.00, Val loss 1896098693120.00\n","Epoch 836, Loss 28342060730368.00, Val loss 1892551229440.00\n","Epoch 837, Loss 28297319284736.00, Val loss 1896199618560.00\n","Epoch 838, Loss 28349349728256.00, Val loss 1892491984896.00\n","Epoch 839, Loss 28303517167616.00, Val loss 1896352972800.00\n","Epoch 840, Loss 28347990415360.00, Val loss 1892448993280.00\n","Epoch 841, Loss 28305900777472.00, Val loss 1896498069504.00\n","Epoch 842, Loss 28350952939520.00, Val loss 1892504436736.00\n","Epoch 843, Loss 28309767061504.00, Val loss 1896563998720.00\n","Epoch 844, Loss 28342163699712.00, Val loss 1892609556480.00\n","Epoch 845, Loss 28302048395264.00, Val loss 1896703983616.00\n","Epoch 846, Loss 28350075498496.00, Val loss 1892696588288.00\n","Epoch 847, Loss 28224309288960.00, Val loss 1813219901440.00\n","Epoch 848, Loss 28394569175040.00, Val loss 1893006835712.00\n","Epoch 849, Loss 28269782564864.00, Val loss 1897007546368.00\n","Epoch 850, Loss 28361592856576.00, Val loss 1893240274944.00\n","Epoch 851, Loss 28301550522368.00, Val loss 1896488239104.00\n","Epoch 852, Loss 28286658834432.00, Val loss 1893260984320.00\n","Epoch 853, Loss 28263928647680.00, Val loss 1897564733440.00\n","Epoch 854, Loss 28143701178368.00, Val loss 1809343447040.00\n","Epoch 855, Loss 28350634741760.00, Val loss 1896864153600.00\n","Epoch 856, Loss 28254489661440.00, Val loss 1893985419264.00\n","Epoch 857, Loss 28248773910528.00, Val loss 1898555768832.00\n","Epoch 858, Loss 28286450966528.00, Val loss 1893771640832.00\n","Epoch 859, Loss 28278183526400.00, Val loss 1895921614848.00\n","Epoch 860, Loss 28309769793536.00, Val loss 1894330269696.00\n","Epoch 861, Loss 27857866096640.00, Val loss 1819853979648.00\n","Epoch 862, Loss 28580099436544.00, Val loss 1896042332160.00\n","Epoch 863, Loss 28266027663360.00, Val loss 1898194534400.00\n","Epoch 864, Loss 28345658593280.00, Val loss 1896321515520.00\n","Epoch 865, Loss 27984053927936.00, Val loss 1815585095680.00\n","Epoch 866, Loss 28574886187008.00, Val loss 1897076228096.00\n","Epoch 867, Loss 28246559064064.00, Val loss 1900308463616.00\n","Epoch 868, Loss 28307300270080.00, Val loss 1897370746880.00\n","Epoch 869, Loss 28192871915520.00, Val loss 1899948670976.00\n","Epoch 870, Loss 28313637376000.00, Val loss 1897205596160.00\n","Epoch 871, Loss 28195556118528.00, Val loss 1901808451584.00\n","Epoch 872, Loss 28285642072064.00, Val loss 1897461710848.00\n","Epoch 873, Loss 28196387446784.00, Val loss 1901467009024.00\n","Epoch 874, Loss 28243064211456.00, Val loss 1897406660608.00\n","Epoch 875, Loss 28157674389504.00, Val loss 1901148372992.00\n","Epoch 876, Loss 28233462302720.00, Val loss 1897361047552.00\n","Epoch 877, Loss 28171567067136.00, Val loss 1901185859584.00\n","Epoch 878, Loss 28194308055040.00, Val loss 1897305604096.00\n","Epoch 879, Loss 28233093599232.00, Val loss 1900963692544.00\n","Epoch 880, Loss 28184232464384.00, Val loss 1897438773248.00\n","Epoch 881, Loss 28227186614272.00, Val loss 1901118750720.00\n","Epoch 882, Loss 28249573939200.00, Val loss 1897693315072.00\n","Epoch 883, Loss 28214216421376.00, Val loss 1898493771776.00\n","Epoch 884, Loss 28366746456064.00, Val loss 1898368860160.00\n","Epoch 885, Loss 28219328487424.00, Val loss 1898870734848.00\n","Epoch 886, Loss 28375344840704.00, Val loss 1898762600448.00\n","Epoch 887, Loss 28221447864320.00, Val loss 1899569610752.00\n","Epoch 888, Loss 28370583236608.00, Val loss 1899198021632.00\n","Epoch 889, Loss 28213432868864.00, Val loss 1900539150336.00\n","Epoch 890, Loss 28365654175744.00, Val loss 1899665424384.00\n","Epoch 891, Loss 28203563364352.00, Val loss 1901349699584.00\n","Epoch 892, Loss 28357469298688.00, Val loss 1900106743808.00\n","Epoch 893, Loss 28191882653696.00, Val loss 1901972291584.00\n","Epoch 894, Loss 28327860946944.00, Val loss 1900443205632.00\n","Epoch 895, Loss 28195125284864.00, Val loss 1902794768384.00\n","Epoch 896, Loss 28335019532288.00, Val loss 1901119799296.00\n","Epoch 897, Loss 28172056383488.00, Val loss 1903532834816.00\n","Epoch 898, Loss 28310597902336.00, Val loss 1901242351616.00\n","Epoch 899, Loss 28163001126912.00, Val loss 1903905603584.00\n","Epoch 900, Loss 28285605591040.00, Val loss 1901430177792.00\n","Epoch 901, Loss 27699421691904.00, Val loss 1826196160512.00\n","Epoch 902, Loss 28336661121024.00, Val loss 1902512701440.00\n","Epoch 903, Loss 28146445946880.00, Val loss 1905669570560.00\n","Epoch 904, Loss 28251933071360.00, Val loss 1902742470656.00\n","Epoch 905, Loss 28153795862528.00, Val loss 1904609722368.00\n","Epoch 906, Loss 28266279337984.00, Val loss 1903024013312.00\n","Epoch 907, Loss 28145880088576.00, Val loss 1903998795776.00\n","Epoch 908, Loss 28324656037888.00, Val loss 1903692611584.00\n","Epoch 909, Loss 28149314064384.00, Val loss 1904202350592.00\n","Epoch 910, Loss 28327549566976.00, Val loss 1904059088896.00\n","Epoch 911, Loss 28132458479616.00, Val loss 1905573101568.00\n","Epoch 912, Loss 28322335309824.00, Val loss 1904434216960.00\n","Epoch 913, Loss 28138330624000.00, Val loss 1905520803840.00\n","Epoch 914, Loss 28295662407680.00, Val loss 1904747216896.00\n","Epoch 915, Loss 28087562166272.00, Val loss 1907168641024.00\n","Epoch 916, Loss 28279798532096.00, Val loss 1904202612736.00\n","Epoch 917, Loss 28095696195584.00, Val loss 1907538657280.00\n","Epoch 918, Loss 28218730899456.00, Val loss 1904756785152.00\n","Epoch 919, Loss 28020731686912.00, Val loss 1907717570560.00\n","Epoch 920, Loss 28221156231168.00, Val loss 1904581935104.00\n","Epoch 921, Loss 28082422530048.00, Val loss 1907445202944.00\n","Epoch 922, Loss 28166843639808.00, Val loss 1904622567424.00\n","Epoch 923, Loss 28119016005632.00, Val loss 1907582304256.00\n","Epoch 924, Loss 28154258096128.00, Val loss 1905178443776.00\n","Epoch 925, Loss 28109806157824.00, Val loss 1905993318400.00\n","Epoch 926, Loss 28295936761856.00, Val loss 1906791284736.00\n","Epoch 927, Loss 28082435149824.00, Val loss 1906767953920.00\n","Epoch 928, Loss 27707890528256.00, Val loss 1832228880384.00\n","Epoch 929, Loss 28169787142144.00, Val loss 1907985219584.00\n","Epoch 930, Loss 28308675903488.00, Val loss 1908909932544.00\n","Epoch 931, Loss 28073646264320.00, Val loss 1909194227712.00\n","Epoch 932, Loss 28285689966592.00, Val loss 1908953841664.00\n","Epoch 933, Loss 28076384960512.00, Val loss 1909304459264.00\n","Epoch 934, Loss 28282973614080.00, Val loss 1909143240704.00\n","Epoch 935, Loss 28064080502784.00, Val loss 1909690335232.00\n","Epoch 936, Loss 28275929710592.00, Val loss 1909101035520.00\n","Epoch 937, Loss 28055364816896.00, Val loss 1909747613696.00\n","Epoch 938, Loss 28250916048896.00, Val loss 1908821065728.00\n","Epoch 939, Loss 28026539888640.00, Val loss 1910886367232.00\n","Epoch 940, Loss 28215475824640.00, Val loss 1908113932288.00\n","Epoch 941, Loss 28036102533120.00, Val loss 1910867099648.00\n","Epoch 942, Loss 28168671864832.00, Val loss 1908009074688.00\n","Epoch 943, Loss 28008760295424.00, Val loss 1911095558144.00\n","Epoch 944, Loss 28171111626752.00, Val loss 1908154826752.00\n","Epoch 945, Loss 28038454628352.00, Val loss 1910006218752.00\n","Epoch 946, Loss 28160995125248.00, Val loss 1908348944384.00\n","Epoch 947, Loss 28033521467392.00, Val loss 1909431599104.00\n","Epoch 948, Loss 28242024443904.00, Val loss 1909885239296.00\n","Epoch 949, Loss 28036765478912.00, Val loss 1909286895616.00\n","Epoch 950, Loss 28258405048320.00, Val loss 1910540992512.00\n","Epoch 951, Loss 28030461616128.00, Val loss 1910385672192.00\n","Epoch 952, Loss 28264124354560.00, Val loss 1910597746688.00\n","Epoch 953, Loss 28023843024896.00, Val loss 1910271508480.00\n","Epoch 954, Loss 28243412905984.00, Val loss 1910141222912.00\n","Epoch 955, Loss 28002279120896.00, Val loss 1911414980608.00\n","Epoch 956, Loss 27679496749056.00, Val loss 1834552786944.00\n","Epoch 957, Loss 28057667887104.00, Val loss 1911995891712.00\n","Epoch 958, Loss 28202705551360.00, Val loss 1911121117184.00\n","Epoch 959, Loss 27961933000704.00, Val loss 1913555779584.00\n","Epoch 960, Loss 28169224904704.00, Val loss 1910328000512.00\n","Epoch 961, Loss 27970128551936.00, Val loss 1912422924288.00\n","Epoch 962, Loss 28136092860416.00, Val loss 1910104391680.00\n","Epoch 963, Loss 27914922401792.00, Val loss 1912859918336.00\n","Epoch 964, Loss 28147140591616.00, Val loss 1910116974592.00\n","Epoch 965, Loss 27977649111040.00, Val loss 1912630673408.00\n","Epoch 966, Loss 28080328478720.00, Val loss 1909803319296.00\n","Epoch 967, Loss 27957207080960.00, Val loss 1912448221184.00\n","Epoch 968, Loss 28176633946112.00, Val loss 1911105781760.00\n","Epoch 969, Loss 27978768240640.00, Val loss 1911017832448.00\n","Epoch 970, Loss 28169735122944.00, Val loss 1911210901504.00\n","Epoch 971, Loss 27952338915328.00, Val loss 1912439701504.00\n","Epoch 972, Loss 28213166465024.00, Val loss 1912630804480.00\n","Epoch 973, Loss 27975665664000.00, Val loss 1911604772864.00\n","Epoch 974, Loss 28210105700352.00, Val loss 1912546131968.00\n","Epoch 975, Loss 27955025174528.00, Val loss 1912042815488.00\n","Epoch 976, Loss 28236417961984.00, Val loss 1912891375616.00\n","Epoch 977, Loss 27965395496960.00, Val loss 1910707191808.00\n","Epoch 978, Loss 28214787338240.00, Val loss 1909000896512.00\n","Epoch 979, Loss 27935136612352.00, Val loss 1909267496960.00\n","Epoch 980, Loss 28199155019776.00, Val loss 1908774141952.00\n","Epoch 981, Loss 27945086582784.00, Val loss 1909149138944.00\n","Epoch 982, Loss 28194529906688.00, Val loss 1908231110656.00\n","Epoch 983, Loss 27296861687808.00, Val loss 1834228645888.00\n","Epoch 984, Loss 28237209776128.00, Val loss 1909478785024.00\n","Epoch 985, Loss 27912471683072.00, Val loss 1911670833152.00\n","Epoch 986, Loss 28076487446528.00, Val loss 1907747192832.00\n","Epoch 987, Loss 27898802618368.00, Val loss 1911252320256.00\n","Epoch 988, Loss 28118077640704.00, Val loss 1907716128768.00\n","Epoch 989, Loss 27888566812672.00, Val loss 1910712434688.00\n","Epoch 990, Loss 28113503481856.00, Val loss 1907302072320.00\n","Epoch 991, Loss 27909435506688.00, Val loss 1911084941312.00\n","Epoch 992, Loss 28081564164096.00, Val loss 1907421085696.00\n","Epoch 993, Loss 27931231145984.00, Val loss 1910737862656.00\n","Epoch 994, Loss 28131727818752.00, Val loss 1908115767296.00\n","Epoch 995, Loss 27927702216704.00, Val loss 1910739435520.00\n","Epoch 996, Loss 28148998320128.00, Val loss 1908867727360.00\n","Epoch 997, Loss 27923428757504.00, Val loss 1910751232000.00\n","Epoch 998, Loss 28173232766976.00, Val loss 1910031777792.00\n","Epoch 999, Loss 27913380143104.00, Val loss 1910698803200.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"status":"ok","timestamp":1646783625136,"user_tz":360,"elapsed":253,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"eff102e1-9819-4c14-bfc5-2199ab5e9878"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["2270403231744.0\n","tensor([201701.3750, 202356.7500, 203033.2500, 203708.4531, 204389.2344,\n","        205089.3906, 205797.5312, 206525.9844, 207252.3906, 207993.6406,\n","        208750.6094, 209524.1406, 210299.4688, 211083.7031, 211865.6250],\n","       grad_fn=<SelectBackward0>)\n","tensor([292187., 293697., 293697., 293697., 293697., 293697., 295701., 296870.,\n","        297729., 297729., 297729., 298362., 298626., 298808., 298993.])\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","    # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_cumulative_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_cumulative_infected_tensor':labeled_output # (52, 15)\n","}\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)"],"metadata":{"id":"s3-8Yge6CAWM","executionInfo":{"status":"ok","timestamp":1646783888026,"user_tz":360,"elapsed":165,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["mean_squared_error = criterion(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","mae_function = torch.nn.L1Loss()\n","mean_absolute_error = mae_function(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","print(\"mean squared error: \", mean_squared_error)\n","print(\"mean absolute error: \", mean_absolute_error)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzdfnLLl7JwD","executionInfo":{"status":"ok","timestamp":1646783891117,"user_tz":360,"elapsed":143,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"141871b5-f1cd-435a-f1f2-dc2758001432"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["mean squared error:  2270403231744.0\n","mean absolute error:  937483.0625\n"]}]}]}