{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.3_new_data_COVID_Forecaster_full.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNKL8F4kwaSb0Zn/IybIo1P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1646782102435,"user_tz":360,"elapsed":23736,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"f7749f99-f797-4f0e-9883-e8e10ed907e5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"52a35727-7ad8-488a-ed29-e75819ba6c6b","executionInfo":{"status":"ok","timestamp":1646782154473,"user_tz":360,"elapsed":52046,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 4.1 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.12\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 4.0 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.5.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (747 kB)\n","\u001b[K     |████████████████████████████████| 747 kB 4.0 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.3.tar.gz (370 kB)\n","\u001b[K     |████████████████████████████████| 370 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Collecting rdflib\n","  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n","\u001b[K     |████████████████████████████████| 482 kB 48.3 MB/s \n","\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Collecting isodate\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 409 kB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (4.11.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.3-py3-none-any.whl size=581968 sha256=7d517dacff16b5547948db06e5f3fffbbe0d46e82548b290488dda1ccd1d471a\n","  Stored in directory: /root/.cache/pip/wheels/c3/2a/58/87ce0508964d4def1aafb92750c4f3ac77038efd1b9a89dcf5\n","Successfully built torch-geometric\n","Installing collected packages: isodate, yacs, rdflib, torch-geometric\n","Successfully installed isodate-0.6.1 rdflib-6.1.1 torch-geometric-2.0.3 yacs-0.1.8\n","Collecting torch-geometric-temporal\n","  Downloading torch_geometric_temporal-0.51.0.tar.gz (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: torch_sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.12)\n","Requirement already satisfied: torch_scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.13)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (6.1.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.6.3)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.1.8)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (4.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (57.4.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (0.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch_geometric->torch-geometric-temporal) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.1.0)\n","Building wheels for collected packages: torch-geometric-temporal\n","  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.51.0-py3-none-any.whl size=83569 sha256=d3b660679d3c1cb0946b8bfe5c5a3343f077951d24206aa5f8d1cb69d6d8d40b\n","  Stored in directory: /root/.cache/pip/wheels/a5/26/64/465700aa43b21fccca9ae446b407de2389f0ba16114e84db8d\n","Successfully built torch-geometric-temporal\n","Installing collected packages: torch-geometric-temporal\n","Successfully installed torch-geometric-temporal-0.51.0\n","Collecting ogb\n","  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Collecting outdated>=0.2.0\n","  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Collecting littleutils\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=49ae4a0dd8f8f7aee6cf3b2d85afd1621ab96ce46f46195feb5d31928a21a03e\n","  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n","PyTorch has version 1.10.0+cu111\n","Collecting epiweeks\n","  Downloading epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n","Installing collected packages: epiweeks\n","Successfully installed epiweeks-2.1.4\n","Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 24\n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3.3_to_3.5_new_data.pickle'\n","save_model_relative_path = './saved_models/v3.3_new_data_COVID_Forecaster_full'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3.3_archived_output.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1646782154474,"user_tz":360,"elapsed":21,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"823ab453-89be-488f-ad85-e09c7f042729"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_confirmed_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_confirmed_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_confirmed_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU","executionInfo":{"status":"ok","timestamp":1646782155916,"user_tz":360,"elapsed":1459,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3","executionInfo":{"status":"ok","timestamp":1646782155917,"user_tz":360,"elapsed":16,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class COVID_Forecaster_full(torch.nn.Module):\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","      self.linear2 = Linear(history_window, outputLayer_num_features)\n","      self.linear3 = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x) + self.linear2(data.x[:, 0:history_window])\n","\n","      x = F.elu(x)\n","      x = self.linear3(x)\n","\n","      return x\n","\n","model = COVID_Forecaster_full().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj","executionInfo":{"status":"ok","timestamp":1646782155918,"user_tz":360,"elapsed":16,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646782155919,"user_tz":360,"elapsed":16,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"32e253a0-d985-4a55-8d96-b562924fa62b"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["COVID_Forecaster_full(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n","  (linear2): Linear(in_features=6, out_features=15, bias=True)\n","  (linear3): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646783841381,"user_tz":360,"elapsed":1685475,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"ba7f052a-9f5a-4f4b-dc64-42b214d5eb1c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 80581957012.00, Val loss 29621104640.00\n","Epoch 1, Loss 48632681313.50, Val loss 30548514816.00\n","Epoch 2, Loss 48294304099.00, Val loss 69096759296.00\n","Epoch 3, Loss 82952894680.50, Val loss 62583431168.00\n","Epoch 4, Loss 74207356807.00, Val loss 69491171328.00\n","Epoch 5, Loss 80226227857.00, Val loss 44439711744.00\n","Epoch 6, Loss 54334926231.00, Val loss 65736032256.00\n","Epoch 7, Loss 71812806814.00, Val loss 67182149632.00\n","Epoch 8, Loss 73543380074.00, Val loss 67598639104.00\n","Epoch 9, Loss 74159885791.50, Val loss 45659312128.00\n","Epoch 10, Loss 54181258063.00, Val loss 67676028928.00\n","Epoch 11, Loss 73697317164.50, Val loss 40718462976.00\n","Epoch 12, Loss 49389161756.00, Val loss 57178054656.00\n","Epoch 13, Loss 62064438055.50, Val loss 51353137152.00\n","Epoch 14, Loss 56484219163.50, Val loss 54339223552.00\n","Epoch 15, Loss 58533915807.50, Val loss 50394701824.00\n","Epoch 16, Loss 54899757376.00, Val loss 51379507200.00\n","Epoch 17, Loss 55423664570.00, Val loss 49752592384.00\n","Epoch 18, Loss 53783959644.00, Val loss 50062200832.00\n","Epoch 19, Loss 54203259203.50, Val loss 49616744448.00\n","Epoch 20, Loss 54154440683.50, Val loss 48936919040.00\n","Epoch 21, Loss 54454887387.00, Val loss 48845455360.00\n","Epoch 22, Loss 55429860206.00, Val loss 48871034880.00\n","Epoch 23, Loss 57278129133.00, Val loss 48977625088.00\n","Epoch 24, Loss 58670194783.00, Val loss 49232252928.00\n","Epoch 25, Loss 60863530455.00, Val loss 49637507072.00\n","Epoch 26, Loss 63449265110.00, Val loss 49600856064.00\n","Epoch 27, Loss 66730586070.00, Val loss 46165241856.00\n","Epoch 28, Loss 66402129904.00, Val loss 46388092928.00\n","Epoch 29, Loss 68169055580.00, Val loss 42183983104.00\n","Epoch 30, Loss 65207731686.00, Val loss 43110498304.00\n","Epoch 31, Loss 67325395902.00, Val loss 39500124160.00\n","Epoch 32, Loss 64363754634.00, Val loss 39006711808.00\n","Epoch 33, Loss 64549263886.00, Val loss 37900197888.00\n","Epoch 34, Loss 64319863656.00, Val loss 34184443904.00\n","Epoch 35, Loss 64220409132.00, Val loss 40170999808.00\n","Epoch 36, Loss 69275762778.00, Val loss 31772340224.00\n","Epoch 37, Loss 62803597050.00, Val loss 38691287040.00\n","==================================================================\n","Saved best model\n","Epoch 38, Loss 68700450708.00, Val loss 29047642112.00\n","Epoch 39, Loss 62102648656.00, Val loss 38192922624.00\n","==================================================================\n","Saved best model\n","Epoch 40, Loss 69487515266.00, Val loss 24798169088.00\n","Epoch 41, Loss 60684714840.00, Val loss 36367040512.00\n","==================================================================\n","Saved best model\n","Epoch 42, Loss 68274948778.00, Val loss 22206447616.00\n","Epoch 43, Loss 58965582084.00, Val loss 33231966208.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 64130245792.00, Val loss 21618210816.00\n","Epoch 45, Loss 48367051827.00, Val loss 40178114560.00\n","Epoch 46, Loss 41424378701.00, Val loss 28940519424.00\n","Epoch 47, Loss 26286027040.00, Val loss 28973838336.00\n","Epoch 48, Loss 26378390689.25, Val loss 28990709760.00\n","Epoch 49, Loss 26714707395.75, Val loss 28815763456.00\n","Epoch 50, Loss 27124167619.75, Val loss 27807168512.00\n","Epoch 51, Loss 28010594419.50, Val loss 27618406400.00\n","Epoch 52, Loss 31448638397.00, Val loss 28868319232.00\n","Epoch 53, Loss 42639895852.00, Val loss 21896386560.00\n","Epoch 54, Loss 56609461316.00, Val loss 21890975744.00\n","==================================================================\n","Saved best model\n","Epoch 55, Loss 55582068580.00, Val loss 17619001344.00\n","Epoch 56, Loss 54238757102.00, Val loss 21163847680.00\n","Epoch 57, Loss 53265543874.00, Val loss 22155874304.00\n","==================================================================\n","Saved best model\n","Epoch 58, Loss 52702801784.00, Val loss 17130792960.00\n","Epoch 59, Loss 51930334040.00, Val loss 17539332096.00\n","Epoch 60, Loss 51535556817.00, Val loss 21630734336.00\n","Epoch 61, Loss 51698127722.00, Val loss 21778483200.00\n","Epoch 62, Loss 51788792346.00, Val loss 21885734912.00\n","Epoch 63, Loss 51981863936.00, Val loss 21578029056.00\n","Epoch 64, Loss 52159185360.00, Val loss 21624287232.00\n","Epoch 65, Loss 52496656532.00, Val loss 21286133760.00\n","Epoch 66, Loss 52723098422.00, Val loss 21299695616.00\n","Epoch 67, Loss 53012427690.00, Val loss 20892393472.00\n","Epoch 68, Loss 53137410658.00, Val loss 20885549056.00\n","Epoch 69, Loss 53310413766.00, Val loss 20427505664.00\n","Epoch 70, Loss 53323724262.00, Val loss 20427171840.00\n","Epoch 71, Loss 53398842100.00, Val loss 19948742656.00\n","Epoch 72, Loss 53350522524.00, Val loss 19982008320.00\n","Epoch 73, Loss 53392972878.00, Val loss 19515006976.00\n","Epoch 74, Loss 53358470110.00, Val loss 19574685696.00\n","Epoch 75, Loss 53417754033.00, Val loss 19144718336.00\n","Epoch 76, Loss 53432435199.00, Val loss 19219034112.00\n","Epoch 77, Loss 53517601976.00, Val loss 18830327808.00\n","Epoch 78, Loss 53573168600.00, Val loss 18897227776.00\n","Epoch 79, Loss 53669557915.00, Val loss 18560071680.00\n","Epoch 80, Loss 53748581609.00, Val loss 18605631488.00\n","Epoch 81, Loss 53844767366.00, Val loss 18321465344.00\n","Epoch 82, Loss 53934756873.00, Val loss 18335252480.00\n","Epoch 83, Loss 54024998671.00, Val loss 18103666688.00\n","Epoch 84, Loss 54119255743.00, Val loss 18090072064.00\n","Epoch 85, Loss 54206364872.00, Val loss 17903835136.00\n","Epoch 86, Loss 54301955350.00, Val loss 17864837120.00\n","Epoch 87, Loss 54388806068.00, Val loss 17715152896.00\n","Epoch 88, Loss 54482673269.00, Val loss 17662740480.00\n","Epoch 89, Loss 54569184256.00, Val loss 17540507648.00\n","Epoch 90, Loss 54663836523.00, Val loss 17475504128.00\n","Epoch 91, Loss 54749125989.00, Val loss 17375004672.00\n","Epoch 92, Loss 54837632789.00, Val loss 17316177920.00\n","Epoch 93, Loss 54919559158.00, Val loss 17226594304.00\n","Epoch 94, Loss 55007306029.00, Val loss 17166593024.00\n","==================================================================\n","Saved best model\n","Epoch 95, Loss 55088225492.00, Val loss 17091225600.00\n","==================================================================\n","Saved best model\n","Epoch 96, Loss 55169316632.00, Val loss 17034160128.00\n","==================================================================\n","Saved best model\n","Epoch 97, Loss 55245536086.00, Val loss 16970338304.00\n","==================================================================\n","Saved best model\n","Epoch 98, Loss 55320690152.00, Val loss 16916921344.00\n","==================================================================\n","Saved best model\n","Epoch 99, Loss 55388262185.00, Val loss 16863352832.00\n","==================================================================\n","Saved best model\n","Epoch 100, Loss 55456514045.00, Val loss 16816974848.00\n","==================================================================\n","Saved best model\n","Epoch 101, Loss 55518627134.00, Val loss 16771819520.00\n","==================================================================\n","Saved best model\n","Epoch 102, Loss 55580916701.50, Val loss 16731255808.00\n","==================================================================\n","Saved best model\n","Epoch 103, Loss 55639361915.00, Val loss 16693677056.00\n","==================================================================\n","Saved best model\n","Epoch 104, Loss 55693928399.50, Val loss 16659615744.00\n","==================================================================\n","Saved best model\n","Epoch 105, Loss 55746773928.00, Val loss 16630088704.00\n","==================================================================\n","Saved best model\n","Epoch 106, Loss 55794615409.50, Val loss 16603953152.00\n","==================================================================\n","Saved best model\n","Epoch 107, Loss 55840778992.00, Val loss 16580191232.00\n","==================================================================\n","Saved best model\n","Epoch 108, Loss 55883315092.50, Val loss 16557058048.00\n","==================================================================\n","Saved best model\n","Epoch 109, Loss 55922202509.00, Val loss 16541651968.00\n","==================================================================\n","Saved best model\n","Epoch 110, Loss 55958083373.50, Val loss 16524026880.00\n","==================================================================\n","Saved best model\n","Epoch 111, Loss 55992159798.50, Val loss 16511942656.00\n","==================================================================\n","Saved best model\n","Epoch 112, Loss 56022289001.00, Val loss 16501300224.00\n","==================================================================\n","Saved best model\n","Epoch 113, Loss 56048079900.00, Val loss 16498290688.00\n","==================================================================\n","Saved best model\n","Epoch 114, Loss 56069894701.00, Val loss 16486266880.00\n","==================================================================\n","Saved best model\n","Epoch 115, Loss 56091784819.00, Val loss 16480160768.00\n","==================================================================\n","Saved best model\n","Epoch 116, Loss 56108667300.00, Val loss 16473402368.00\n","==================================================================\n","Saved best model\n","Epoch 117, Loss 56120293778.50, Val loss 16462407680.00\n","Epoch 118, Loss 56135757830.50, Val loss 16464481280.00\n","==================================================================\n","Saved best model\n","Epoch 119, Loss 56142191288.00, Val loss 16457378816.00\n","==================================================================\n","Saved best model\n","Epoch 120, Loss 56148129290.50, Val loss 16452835328.00\n","==================================================================\n","Saved best model\n","Epoch 121, Loss 56149818652.50, Val loss 16448702464.00\n","==================================================================\n","Saved best model\n","Epoch 122, Loss 56148491553.50, Val loss 16440389632.00\n","==================================================================\n","Saved best model\n","Epoch 123, Loss 56145170622.50, Val loss 16439621632.00\n","==================================================================\n","Saved best model\n","Epoch 124, Loss 56135058819.00, Val loss 16428048384.00\n","==================================================================\n","Saved best model\n","Epoch 125, Loss 56126506810.00, Val loss 16424886272.00\n","==================================================================\n","Saved best model\n","Epoch 126, Loss 56111228612.00, Val loss 16413562880.00\n","==================================================================\n","Saved best model\n","Epoch 127, Loss 56095738029.00, Val loss 16403282944.00\n","==================================================================\n","Saved best model\n","Epoch 128, Loss 56075675321.00, Val loss 16391903232.00\n","==================================================================\n","Saved best model\n","Epoch 129, Loss 56053465739.50, Val loss 16376372224.00\n","==================================================================\n","Saved best model\n","Epoch 130, Loss 56028425428.00, Val loss 16358958080.00\n","==================================================================\n","Saved best model\n","Epoch 131, Loss 55999322283.50, Val loss 16337399808.00\n","==================================================================\n","Saved best model\n","Epoch 132, Loss 55968991045.50, Val loss 16316743680.00\n","==================================================================\n","Saved best model\n","Epoch 133, Loss 55933792199.00, Val loss 16287252480.00\n","==================================================================\n","Saved best model\n","Epoch 134, Loss 55897208901.00, Val loss 16254922752.00\n","==================================================================\n","Saved best model\n","Epoch 135, Loss 55857534393.00, Val loss 16221869056.00\n","==================================================================\n","Saved best model\n","Epoch 136, Loss 55813092217.50, Val loss 16185544704.00\n","==================================================================\n","Saved best model\n","Epoch 137, Loss 55796952075.00, Val loss 16120864768.00\n","==================================================================\n","Saved best model\n","Epoch 138, Loss 55739870421.50, Val loss 16110123008.00\n","==================================================================\n","Saved best model\n","Epoch 139, Loss 55673791717.00, Val loss 16054545408.00\n","==================================================================\n","Saved best model\n","Epoch 140, Loss 55619553402.00, Val loss 15983120384.00\n","==================================================================\n","Saved best model\n","Epoch 141, Loss 55565165707.50, Val loss 15911942144.00\n","==================================================================\n","Saved best model\n","Epoch 142, Loss 55506399935.50, Val loss 15835964416.00\n","==================================================================\n","Saved best model\n","Epoch 143, Loss 55445516614.00, Val loss 15752691712.00\n","==================================================================\n","Saved best model\n","Epoch 144, Loss 55383150965.00, Val loss 15665072128.00\n","==================================================================\n","Saved best model\n","Epoch 145, Loss 55317612062.50, Val loss 15570411520.00\n","==================================================================\n","Saved best model\n","Epoch 146, Loss 55251360693.00, Val loss 15466851328.00\n","==================================================================\n","Saved best model\n","Epoch 147, Loss 55183380368.50, Val loss 15351519232.00\n","==================================================================\n","Saved best model\n","Epoch 148, Loss 55114191878.00, Val loss 15231112192.00\n","==================================================================\n","Saved best model\n","Epoch 149, Loss 55044050947.00, Val loss 15100923904.00\n","==================================================================\n","Saved best model\n","Epoch 150, Loss 54974203966.50, Val loss 14962443264.00\n","==================================================================\n","Saved best model\n","Epoch 151, Loss 54904114366.50, Val loss 14814156800.00\n","==================================================================\n","Saved best model\n","Epoch 152, Loss 54835319820.50, Val loss 14655165440.00\n","==================================================================\n","Saved best model\n","Epoch 153, Loss 54768427233.00, Val loss 14485106688.00\n","==================================================================\n","Saved best model\n","Epoch 154, Loss 54704599099.00, Val loss 14305856512.00\n","==================================================================\n","Saved best model\n","Epoch 155, Loss 54643972235.50, Val loss 14111848448.00\n","==================================================================\n","Saved best model\n","Epoch 156, Loss 54546392409.00, Val loss 13903820800.00\n","==================================================================\n","Saved best model\n","Epoch 157, Loss 55366489670.50, Val loss 9760633856.00\n","Epoch 158, Loss 54901137629.50, Val loss 26696429568.00\n","Epoch 159, Loss 57914477306.00, Val loss 19102623744.00\n","Epoch 160, Loss 54057314253.00, Val loss 10294123520.00\n","Epoch 161, Loss 55145120731.00, Val loss 21892874240.00\n","Epoch 162, Loss 58709791818.00, Val loss 28771465216.00\n","Epoch 163, Loss 58302156024.00, Val loss 15644675072.00\n","Epoch 164, Loss 50589769654.50, Val loss 20990023680.00\n","Epoch 165, Loss 49961514858.00, Val loss 18917582848.00\n","Epoch 166, Loss 49524144694.00, Val loss 18429911040.00\n","Epoch 167, Loss 52676963541.00, Val loss 18096850944.00\n","Epoch 168, Loss 51135530959.00, Val loss 14170994688.00\n","Epoch 169, Loss 60006277068.00, Val loss 17646280704.00\n","Epoch 170, Loss 62480055003.00, Val loss 44815515648.00\n","Epoch 171, Loss 73840545552.00, Val loss 19380383744.00\n","Epoch 172, Loss 48880175982.00, Val loss 18689503232.00\n","Epoch 173, Loss 49527488125.00, Val loss 17463828480.00\n","Epoch 174, Loss 50638913112.00, Val loss 13399241728.00\n","Epoch 175, Loss 53202729694.50, Val loss 11318055936.00\n","Epoch 176, Loss 55304826023.50, Val loss 10403620864.00\n","Epoch 177, Loss 56145244917.00, Val loss 9950802944.00\n","==================================================================\n","Saved best model\n","Epoch 178, Loss 56311918737.00, Val loss 9743935488.00\n","Epoch 179, Loss 55967931407.00, Val loss 9757432832.00\n","Epoch 180, Loss 55229590604.00, Val loss 10113620992.00\n","Epoch 181, Loss 53777786540.00, Val loss 11162947584.00\n","Epoch 182, Loss 51630996570.50, Val loss 13334556672.00\n","Epoch 183, Loss 49746632450.00, Val loss 15893287936.00\n","Epoch 184, Loss 49367728406.00, Val loss 16695990272.00\n","Epoch 185, Loss 49543079418.00, Val loss 14864726016.00\n","Epoch 186, Loss 49969832043.00, Val loss 12252626944.00\n","Epoch 187, Loss 51450451348.50, Val loss 11164340224.00\n","Epoch 188, Loss 52378558103.00, Val loss 10896517120.00\n","Epoch 189, Loss 52562415861.00, Val loss 10844385280.00\n","Epoch 190, Loss 52520944479.00, Val loss 10833494016.00\n","Epoch 191, Loss 52480213850.00, Val loss 10822902784.00\n","Epoch 192, Loss 52455529190.00, Val loss 10804189184.00\n","Epoch 193, Loss 52444375473.50, Val loss 10777415680.00\n","Epoch 194, Loss 52445642812.50, Val loss 10747086848.00\n","Epoch 195, Loss 52454395190.00, Val loss 10713980928.00\n","Epoch 196, Loss 52471120112.00, Val loss 10679592960.00\n","Epoch 197, Loss 52492035752.50, Val loss 10645805056.00\n","Epoch 198, Loss 52514440501.50, Val loss 10614129664.00\n","Epoch 199, Loss 52537707930.00, Val loss 10582692864.00\n","Epoch 200, Loss 52562690358.50, Val loss 10553119744.00\n","Epoch 201, Loss 52587781386.00, Val loss 10524007424.00\n","Epoch 202, Loss 52614937437.50, Val loss 10496169984.00\n","Epoch 203, Loss 52642406313.00, Val loss 10470013952.00\n","Epoch 204, Loss 52669942034.00, Val loss 10443969536.00\n","Epoch 205, Loss 52699120557.00, Val loss 10419908608.00\n","Epoch 206, Loss 52727269485.50, Val loss 10397733888.00\n","Epoch 207, Loss 52755003440.50, Val loss 10374502400.00\n","Epoch 208, Loss 52784798918.00, Val loss 10353693696.00\n","Epoch 209, Loss 52813849288.50, Val loss 10333165568.00\n","Epoch 210, Loss 52843032670.50, Val loss 10313962496.00\n","Epoch 211, Loss 52871949862.50, Val loss 10294384640.00\n","Epoch 212, Loss 52903732414.00, Val loss 10276636672.00\n","Epoch 213, Loss 52932826093.00, Val loss 10261140480.00\n","Epoch 214, Loss 52955024309.00, Val loss 10247237632.00\n","Epoch 215, Loss 52979778652.50, Val loss 10231010304.00\n","Epoch 216, Loss 53009046964.00, Val loss 10215578624.00\n","Epoch 217, Loss 52984223582.00, Val loss 10201377792.00\n","Epoch 218, Loss 53066836836.00, Val loss 10186734592.00\n","Epoch 219, Loss 53096027818.00, Val loss 10174484480.00\n","Epoch 220, Loss 53123801144.50, Val loss 10160520192.00\n","Epoch 221, Loss 53152836196.50, Val loss 10149621760.00\n","Epoch 222, Loss 53179832959.00, Val loss 10135644160.00\n","Epoch 223, Loss 53210414894.00, Val loss 10125024256.00\n","Epoch 224, Loss 53237265541.50, Val loss 10112978944.00\n","Epoch 225, Loss 53266510264.00, Val loss 10102590464.00\n","Epoch 226, Loss 53293533834.00, Val loss 10092328960.00\n","Epoch 227, Loss 53321269224.50, Val loss 10081390592.00\n","Epoch 228, Loss 53349756975.00, Val loss 10072839168.00\n","Epoch 229, Loss 53375134663.00, Val loss 10063522816.00\n","Epoch 230, Loss 53401511398.50, Val loss 10055203840.00\n","Epoch 231, Loss 53427465460.00, Val loss 10045399040.00\n","Epoch 232, Loss 54107002295.00, Val loss 10034895872.00\n","Epoch 233, Loss 53480737272.50, Val loss 10033115136.00\n","Epoch 234, Loss 53500657269.00, Val loss 10021895168.00\n","Epoch 235, Loss 53530134871.00, Val loss 10013854720.00\n","Epoch 236, Loss 53555855184.00, Val loss 10005494784.00\n","Epoch 237, Loss 53582036260.50, Val loss 9998280704.00\n","Epoch 238, Loss 53606930597.00, Val loss 9989363712.00\n","Epoch 239, Loss 53634226213.50, Val loss 9982474240.00\n","Epoch 240, Loss 53566746586.50, Val loss 9974165504.00\n","Epoch 241, Loss 53683012253.50, Val loss 9969695744.00\n","Epoch 242, Loss 53705257273.00, Val loss 9963030528.00\n","Epoch 243, Loss 53728917574.50, Val loss 9955986432.00\n","Epoch 244, Loss 53753262773.50, Val loss 9949322240.00\n","Epoch 245, Loss 53777250440.00, Val loss 9943461888.00\n","Epoch 246, Loss 53799672077.50, Val loss 9936984064.00\n","Epoch 247, Loss 53820886608.00, Val loss 9930771456.00\n","Epoch 248, Loss 53851111040.50, Val loss 9929309184.00\n","Epoch 249, Loss 53868009322.00, Val loss 9925629952.00\n","Epoch 250, Loss 53887512726.50, Val loss 9918163968.00\n","Epoch 251, Loss 53909339362.50, Val loss 9915398144.00\n","Epoch 252, Loss 53927422044.00, Val loss 9906648064.00\n","Epoch 253, Loss 53951826866.50, Val loss 9904166912.00\n","Epoch 254, Loss 53969271758.50, Val loss 9896183808.00\n","Epoch 255, Loss 53992096120.00, Val loss 9893534720.00\n","Epoch 256, Loss 54009442590.50, Val loss 9886097408.00\n","Epoch 257, Loss 54031529310.00, Val loss 9882053632.00\n","Epoch 258, Loss 54050347037.00, Val loss 9876000768.00\n","Epoch 259, Loss 54069700333.50, Val loss 9873689600.00\n","Epoch 260, Loss 54085499688.00, Val loss 9865889792.00\n","Epoch 261, Loss 54107345173.00, Val loss 9863075840.00\n","Epoch 262, Loss 54123142412.00, Val loss 9856752640.00\n","Epoch 263, Loss 54142684301.00, Val loss 9853837312.00\n","Epoch 264, Loss 54158406699.00, Val loss 9846809600.00\n","Epoch 265, Loss 54178344003.50, Val loss 9843843072.00\n","Epoch 266, Loss 54193531912.00, Val loss 9838329856.00\n","Epoch 267, Loss 54210605257.00, Val loss 9835502592.00\n","Epoch 268, Loss 54225358031.00, Val loss 9829035008.00\n","Epoch 269, Loss 54243446514.00, Val loss 9826322432.00\n","Epoch 270, Loss 54257846944.00, Val loss 9820441600.00\n","Epoch 271, Loss 54274498193.00, Val loss 9817250816.00\n","Epoch 272, Loss 54288303987.00, Val loss 9812411392.00\n","Epoch 273, Loss 54303144363.00, Val loss 9808967680.00\n","Epoch 274, Loss 54316827317.00, Val loss 9804438528.00\n","Epoch 275, Loss 54331218629.50, Val loss 9801200640.00\n","Epoch 276, Loss 54344560263.00, Val loss 9795496960.00\n","Epoch 277, Loss 54359995697.50, Val loss 9792766976.00\n","Epoch 278, Loss 54371416301.00, Val loss 9789388800.00\n","Epoch 279, Loss 54383332957.50, Val loss 9785511936.00\n","Epoch 280, Loss 54395964502.50, Val loss 9781350400.00\n","Epoch 281, Loss 54408666338.50, Val loss 9777593344.00\n","Epoch 282, Loss 54418925493.00, Val loss 9774534656.00\n","Epoch 283, Loss 54427367991.00, Val loss 9770241024.00\n","Epoch 284, Loss 54439204664.00, Val loss 9767103488.00\n","Epoch 285, Loss 54449028342.00, Val loss 9763437568.00\n","Epoch 286, Loss 54459615772.00, Val loss 9760250880.00\n","Epoch 287, Loss 54469704014.00, Val loss 9756808192.00\n","Epoch 288, Loss 54479526400.50, Val loss 9754395648.00\n","Epoch 289, Loss 54487641872.00, Val loss 9750725632.00\n","Epoch 290, Loss 54497703636.00, Val loss 9748004864.00\n","Epoch 291, Loss 54505817296.00, Val loss 9744406528.00\n","==================================================================\n","Saved best model\n","Epoch 292, Loss 54515125334.00, Val loss 9741741056.00\n","==================================================================\n","Saved best model\n","Epoch 293, Loss 54522863480.50, Val loss 9739105280.00\n","==================================================================\n","Saved best model\n","Epoch 294, Loss 54529827652.00, Val loss 9736572928.00\n","==================================================================\n","Saved best model\n","Epoch 295, Loss 54537216236.50, Val loss 9733332992.00\n","==================================================================\n","Saved best model\n","Epoch 296, Loss 54544588503.00, Val loss 9731136512.00\n","==================================================================\n","Saved best model\n","Epoch 297, Loss 54551212320.50, Val loss 9728888832.00\n","==================================================================\n","Saved best model\n","Epoch 298, Loss 54556589896.50, Val loss 9726100480.00\n","==================================================================\n","Saved best model\n","Epoch 299, Loss 54563252616.50, Val loss 9724056576.00\n","==================================================================\n","Saved best model\n","Epoch 300, Loss 54568006161.00, Val loss 9721615360.00\n","==================================================================\n","Saved best model\n","Epoch 301, Loss 54573510948.50, Val loss 9720401920.00\n","==================================================================\n","Saved best model\n","Epoch 302, Loss 54577140580.50, Val loss 9717288960.00\n","==================================================================\n","Saved best model\n","Epoch 303, Loss 54583148441.50, Val loss 9716202496.00\n","==================================================================\n","Saved best model\n","Epoch 304, Loss 54586308742.50, Val loss 9713991680.00\n","==================================================================\n","Saved best model\n","Epoch 305, Loss 54590761533.00, Val loss 9711972352.00\n","==================================================================\n","Saved best model\n","Epoch 306, Loss 54594385933.50, Val loss 9711440896.00\n","==================================================================\n","Saved best model\n","Epoch 307, Loss 54596064781.50, Val loss 9709604864.00\n","==================================================================\n","Saved best model\n","Epoch 308, Loss 54599106668.50, Val loss 9708065792.00\n","==================================================================\n","Saved best model\n","Epoch 309, Loss 54601749692.50, Val loss 9707257856.00\n","==================================================================\n","Saved best model\n","Epoch 310, Loss 54603013093.00, Val loss 9706443776.00\n","==================================================================\n","Saved best model\n","Epoch 311, Loss 54604206194.50, Val loss 9704468480.00\n","Epoch 312, Loss 54606959169.50, Val loss 9704498176.00\n","==================================================================\n","Saved best model\n","Epoch 313, Loss 54606210533.00, Val loss 9704212480.00\n","==================================================================\n","Saved best model\n","Epoch 314, Loss 54607186420.00, Val loss 9703022592.00\n","==================================================================\n","Saved best model\n","Epoch 315, Loss 54601026974.50, Val loss 9701426176.00\n","Epoch 316, Loss 54602155366.50, Val loss 9701696512.00\n","Epoch 317, Loss 54600160594.50, Val loss 9702435840.00\n","Epoch 318, Loss 54597402528.50, Val loss 9702284288.00\n","Epoch 319, Loss 54596057377.00, Val loss 9702300672.00\n","Epoch 320, Loss 54594272992.50, Val loss 9702043648.00\n","Epoch 321, Loss 54592717278.50, Val loss 9702917120.00\n","Epoch 322, Loss 54589629481.00, Val loss 9703121920.00\n","Epoch 323, Loss 54586640720.50, Val loss 9704685568.00\n","Epoch 324, Loss 54581019674.50, Val loss 9705100288.00\n","Epoch 325, Loss 54578169610.50, Val loss 9705567232.00\n","Epoch 326, Loss 54574620898.00, Val loss 9708174336.00\n","Epoch 327, Loss 54567379606.00, Val loss 9710065664.00\n","Epoch 328, Loss 54561485254.00, Val loss 9711128576.00\n","Epoch 329, Loss 54556478047.00, Val loss 9713162240.00\n","Epoch 330, Loss 54550273398.50, Val loss 9714518016.00\n","Epoch 331, Loss 54543998466.50, Val loss 9717899264.00\n","Epoch 332, Loss 54535229556.00, Val loss 9719387136.00\n","Epoch 333, Loss 54529175683.50, Val loss 9722514432.00\n","Epoch 334, Loss 54520355055.50, Val loss 9724627968.00\n","Epoch 335, Loss 54512509585.50, Val loss 9728414720.00\n","Epoch 336, Loss 54502770072.00, Val loss 9730869248.00\n","Epoch 337, Loss 54494537580.00, Val loss 9734522880.00\n","Epoch 338, Loss 54484286968.00, Val loss 9738452992.00\n","Epoch 339, Loss 54473348583.50, Val loss 9742414848.00\n","Epoch 340, Loss 54462775951.50, Val loss 9746330624.00\n","Epoch 341, Loss 54451182085.50, Val loss 9750925312.00\n","Epoch 342, Loss 54440032362.00, Val loss 9754819584.00\n","Epoch 343, Loss 54428018083.00, Val loss 9759211520.00\n","Epoch 344, Loss 54416864885.50, Val loss 9763850240.00\n","Epoch 345, Loss 54403245265.50, Val loss 9769835520.00\n","Epoch 346, Loss 54388526874.00, Val loss 9776328704.00\n","Epoch 347, Loss 54372346325.00, Val loss 9782123520.00\n","Epoch 348, Loss 54358417089.50, Val loss 9786647552.00\n","Epoch 349, Loss 54345125358.50, Val loss 9793614848.00\n","Epoch 350, Loss 54328711637.00, Val loss 9799671808.00\n","Epoch 351, Loss 54313722926.50, Val loss 9806120960.00\n","Epoch 352, Loss 54297494548.50, Val loss 9813715968.00\n","Epoch 353, Loss 54279694897.00, Val loss 9820825600.00\n","Epoch 354, Loss 54262515846.00, Val loss 9828594688.00\n","Epoch 355, Loss 54244007512.00, Val loss 9835435008.00\n","Epoch 356, Loss 54227123966.00, Val loss 9844122624.00\n","Epoch 357, Loss 54207122043.00, Val loss 9852365824.00\n","Epoch 358, Loss 54187970877.50, Val loss 9860461568.00\n","Epoch 359, Loss 54167678275.50, Val loss 9869820928.00\n","Epoch 360, Loss 54147059465.50, Val loss 9879558144.00\n","Epoch 361, Loss 54124887194.00, Val loss 9888717824.00\n","Epoch 362, Loss 54104053227.00, Val loss 9898323968.00\n","Epoch 363, Loss 54082713676.00, Val loss 9908885504.00\n","Epoch 364, Loss 54059606389.00, Val loss 9919000576.00\n","Epoch 365, Loss 54037379639.00, Val loss 9928736768.00\n","Epoch 366, Loss 54015365916.00, Val loss 9940286464.00\n","Epoch 367, Loss 53990727228.00, Val loss 9951822848.00\n","Epoch 368, Loss 53965744051.00, Val loss 9963080704.00\n","Epoch 369, Loss 53941996888.00, Val loss 9975271424.00\n","Epoch 370, Loss 53916084745.00, Val loss 9987763200.00\n","Epoch 371, Loss 53905944823.50, Val loss 9993119744.00\n","Epoch 372, Loss 53882125211.50, Val loss 10004703232.00\n","Epoch 373, Loss 53855047659.00, Val loss 10020612096.00\n","Epoch 374, Loss 53823061176.00, Val loss 10033405952.00\n","Epoch 375, Loss 53796328642.00, Val loss 10046182400.00\n","Epoch 376, Loss 53769189203.50, Val loss 10059993088.00\n","Epoch 377, Loss 53740908494.00, Val loss 10074163200.00\n","Epoch 378, Loss 53712098424.50, Val loss 10090699776.00\n","Epoch 379, Loss 53679745657.50, Val loss 10107456512.00\n","Epoch 380, Loss 53647943028.50, Val loss 10121785344.00\n","Epoch 381, Loss 53618863497.00, Val loss 10139032576.00\n","Epoch 382, Loss 53586266884.00, Val loss 10155360256.00\n","Epoch 383, Loss 53555045520.50, Val loss 10174325760.00\n","Epoch 384, Loss 53519666971.50, Val loss 10191680512.00\n","Epoch 385, Loss 53486876876.50, Val loss 10208685056.00\n","Epoch 386, Loss 53454958317.50, Val loss 10228008960.00\n","Epoch 387, Loss 53419619878.50, Val loss 10246295552.00\n","Epoch 388, Loss 53386064313.00, Val loss 10264348672.00\n","Epoch 389, Loss 53351658786.00, Val loss 10283309056.00\n","Epoch 390, Loss 53317925045.00, Val loss 10304411648.00\n","Epoch 391, Loss 53280544098.00, Val loss 10323962880.00\n","Epoch 392, Loss 53245001966.50, Val loss 10346503168.00\n","Epoch 393, Loss 53205389150.00, Val loss 10366532608.00\n","Epoch 394, Loss 53170240812.00, Val loss 10388445184.00\n","Epoch 395, Loss 53132509460.00, Val loss 10410317824.00\n","Epoch 396, Loss 53095049959.50, Val loss 10430545920.00\n","Epoch 397, Loss 53059355155.50, Val loss 10454823936.00\n","Epoch 398, Loss 53018050869.00, Val loss 10478350336.00\n","Epoch 399, Loss 52978633212.50, Val loss 10500449280.00\n","Epoch 400, Loss 52941086700.50, Val loss 10524371968.00\n","Epoch 401, Loss 52901121835.50, Val loss 10548434944.00\n","Epoch 402, Loss 52861096082.00, Val loss 10573258752.00\n","Epoch 403, Loss 52819973515.00, Val loss 10598344704.00\n","Epoch 404, Loss 52779785761.50, Val loss 10623025152.00\n","Epoch 405, Loss 52739362901.50, Val loss 10649575424.00\n","Epoch 406, Loss 52697153683.50, Val loss 10676784128.00\n","Epoch 407, Loss 52652938030.00, Val loss 10702319616.00\n","Epoch 408, Loss 52614192843.00, Val loss 10730421248.00\n","Epoch 409, Loss 52569333568.00, Val loss 10758697984.00\n","Epoch 410, Loss 52526578202.50, Val loss 10787319808.00\n","Epoch 411, Loss 52482148478.00, Val loss 10817572864.00\n","Epoch 412, Loss 52436242937.00, Val loss 10843067392.00\n","Epoch 413, Loss 52397882891.50, Val loss 10874492928.00\n","Epoch 414, Loss 52352172016.00, Val loss 10903513088.00\n","Epoch 415, Loss 52307826095.50, Val loss 10934848512.00\n","Epoch 416, Loss 52263121155.50, Val loss 10963500032.00\n","Epoch 417, Loss 52219860848.00, Val loss 10994644992.00\n","Epoch 418, Loss 52176303609.50, Val loss 11027275776.00\n","Epoch 419, Loss 52128855082.50, Val loss 11060049920.00\n","Epoch 420, Loss 52084415349.00, Val loss 11093111808.00\n","Epoch 421, Loss 52035445514.00, Val loss 11121478656.00\n","Epoch 422, Loss 51997336415.50, Val loss 11156597760.00\n","Epoch 423, Loss 51948824062.00, Val loss 11191391232.00\n","Epoch 424, Loss 51901987506.00, Val loss 11223815168.00\n","Epoch 425, Loss 51856694888.50, Val loss 11260087296.00\n","Epoch 426, Loss 51810326552.00, Val loss 11292862464.00\n","Epoch 427, Loss 51765160729.00, Val loss 11330085888.00\n","Epoch 428, Loss 51715279113.50, Val loss 11360803840.00\n","Epoch 429, Loss 51676785796.00, Val loss 11397861376.00\n","Epoch 430, Loss 51628049852.50, Val loss 11435624448.00\n","Epoch 431, Loss 51581870740.50, Val loss 11470038016.00\n","Epoch 432, Loss 51535556519.00, Val loss 11508744192.00\n","Epoch 433, Loss 51486602880.50, Val loss 11550331904.00\n","Epoch 434, Loss 51434386741.00, Val loss 11580967936.00\n","Epoch 435, Loss 51395034161.00, Val loss 11618673664.00\n","Epoch 436, Loss 51349601049.00, Val loss 11657089024.00\n","Epoch 437, Loss 51302849906.50, Val loss 11695597568.00\n","Epoch 438, Loss 51258062710.00, Val loss 11733881856.00\n","Epoch 439, Loss 51212397740.50, Val loss 11774129152.00\n","Epoch 440, Loss 51164851942.00, Val loss 11810185216.00\n","Epoch 441, Loss 51123331403.00, Val loss 11850162176.00\n","Epoch 442, Loss 51077436620.50, Val loss 11890937856.00\n","Epoch 443, Loss 51031335464.50, Val loss 11931587584.00\n","Epoch 444, Loss 50986247643.00, Val loss 11971569664.00\n","Epoch 445, Loss 50941699346.00, Val loss 12014368768.00\n","Epoch 446, Loss 50894787441.00, Val loss 12053632000.00\n","Epoch 447, Loss 50852981395.50, Val loss 12096756736.00\n","Epoch 448, Loss 50806695242.00, Val loss 12139334656.00\n","Epoch 449, Loss 50761953159.50, Val loss 12181296128.00\n","Epoch 450, Loss 50718247272.00, Val loss 12223741952.00\n","Epoch 451, Loss 50673973632.50, Val loss 12266714112.00\n","Epoch 452, Loss 50630751421.50, Val loss 12310450176.00\n","Epoch 453, Loss 50586687725.50, Val loss 12353308672.00\n","Epoch 454, Loss 50543692175.00, Val loss 12396897280.00\n","Epoch 455, Loss 50500804227.00, Val loss 12440825856.00\n","Epoch 456, Loss 50458671502.00, Val loss 12485339136.00\n","Epoch 457, Loss 50415724494.00, Val loss 12529908736.00\n","Epoch 458, Loss 50374111451.00, Val loss 12575596544.00\n","Epoch 459, Loss 50330198156.50, Val loss 12617895936.00\n","Epoch 460, Loss 50292292459.00, Val loss 12665257984.00\n","Epoch 461, Loss 50249238850.00, Val loss 12710117376.00\n","Epoch 462, Loss 50208574247.00, Val loss 12758251520.00\n","Epoch 463, Loss 50166101095.00, Val loss 12805360640.00\n","Epoch 464, Loss 50125016775.00, Val loss 12848889856.00\n","Epoch 465, Loss 50088107230.00, Val loss 12897213440.00\n","Epoch 466, Loss 50046970079.00, Val loss 12942120960.00\n","Epoch 467, Loss 50007882137.50, Val loss 12991078400.00\n","Epoch 468, Loss 49968106149.50, Val loss 13038553088.00\n","Epoch 469, Loss 49928443306.00, Val loss 13084940288.00\n","Epoch 470, Loss 49889810777.50, Val loss 13132846080.00\n","Epoch 471, Loss 49851908585.50, Val loss 13182908416.00\n","Epoch 472, Loss 49812650836.00, Val loss 13231408128.00\n","Epoch 473, Loss 49775021161.00, Val loss 13278046208.00\n","Epoch 474, Loss 49738414847.50, Val loss 13325908992.00\n","Epoch 475, Loss 49701664303.50, Val loss 13375279104.00\n","Epoch 476, Loss 49664752344.50, Val loss 13424346112.00\n","Epoch 477, Loss 49628596451.50, Val loss 13471860736.00\n","Epoch 478, Loss 49593428659.50, Val loss 13519164416.00\n","Epoch 479, Loss 49558636933.00, Val loss 13568928768.00\n","Epoch 480, Loss 49523312170.50, Val loss 13618493440.00\n","Epoch 481, Loss 49488408368.00, Val loss 13669527552.00\n","Epoch 482, Loss 49453973597.50, Val loss 13719078912.00\n","Epoch 483, Loss 49419701276.50, Val loss 13771928576.00\n","Epoch 484, Loss 49385112970.00, Val loss 13825388544.00\n","Epoch 485, Loss 49350601648.00, Val loss 13875513344.00\n","Epoch 486, Loss 49319120351.50, Val loss 13928001536.00\n","Epoch 487, Loss 49285214647.00, Val loss 13976676352.00\n","Epoch 488, Loss 49255659079.50, Val loss 14030521344.00\n","Epoch 489, Loss 49222652810.50, Val loss 14081940480.00\n","Epoch 490, Loss 49192417358.50, Val loss 14132467712.00\n","Epoch 491, Loss 49162141827.50, Val loss 14186999808.00\n","Epoch 492, Loss 49131608906.00, Val loss 14238179328.00\n","Epoch 493, Loss 49102001810.00, Val loss 14292284416.00\n","Epoch 494, Loss 49072496347.00, Val loss 14343282688.00\n","Epoch 495, Loss 49044405073.50, Val loss 14398456832.00\n","Epoch 496, Loss 49015679843.00, Val loss 14449024000.00\n","Epoch 497, Loss 48988392067.00, Val loss 14505668608.00\n","Epoch 498, Loss 48960819140.50, Val loss 14558356480.00\n","Epoch 499, Loss 48933165656.00, Val loss 14613595136.00\n","Epoch 500, Loss 48907291876.00, Val loss 14663497728.00\n","Epoch 501, Loss 48881623400.50, Val loss 14723637248.00\n","Epoch 502, Loss 48854868044.00, Val loss 14773814272.00\n","Epoch 503, Loss 48829676009.50, Val loss 14830760960.00\n","Epoch 504, Loss 48804354716.00, Val loss 14884442112.00\n","Epoch 505, Loss 48779240724.50, Val loss 14944331776.00\n","Epoch 506, Loss 48754745821.00, Val loss 14998030336.00\n","Epoch 507, Loss 48731276825.00, Val loss 15056727040.00\n","Epoch 508, Loss 48709250906.50, Val loss 15109562368.00\n","Epoch 509, Loss 48686773405.00, Val loss 15164546048.00\n","Epoch 510, Loss 48666681835.00, Val loss 15216285696.00\n","Epoch 511, Loss 48645600222.00, Val loss 15276704768.00\n","Epoch 512, Loss 48624638895.00, Val loss 15325811712.00\n","Epoch 513, Loss 48606153619.00, Val loss 15389222912.00\n","Epoch 514, Loss 48585801298.50, Val loss 15437853696.00\n","Epoch 515, Loss 48567450607.00, Val loss 15496732672.00\n","Epoch 516, Loss 48549582383.00, Val loss 15550962688.00\n","Epoch 517, Loss 48530891819.50, Val loss 15611130880.00\n","Epoch 518, Loss 48514008457.50, Val loss 15663305728.00\n","Epoch 519, Loss 48496423823.50, Val loss 15720837120.00\n","Epoch 520, Loss 48480809597.00, Val loss 15773020160.00\n","Epoch 521, Loss 48464727348.00, Val loss 15834987520.00\n","Epoch 522, Loss 48449629553.00, Val loss 15888675840.00\n","Epoch 523, Loss 48433650039.00, Val loss 15947998208.00\n","Epoch 524, Loss 48420259042.00, Val loss 16001686528.00\n","Epoch 525, Loss 48405502309.00, Val loss 16061448192.00\n","Epoch 526, Loss 48392844712.00, Val loss 16117184512.00\n","Epoch 527, Loss 48378623102.00, Val loss 16177082368.00\n","Epoch 528, Loss 48367243497.00, Val loss 16231668736.00\n","Epoch 529, Loss 48355500179.00, Val loss 16293865472.00\n","Epoch 530, Loss 48343236991.00, Val loss 16344493056.00\n","Epoch 531, Loss 48332712494.00, Val loss 16412365824.00\n","Epoch 532, Loss 48322379109.00, Val loss 16461345792.00\n","Epoch 533, Loss 48312116408.00, Val loss 16526703616.00\n","Epoch 534, Loss 48303229370.00, Val loss 16579263488.00\n","Epoch 535, Loss 48294428256.00, Val loss 16644327424.00\n","Epoch 536, Loss 48286323596.00, Val loss 16695799808.00\n","Epoch 537, Loss 48277401456.00, Val loss 16761140224.00\n","Epoch 538, Loss 48270758389.00, Val loss 16815174656.00\n","Epoch 539, Loss 48394010317.00, Val loss 16856545280.00\n","Epoch 540, Loss 48260541604.00, Val loss 16941911040.00\n","Epoch 541, Loss 48249892751.00, Val loss 16990450688.00\n","Epoch 542, Loss 48247073520.00, Val loss 17049048064.00\n","Epoch 543, Loss 48241030164.00, Val loss 17110106112.00\n","Epoch 544, Loss 48236348847.00, Val loss 17162107904.00\n","Epoch 545, Loss 48232034062.00, Val loss 17230073856.00\n","Epoch 546, Loss 48229721294.00, Val loss 17280049152.00\n","Epoch 547, Loss 48225686552.00, Val loss 17345198080.00\n","Epoch 548, Loss 48224907825.00, Val loss 17398605824.00\n","Epoch 549, Loss 48221913386.00, Val loss 17462618112.00\n","Epoch 550, Loss 48221614866.00, Val loss 17512910848.00\n","Epoch 551, Loss 48219871918.00, Val loss 17578276864.00\n","Epoch 552, Loss 48220148076.00, Val loss 17634224128.00\n","Epoch 553, Loss 48218036062.00, Val loss 17703280640.00\n","Epoch 554, Loss 48220192368.00, Val loss 17764341760.00\n","Epoch 555, Loss 48219036867.00, Val loss 17829718016.00\n","Epoch 556, Loss 48221053579.00, Val loss 17884721152.00\n","Epoch 557, Loss 48221713973.00, Val loss 17956401152.00\n","Epoch 558, Loss 48225651860.00, Val loss 18006243328.00\n","Epoch 559, Loss 48226067184.00, Val loss 18076293120.00\n","Epoch 560, Loss 48231027697.00, Val loss 18133487616.00\n","Epoch 561, Loss 48232598431.00, Val loss 18206203904.00\n","Epoch 562, Loss 48238630764.00, Val loss 18270969856.00\n","Epoch 563, Loss 48242880667.00, Val loss 18344417280.00\n","Epoch 564, Loss 48251477199.00, Val loss 18403680256.00\n","Epoch 565, Loss 48256300156.00, Val loss 18472241152.00\n","Epoch 566, Loss 48265317510.00, Val loss 18528518144.00\n","Epoch 567, Loss 48269526712.00, Val loss 18595796992.00\n","Epoch 568, Loss 48279712382.00, Val loss 18653687808.00\n","Epoch 569, Loss 48285897130.00, Val loss 18724732928.00\n","Epoch 570, Loss 48296891304.00, Val loss 18777837568.00\n","Epoch 571, Loss 48303124760.00, Val loss 18846279680.00\n","Epoch 572, Loss 48315898068.00, Val loss 18897403904.00\n","Epoch 573, Loss 48322073590.00, Val loss 18960056320.00\n","Epoch 574, Loss 48169216718.00, Val loss 18973052928.00\n","Epoch 575, Loss 48343212068.00, Val loss 19091965952.00\n","Epoch 576, Loss 48356524236.00, Val loss 19074775040.00\n","Epoch 577, Loss 48354251048.00, Val loss 19180013568.00\n","Epoch 578, Loss 48374683652.00, Val loss 19205648384.00\n","Epoch 579, Loss 48379707670.00, Val loss 19292504064.00\n","Epoch 580, Loss 48397446198.00, Val loss 19329245184.00\n","Epoch 581, Loss 48405637176.00, Val loss 19415734272.00\n","Epoch 582, Loss 48424688310.00, Val loss 19454562304.00\n","Epoch 583, Loss 48433918082.00, Val loss 19531958272.00\n","Epoch 584, Loss 48452019746.00, Val loss 19579490304.00\n","Epoch 585, Loss 48463869260.00, Val loss 19645999104.00\n","Epoch 586, Loss 48480033890.00, Val loss 19698958336.00\n","Epoch 587, Loss 48493890946.00, Val loss 19766212608.00\n","Epoch 588, Loss 48511321648.00, Val loss 19822929920.00\n","Epoch 589, Loss 48527389792.00, Val loss 19882283008.00\n","Epoch 590, Loss 48542881158.00, Val loss 19941656576.00\n","Epoch 591, Loss 48558515702.00, Val loss 20008972288.00\n","Epoch 592, Loss 48578665210.00, Val loss 20073654272.00\n","Epoch 593, Loss 48597362888.00, Val loss 20140238848.00\n","Epoch 594, Loss 48617448556.00, Val loss 20205557760.00\n","Epoch 595, Loss 48636949778.00, Val loss 20263618560.00\n","Epoch 596, Loss 48657018098.00, Val loss 20323622912.00\n","Epoch 597, Loss 48677155828.00, Val loss 20382894080.00\n","Epoch 598, Loss 48696348816.00, Val loss 20438552576.00\n","Epoch 599, Loss 48716252210.00, Val loss 20501989376.00\n","Epoch 600, Loss 48738095488.00, Val loss 20557148160.00\n","Epoch 601, Loss 48755992680.00, Val loss 20625788928.00\n","Epoch 602, Loss 48770935670.00, Val loss 20694149120.00\n","Epoch 603, Loss 48819163242.00, Val loss 20748969984.00\n","Epoch 604, Loss 48840558982.00, Val loss 20812709888.00\n","Epoch 605, Loss 48864127302.00, Val loss 20867237888.00\n","Epoch 606, Loss 48885249900.00, Val loss 20929128448.00\n","Epoch 607, Loss 48908869400.00, Val loss 20985880576.00\n","Epoch 608, Loss 48930072354.00, Val loss 21043519488.00\n","Epoch 609, Loss 48953200038.00, Val loss 21104459776.00\n","Epoch 610, Loss 48954380920.00, Val loss 21224318976.00\n","Epoch 611, Loss 49026073828.00, Val loss 21241675776.00\n","Epoch 612, Loss 49041721702.00, Val loss 21301692416.00\n","Epoch 613, Loss 49067181412.00, Val loss 21352433664.00\n","Epoch 614, Loss 49088779698.00, Val loss 21411670016.00\n","Epoch 615, Loss 49115970016.00, Val loss 21471653888.00\n","Epoch 616, Loss 49142001378.00, Val loss 21525145600.00\n","Epoch 617, Loss 49166807894.00, Val loss 21584035840.00\n","Epoch 618, Loss 49193390572.00, Val loss 21640323072.00\n","Epoch 619, Loss 49221558442.00, Val loss 21699864576.00\n","Epoch 620, Loss 49248228078.00, Val loss 21751177216.00\n","Epoch 621, Loss 49274041590.00, Val loss 21809897472.00\n","Epoch 622, Loss 49301623790.00, Val loss 21862017024.00\n","Epoch 623, Loss 49327340866.00, Val loss 21920874496.00\n","Epoch 624, Loss 49357135482.00, Val loss 21979430912.00\n","Epoch 625, Loss 49385910982.00, Val loss 22028756992.00\n","Epoch 626, Loss 49409679648.00, Val loss 22092339200.00\n","Epoch 627, Loss 49440832578.00, Val loss 22145533952.00\n","Epoch 628, Loss 49468163874.00, Val loss 22209980416.00\n","Epoch 629, Loss 49502299934.00, Val loss 22262444032.00\n","Epoch 630, Loss 49527947268.00, Val loss 22319022080.00\n","Epoch 631, Loss 49557669252.00, Val loss 22373081088.00\n","Epoch 632, Loss 49583854954.00, Val loss 22442768384.00\n","Epoch 633, Loss 49619980700.00, Val loss 22492868608.00\n","Epoch 634, Loss 49649367728.00, Val loss 22556641280.00\n","Epoch 635, Loss 49685529662.00, Val loss 22598823936.00\n","Epoch 636, Loss 49695498420.00, Val loss 22689265664.00\n","Epoch 637, Loss 49761714134.00, Val loss 22641930240.00\n","Epoch 638, Loss 49740742766.00, Val loss 22748174336.00\n","Epoch 639, Loss 49797174406.00, Val loss 22750068736.00\n","Epoch 640, Loss 49803198350.00, Val loss 22839961600.00\n","Epoch 641, Loss 49852944712.00, Val loss 22862804992.00\n","Epoch 642, Loss 49870108220.00, Val loss 22937270272.00\n","Epoch 643, Loss 49912199670.00, Val loss 22966781952.00\n","Epoch 644, Loss 49932321218.00, Val loss 23036039168.00\n","Epoch 645, Loss 49972830494.00, Val loss 23071111168.00\n","Epoch 646, Loss 49995884226.00, Val loss 23133136896.00\n","Epoch 647, Loss 50034092628.00, Val loss 23178616832.00\n","Epoch 648, Loss 50054842818.00, Val loss 23250614272.00\n","Epoch 649, Loss 50097134738.00, Val loss 23279892480.00\n","Epoch 650, Loss 50122446404.00, Val loss 23359121408.00\n","Epoch 651, Loss 50170087398.00, Val loss 23382616064.00\n","Epoch 652, Loss 50187924382.00, Val loss 23453954048.00\n","Epoch 653, Loss 50233031854.00, Val loss 23487010816.00\n","Epoch 654, Loss 50256002278.00, Val loss 23546255360.00\n","Epoch 655, Loss 50294175984.00, Val loss 23587561472.00\n","Epoch 656, Loss 50322844798.00, Val loss 23641726976.00\n","Epoch 657, Loss 50358805382.00, Val loss 23685310464.00\n","Epoch 658, Loss 50389572490.00, Val loss 23740311552.00\n","Epoch 659, Loss 50425944586.00, Val loss 23780532224.00\n","Epoch 660, Loss 50456515466.00, Val loss 23821635584.00\n","Epoch 661, Loss 50485574410.00, Val loss 23876839424.00\n","Epoch 662, Loss 50521669350.00, Val loss 23915165696.00\n","Epoch 663, Loss 50547110222.00, Val loss 23978420224.00\n","Epoch 664, Loss 50590230924.00, Val loss 24000329728.00\n","Epoch 665, Loss 50609058682.00, Val loss 24056264704.00\n","Epoch 666, Loss 50646628854.00, Val loss 24090361856.00\n","Epoch 667, Loss 50667527630.00, Val loss 24172677120.00\n","Epoch 668, Loss 50728866106.00, Val loss 24160137216.00\n","Epoch 669, Loss 50728373674.00, Val loss 24262096896.00\n","Epoch 670, Loss 50795301194.00, Val loss 24253939712.00\n","Epoch 671, Loss 50796836530.00, Val loss 24347160576.00\n","Epoch 672, Loss 50859357000.00, Val loss 24349810688.00\n","Epoch 673, Loss 50867125800.00, Val loss 24436160512.00\n","Epoch 674, Loss 50926380666.00, Val loss 24446048256.00\n","Epoch 675, Loss 50938303248.00, Val loss 24524339200.00\n","Epoch 676, Loss 50994683956.00, Val loss 24533671936.00\n","Epoch 677, Loss 51008049006.00, Val loss 24594989056.00\n","Epoch 678, Loss 51053894670.00, Val loss 24624893952.00\n","Epoch 679, Loss 51078214822.00, Val loss 24677378048.00\n","Epoch 680, Loss 51117973390.00, Val loss 24712581120.00\n","Epoch 681, Loss 51145743694.00, Val loss 24759025664.00\n","Epoch 682, Loss 51181678208.00, Val loss 24798908416.00\n","Epoch 683, Loss 51213177222.00, Val loss 24839917568.00\n","Epoch 684, Loss 51245992264.00, Val loss 24885811200.00\n","Epoch 685, Loss 51282007456.00, Val loss 24921745408.00\n","Epoch 686, Loss 51311379068.00, Val loss 24967223296.00\n","Epoch 687, Loss 51346481318.00, Val loss 25008883712.00\n","Epoch 688, Loss 51381588220.00, Val loss 25049663488.00\n","Epoch 689, Loss 51411590576.00, Val loss 25096370176.00\n","Epoch 690, Loss 51452380354.00, Val loss 25125824512.00\n","Epoch 691, Loss 51479706688.00, Val loss 25180901376.00\n","Epoch 692, Loss 51522885446.00, Val loss 25207216128.00\n","Epoch 693, Loss 51546741614.00, Val loss 25258080256.00\n","Epoch 694, Loss 51587349906.00, Val loss 25284507648.00\n","Epoch 695, Loss 51611961182.00, Val loss 25335003136.00\n","Epoch 696, Loss 51651923782.00, Val loss 25357170688.00\n","Epoch 697, Loss 51673224012.00, Val loss 25409255424.00\n","Epoch 698, Loss 51715847804.00, Val loss 25437378560.00\n","Epoch 699, Loss 51741230126.00, Val loss 25479186432.00\n","Epoch 700, Loss 51777169400.00, Val loss 25512302592.00\n","Epoch 701, Loss 51805715932.00, Val loss 25547255808.00\n","Epoch 702, Loss 51836629974.00, Val loss 25587271680.00\n","Epoch 703, Loss 51871265178.00, Val loss 25618708480.00\n","Epoch 704, Loss 51899353258.00, Val loss 25656799232.00\n","Epoch 705, Loss 51932948372.00, Val loss 25692086272.00\n","Epoch 706, Loss 51964542404.00, Val loss 25727451136.00\n","Epoch 707, Loss 51993830990.00, Val loss 25761658880.00\n","Epoch 708, Loss 52027471858.00, Val loss 25806600192.00\n","Epoch 709, Loss 52065407156.00, Val loss 25824897024.00\n","Epoch 710, Loss 52084483420.00, Val loss 25868294144.00\n","Epoch 711, Loss 52122068000.00, Val loss 25892675584.00\n","Epoch 712, Loss 52145595246.00, Val loss 25938184192.00\n","Epoch 713, Loss 52178267644.00, Val loss 25979142144.00\n","Epoch 714, Loss 52222058450.00, Val loss 26007468032.00\n","Epoch 715, Loss 52248916548.00, Val loss 26029635584.00\n","Epoch 716, Loss 52271441326.00, Val loss 26071113728.00\n","Epoch 717, Loss 52307049774.00, Val loss 26094438400.00\n","Epoch 718, Loss 52321685328.00, Val loss 26172540928.00\n","Epoch 719, Loss 52394561102.00, Val loss 26132324352.00\n","Epoch 720, Loss 52372048512.00, Val loss 26223228928.00\n","Epoch 721, Loss 52445691812.00, Val loss 26201264128.00\n","Epoch 722, Loss 52432520652.00, Val loss 26290759680.00\n","Epoch 723, Loss 52510261256.00, Val loss 26273337344.00\n","Epoch 724, Loss 52502742692.00, Val loss 26336851968.00\n","Epoch 725, Loss 52556965308.00, Val loss 26333462528.00\n","Epoch 726, Loss 52560632952.00, Val loss 26393841664.00\n","Epoch 727, Loss 52611787028.00, Val loss 26392711168.00\n","Epoch 728, Loss 52613382314.00, Val loss 26461390848.00\n","Epoch 729, Loss 52676727136.00, Val loss 26456506368.00\n","Epoch 730, Loss 52678663742.00, Val loss 26512908288.00\n","Epoch 731, Loss 52727969910.00, Val loss 26513487872.00\n","Epoch 732, Loss 52734629376.00, Val loss 26570641408.00\n","Epoch 733, Loss 52779953578.00, Val loss 26575446016.00\n","Epoch 734, Loss 52795326666.00, Val loss 26630039552.00\n","Epoch 735, Loss 52843059728.00, Val loss 26626598912.00\n","Epoch 736, Loss 52846790112.00, Val loss 26679306240.00\n","Epoch 737, Loss 52893335318.00, Val loss 26681593856.00\n","Epoch 738, Loss 52898212878.00, Val loss 26739437568.00\n","Epoch 739, Loss 52953261848.00, Val loss 26746867712.00\n","Epoch 740, Loss 52964987414.00, Val loss 26779150336.00\n","Epoch 741, Loss 52995832712.00, Val loss 26798241792.00\n","Epoch 742, Loss 53017375102.00, Val loss 26831511552.00\n","Epoch 743, Loss 53049457364.00, Val loss 26850844672.00\n","Epoch 744, Loss 53070482924.00, Val loss 26881046528.00\n","Epoch 745, Loss 53097619502.00, Val loss 26906476544.00\n","Epoch 746, Loss 53126868254.00, Val loss 26941966336.00\n","Epoch 747, Loss 53160986490.00, Val loss 26942687232.00\n","Epoch 748, Loss 53167426942.00, Val loss 26988697600.00\n","Epoch 749, Loss 53210462394.00, Val loss 26994573312.00\n","Epoch 750, Loss 53220733674.00, Val loss 27032502272.00\n","Epoch 751, Loss 53256180216.00, Val loss 27042174976.00\n","Epoch 752, Loss 53264236768.00, Val loss 27098902528.00\n","Epoch 753, Loss 53322695922.00, Val loss 27086102528.00\n","Epoch 754, Loss 53317443980.00, Val loss 27133714432.00\n","Epoch 755, Loss 53361299398.00, Val loss 27133736960.00\n","Epoch 756, Loss 53367911178.00, Val loss 27177680896.00\n","Epoch 757, Loss 53408796998.00, Val loss 27177789440.00\n","Epoch 758, Loss 53407365212.00, Val loss 27233374208.00\n","Epoch 759, Loss 53475943478.00, Val loss 27214313472.00\n","Epoch 760, Loss 53455583516.00, Val loss 27249094656.00\n","Epoch 761, Loss 53485835098.00, Val loss 27282999296.00\n","Epoch 762, Loss 53523072450.00, Val loss 27294873600.00\n","Epoch 763, Loss 53537149078.00, Val loss 27323873280.00\n","Epoch 764, Loss 53564793968.00, Val loss 27342565376.00\n","Epoch 765, Loss 53588959106.00, Val loss 27379087360.00\n","Epoch 766, Loss 53625371482.00, Val loss 27370170368.00\n","Epoch 767, Loss 53619894586.00, Val loss 27418249216.00\n","Epoch 768, Loss 53667408682.00, Val loss 27402661888.00\n","Epoch 769, Loss 53660648002.00, Val loss 27457703936.00\n","Epoch 770, Loss 53711449676.00, Val loss 27444484096.00\n","Epoch 771, Loss 53699401120.00, Val loss 27503423488.00\n","Epoch 772, Loss 53760626000.00, Val loss 27488571392.00\n","Epoch 773, Loss 53752934996.00, Val loss 27528101888.00\n","Epoch 774, Loss 53791305468.00, Val loss 27528065024.00\n","Epoch 775, Loss 53796087248.00, Val loss 27558313984.00\n","Epoch 776, Loss 53823778246.00, Val loss 27575212032.00\n","Epoch 777, Loss 53846718012.00, Val loss 27602497536.00\n","Epoch 778, Loss 53874993664.00, Val loss 27597512704.00\n","Epoch 779, Loss 53875437728.00, Val loss 27632721920.00\n","Epoch 780, Loss 53910255276.00, Val loss 27635392512.00\n","Epoch 781, Loss 53917736982.00, Val loss 27666550784.00\n","Epoch 782, Loss 53944888504.00, Val loss 27672602624.00\n","Epoch 783, Loss 53960013668.00, Val loss 27710836736.00\n","Epoch 784, Loss 53996991838.00, Val loss 27695194112.00\n","Epoch 785, Loss 53987711082.00, Val loss 27734949888.00\n","Epoch 786, Loss 54024026770.00, Val loss 27739328512.00\n","Epoch 787, Loss 54036107192.00, Val loss 27778783232.00\n","Epoch 788, Loss 54073887616.00, Val loss 27751641088.00\n","Epoch 789, Loss 54055492850.00, Val loss 27807639552.00\n","Epoch 790, Loss 54108287162.00, Val loss 27790678016.00\n","Epoch 791, Loss 54098964378.00, Val loss 27833470976.00\n","Epoch 792, Loss 54138329336.00, Val loss 27818663936.00\n","Epoch 793, Loss 54115660892.00, Val loss 27923120128.00\n","Epoch 794, Loss 54226043912.00, Val loss 27802218496.00\n","Epoch 795, Loss 54126020788.00, Val loss 27936813056.00\n","Epoch 796, Loss 54239184834.00, Val loss 27858225152.00\n","Epoch 797, Loss 54184448896.00, Val loss 27950929920.00\n","Epoch 798, Loss 54268769840.00, Val loss 27878930432.00\n","Epoch 799, Loss 54211450858.00, Val loss 27970205696.00\n","Epoch 800, Loss 54294186448.00, Val loss 27910129664.00\n","Epoch 801, Loss 54247488278.00, Val loss 27991449600.00\n","Epoch 802, Loss 54321115052.00, Val loss 27936049152.00\n","Epoch 803, Loss 54273391270.00, Val loss 28025327616.00\n","Epoch 804, Loss 54360586684.00, Val loss 27971184640.00\n","Epoch 805, Loss 54317210710.00, Val loss 28030164992.00\n","Epoch 806, Loss 54372797810.00, Val loss 27999594496.00\n","Epoch 807, Loss 54329358792.00, Val loss 28101457920.00\n","Epoch 808, Loss 54443821166.00, Val loss 28002297856.00\n","Epoch 809, Loss 54362120406.00, Val loss 28101277696.00\n","Epoch 810, Loss 54451324170.00, Val loss 28023887872.00\n","Epoch 811, Loss 54388292148.00, Val loss 28116996096.00\n","Epoch 812, Loss 54471772692.00, Val loss 28058761216.00\n","Epoch 813, Loss 54423023244.00, Val loss 28147017728.00\n","Epoch 814, Loss 54508667296.00, Val loss 28084639744.00\n","Epoch 815, Loss 54457534672.00, Val loss 28152993792.00\n","Epoch 816, Loss 54521061030.00, Val loss 28107481088.00\n","Epoch 817, Loss 54481080522.00, Val loss 28174385152.00\n","Epoch 818, Loss 54547708630.00, Val loss 28139591680.00\n","Epoch 819, Loss 54520911162.00, Val loss 28185247744.00\n","Epoch 820, Loss 54565664268.00, Val loss 28162447360.00\n","Epoch 821, Loss 54545531830.00, Val loss 28199370752.00\n","Epoch 822, Loss 54584910640.00, Val loss 28193589248.00\n","Epoch 823, Loss 54583759494.00, Val loss 28209598464.00\n","Epoch 824, Loss 54601348024.00, Val loss 28211939328.00\n","Epoch 825, Loss 54607256932.00, Val loss 28225503232.00\n","Epoch 826, Loss 54622417484.00, Val loss 28229365760.00\n","Epoch 827, Loss 54617543702.00, Val loss 28266000384.00\n","Epoch 828, Loss 54664112438.00, Val loss 28244682752.00\n","Epoch 829, Loss 54649956222.00, Val loss 28261015552.00\n","Epoch 830, Loss 54668022092.00, Val loss 28264151040.00\n","Epoch 831, Loss 54674098654.00, Val loss 28275941376.00\n","Epoch 832, Loss 54687713262.00, Val loss 28277690368.00\n","Epoch 833, Loss 54692779414.00, Val loss 28291811328.00\n","Epoch 834, Loss 54708663080.00, Val loss 28296243200.00\n","Epoch 835, Loss 54713093288.00, Val loss 28313149440.00\n","Epoch 836, Loss 54733826540.00, Val loss 28323033088.00\n","Epoch 837, Loss 54746058008.00, Val loss 28308338688.00\n","Epoch 838, Loss 54736476422.00, Val loss 28339357696.00\n","Epoch 839, Loss 54767030364.00, Val loss 28321789952.00\n","Epoch 840, Loss 54754464540.00, Val loss 28349790208.00\n","Epoch 841, Loss 54774974118.00, Val loss 28350025728.00\n","Epoch 842, Loss 54785701014.00, Val loss 28370046976.00\n","Epoch 843, Loss 54806761798.00, Val loss 28347811840.00\n","Epoch 844, Loss 54789728288.00, Val loss 28377391104.00\n","Epoch 845, Loss 54819420508.00, Val loss 28362004480.00\n","Epoch 846, Loss 54805594994.00, Val loss 28392407040.00\n","Epoch 847, Loss 54838933704.00, Val loss 28391761920.00\n","Epoch 848, Loss 54840428026.00, Val loss 28380035072.00\n","Epoch 849, Loss 54832588534.00, Val loss 28403124224.00\n","Epoch 850, Loss 54855567938.00, Val loss 28395067392.00\n","Epoch 851, Loss 54852652086.00, Val loss 28422598656.00\n","Epoch 852, Loss 54879088858.00, Val loss 28397383680.00\n","Epoch 853, Loss 54846095512.00, Val loss 28459149312.00\n","Epoch 854, Loss 54915466130.00, Val loss 28411598848.00\n","Epoch 855, Loss 54877796832.00, Val loss 28443672576.00\n","Epoch 856, Loss 54908184382.00, Val loss 28418162688.00\n","Epoch 857, Loss 54887379510.00, Val loss 28442501120.00\n","Epoch 858, Loss 54912457006.00, Val loss 28439097344.00\n","Epoch 859, Loss 54914176530.00, Val loss 28444872704.00\n","Epoch 860, Loss 54915175964.00, Val loss 28458242048.00\n","Epoch 861, Loss 54936257442.00, Val loss 28462528512.00\n","Epoch 862, Loss 54941931996.00, Val loss 28451299328.00\n","Epoch 863, Loss 54929506640.00, Val loss 28477202432.00\n","Epoch 864, Loss 54960415026.00, Val loss 28464306176.00\n","Epoch 865, Loss 54946487550.00, Val loss 28476962816.00\n","Epoch 866, Loss 54964912930.00, Val loss 28482695168.00\n","Epoch 867, Loss 54968152954.00, Val loss 28470487040.00\n","Epoch 868, Loss 54962775064.00, Val loss 28507392000.00\n","Epoch 869, Loss 54997608052.00, Val loss 28460468224.00\n","Epoch 870, Loss 54958149560.00, Val loss 28516745216.00\n","Epoch 871, Loss 55010486624.00, Val loss 28467077120.00\n","Epoch 872, Loss 54968441560.00, Val loss 28519274496.00\n","Epoch 873, Loss 55008426918.00, Val loss 28487694336.00\n","Epoch 874, Loss 54990668300.00, Val loss 28531382272.00\n","Epoch 875, Loss 55031791656.00, Val loss 28481333248.00\n","Epoch 876, Loss 54988970670.00, Val loss 28527300608.00\n","Epoch 877, Loss 55029442700.00, Val loss 28494706688.00\n","Epoch 878, Loss 55004542660.00, Val loss 28546150400.00\n","Epoch 879, Loss 55053490302.00, Val loss 28486387712.00\n","Epoch 880, Loss 55000658776.00, Val loss 28543047680.00\n","Epoch 881, Loss 55054118402.00, Val loss 28501155840.00\n","Epoch 882, Loss 55010787534.00, Val loss 28558655488.00\n","Epoch 883, Loss 55069066076.00, Val loss 28510271488.00\n","Epoch 884, Loss 55029380322.00, Val loss 28549619712.00\n","Epoch 885, Loss 55062769732.00, Val loss 28514465792.00\n","Epoch 886, Loss 55036512152.00, Val loss 28555243520.00\n","Epoch 887, Loss 55072132476.00, Val loss 28515454976.00\n","Epoch 888, Loss 55040558178.00, Val loss 28565016576.00\n","Epoch 889, Loss 55086894068.00, Val loss 28509980672.00\n","Epoch 890, Loss 55034431914.00, Val loss 28569696256.00\n","Epoch 891, Loss 55080225990.00, Val loss 28550895616.00\n","Epoch 892, Loss 55075411866.00, Val loss 28535826432.00\n","Epoch 893, Loss 55066618666.00, Val loss 28562448384.00\n","Epoch 894, Loss 55079360858.00, Val loss 28555816960.00\n","Epoch 895, Loss 55081660566.00, Val loss 28553232384.00\n","Epoch 896, Loss 55086857242.00, Val loss 28567453696.00\n","Epoch 897, Loss 55100749236.00, Val loss 28532965376.00\n","Epoch 898, Loss 55070506602.00, Val loss 28568875008.00\n","Epoch 899, Loss 55104521982.00, Val loss 28535474176.00\n","Epoch 900, Loss 55066837510.00, Val loss 28579926016.00\n","Epoch 901, Loss 55117529504.00, Val loss 28547416064.00\n","Epoch 902, Loss 55085907500.00, Val loss 28575244288.00\n","Epoch 903, Loss 55112436312.00, Val loss 28544745472.00\n","Epoch 904, Loss 55087607158.00, Val loss 28575244288.00\n","Epoch 905, Loss 55114942752.00, Val loss 28544077824.00\n","Epoch 906, Loss 55089310460.00, Val loss 28582291456.00\n","Epoch 907, Loss 55126471978.00, Val loss 28547342336.00\n","Epoch 908, Loss 55096090284.00, Val loss 28574296064.00\n","Epoch 909, Loss 55120692504.00, Val loss 28546021376.00\n","Epoch 910, Loss 55093940184.00, Val loss 28576202752.00\n","Epoch 911, Loss 55124724480.00, Val loss 28554694656.00\n","Epoch 912, Loss 55103323416.00, Val loss 28568961024.00\n","Epoch 913, Loss 55118803438.00, Val loss 28559124480.00\n","Epoch 914, Loss 55111143456.00, Val loss 28568289280.00\n","Epoch 915, Loss 55116163734.00, Val loss 28562333696.00\n","Epoch 916, Loss 55107663302.00, Val loss 28597434368.00\n","Epoch 917, Loss 55126595448.00, Val loss 28648849408.00\n","Epoch 918, Loss 55183632694.00, Val loss 28577517568.00\n","Epoch 919, Loss 55119957748.00, Val loss 28650612736.00\n","Epoch 920, Loss 55187786296.00, Val loss 28562296832.00\n","Epoch 921, Loss 55108477250.00, Val loss 28681992192.00\n","Epoch 922, Loss 55218636934.00, Val loss 28565977088.00\n","Epoch 923, Loss 55112275454.00, Val loss 28663433216.00\n","Epoch 924, Loss 55202551936.00, Val loss 28582203392.00\n","Epoch 925, Loss 55128956274.00, Val loss 28654764032.00\n","Epoch 926, Loss 55191970900.00, Val loss 28594558976.00\n","Epoch 927, Loss 55141212834.00, Val loss 28641374208.00\n","Epoch 928, Loss 55185408330.00, Val loss 28597676032.00\n","Epoch 929, Loss 55146459914.00, Val loss 28635224064.00\n","Epoch 930, Loss 55181759288.00, Val loss 28600041472.00\n","Epoch 931, Loss 55150830880.00, Val loss 28631556096.00\n","Epoch 932, Loss 55179803978.00, Val loss 28600543232.00\n","Epoch 933, Loss 55152447902.00, Val loss 28626780160.00\n","Epoch 934, Loss 55177417050.00, Val loss 28601511936.00\n","Epoch 935, Loss 55155032604.00, Val loss 28623263744.00\n","Epoch 936, Loss 55175519198.00, Val loss 28600711168.00\n","Epoch 937, Loss 55155762346.00, Val loss 28619411456.00\n","Epoch 938, Loss 55172019074.00, Val loss 28597415936.00\n","Epoch 939, Loss 55152388830.00, Val loss 28617707520.00\n","Epoch 940, Loss 55171013616.00, Val loss 28596117504.00\n","Epoch 941, Loss 55151595854.00, Val loss 28614541312.00\n","Epoch 942, Loss 55167957282.00, Val loss 28596787200.00\n","Epoch 943, Loss 55152457228.00, Val loss 28616456192.00\n","Epoch 944, Loss 55170731614.00, Val loss 28595451904.00\n","Epoch 945, Loss 55151509114.00, Val loss 28611743744.00\n","Epoch 946, Loss 55166016364.00, Val loss 28597575680.00\n","Epoch 947, Loss 55153650240.00, Val loss 28611684352.00\n","Epoch 948, Loss 55166483482.00, Val loss 28596598784.00\n","Epoch 949, Loss 55153226826.00, Val loss 28612460544.00\n","Epoch 950, Loss 55154439084.00, Val loss 28623947776.00\n","Epoch 951, Loss 55177649250.00, Val loss 28582746112.00\n","Epoch 952, Loss 55141263250.00, Val loss 28620871680.00\n","Epoch 953, Loss 55176219528.00, Val loss 28581197824.00\n","Epoch 954, Loss 55140090126.00, Val loss 28614778880.00\n","Epoch 955, Loss 55170856238.00, Val loss 28583071744.00\n","Epoch 956, Loss 55142117542.00, Val loss 28611170304.00\n","Epoch 957, Loss 55167966432.00, Val loss 28588058624.00\n","Epoch 958, Loss 55147159502.00, Val loss 28604164096.00\n","Epoch 959, Loss 55161231526.00, Val loss 28585773056.00\n","Epoch 960, Loss 55145196150.00, Val loss 28605726720.00\n","Epoch 961, Loss 55162872054.00, Val loss 28583944192.00\n","Epoch 962, Loss 55143924584.00, Val loss 28604268544.00\n","Epoch 963, Loss 55162644886.00, Val loss 28578398208.00\n","Epoch 964, Loss 55138245002.00, Val loss 28602255360.00\n","Epoch 965, Loss 55160076014.00, Val loss 28577304576.00\n","Epoch 966, Loss 55136574212.00, Val loss 28601231360.00\n","Epoch 967, Loss 55159133918.00, Val loss 28574685184.00\n","Epoch 968, Loss 55135200212.00, Val loss 28596934656.00\n","Epoch 969, Loss 55154888800.00, Val loss 28571467776.00\n","Epoch 970, Loss 55132662928.00, Val loss 28598448128.00\n","Epoch 971, Loss 55156642046.00, Val loss 28566063104.00\n","Epoch 972, Loss 55126589042.00, Val loss 28594399232.00\n","Epoch 973, Loss 55153199098.00, Val loss 28571111424.00\n","Epoch 974, Loss 55131402342.00, Val loss 28585369600.00\n","Epoch 975, Loss 55143439186.00, Val loss 28572313600.00\n","Epoch 976, Loss 55132237054.00, Val loss 28586606592.00\n","Epoch 977, Loss 55144957076.00, Val loss 28568502272.00\n","Epoch 978, Loss 55127747028.00, Val loss 28587319296.00\n","Epoch 979, Loss 55144637614.00, Val loss 28560373760.00\n","Epoch 980, Loss 55120746522.00, Val loss 28588627968.00\n","Epoch 981, Loss 55146210642.00, Val loss 28559884288.00\n","Epoch 982, Loss 55119485710.00, Val loss 28579477504.00\n","Epoch 983, Loss 55137179938.00, Val loss 28562446336.00\n","Epoch 984, Loss 55121816602.00, Val loss 28575584256.00\n","Epoch 985, Loss 55133332244.00, Val loss 28558338048.00\n","Epoch 986, Loss 55117000886.00, Val loss 28573306880.00\n","Epoch 987, Loss 55130734982.00, Val loss 28558870528.00\n","Epoch 988, Loss 55116931106.00, Val loss 28569186304.00\n","Epoch 989, Loss 55126570288.00, Val loss 28560797696.00\n","Epoch 990, Loss 55118460936.00, Val loss 28562382848.00\n","Epoch 991, Loss 55119214392.00, Val loss 28555939840.00\n","Epoch 992, Loss 55101973268.00, Val loss 28590178304.00\n","Epoch 993, Loss 55143783880.00, Val loss 28530900992.00\n","Epoch 994, Loss 55089813590.00, Val loss 28583499776.00\n","Epoch 995, Loss 55136961546.00, Val loss 28532226048.00\n","Epoch 996, Loss 55090342454.00, Val loss 28578412544.00\n","Epoch 997, Loss 55131114402.00, Val loss 28532326400.00\n","Epoch 998, Loss 55089914800.00, Val loss 28575629312.00\n","Epoch 999, Loss 55128288078.00, Val loss 28529526784.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"status":"ok","timestamp":1646783841709,"user_tz":360,"elapsed":407,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"7b170e92-2cd3-427d-9010-745294361357"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["38665711616.0\n","tensor([313560.9375, 314537.4688, 315523.5938, 316434.6875, 317103.8750,\n","        317323.8125, 317741.5000, 319034.7500, 320533.1875, 321536.0625,\n","        321875.0938, 322897.8438, 324330.6562, 325789.6250, 327098.4062],\n","       grad_fn=<SelectBackward0>)\n","tensor([292187., 293697., 293697., 293697., 293697., 293697., 295701., 296870.,\n","        297729., 297729., 297729., 298362., 298626., 298808., 298993.])\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","    # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_cumulative_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_cumulative_infected_tensor':labeled_output # (52, 15)\n","}\n","\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)"],"metadata":{"id":"s3-8Yge6CAWM","executionInfo":{"status":"ok","timestamp":1646783878808,"user_tz":360,"elapsed":256,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["mean_squared_error = criterion(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","mae_function = torch.nn.L1Loss()\n","mean_absolute_error = mae_function(predicted_output.reshape((52, 15)), labeled_output.reshape((52, 15))).item()\n","print(\"mean squared error: \", mean_squared_error)\n","print(\"mean absolute error: \", mean_absolute_error)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddc7iLV-7OJb","executionInfo":{"status":"ok","timestamp":1646783882343,"user_tz":360,"elapsed":313,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"6aa9fd42-033d-4ea0-b4dd-3a2865817f81"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["mean squared error:  38665711616.0\n","mean absolute error:  128769.2890625\n"]}]}]}