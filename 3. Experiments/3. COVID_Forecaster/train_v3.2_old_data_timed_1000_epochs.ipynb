{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.2_timed_1000_epochs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOEEICzUvdGBjuNMsvwEml5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Mount Google Colab Notebook correctly so we are in the correct relative directory\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"3. COVID_Forecaster\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4C3p0DN1NAtF","executionInfo":{"status":"ok","timestamp":1646754602100,"user_tz":360,"elapsed":17424,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"2b2dc950-0626-46eb-89d7-b8af0339db9c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/3. COVID_Forecaster\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5eyEWD8uaydI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"94d2c560-f9d4-4167-cad5-bfea7d97354f","executionInfo":{"status":"ok","timestamp":1646754647766,"user_tz":360,"elapsed":45681,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 8.0 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.12\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 7.5 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.5.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (747 kB)\n","\u001b[K     |████████████████████████████████| 747 kB 1.7 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.3.tar.gz (370 kB)\n","\u001b[K     |████████████████████████████████| 370 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Collecting rdflib\n","  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n","\u001b[K     |████████████████████████████████| 482 kB 42.4 MB/s \n","\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (4.11.2)\n","Collecting isodate\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 615 kB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch-geometric) (3.10.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.3-py3-none-any.whl size=581968 sha256=9c37e6dfa9df444c6b8e95940771281b6efe99ac02eec03690f7d5514dc7cf03\n","  Stored in directory: /root/.cache/pip/wheels/c3/2a/58/87ce0508964d4def1aafb92750c4f3ac77038efd1b9a89dcf5\n","Successfully built torch-geometric\n","Installing collected packages: isodate, yacs, rdflib, torch-geometric\n","Successfully installed isodate-0.6.1 rdflib-6.1.1 torch-geometric-2.0.3 yacs-0.1.8\n","Collecting torch-geometric-temporal\n","  Downloading torch_geometric_temporal-0.51.0.tar.gz (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.4.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.10.0+cu111)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.29.28)\n","Requirement already satisfied: pandas<=1.3.5 in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.3.5)\n","Requirement already satisfied: torch_sparse in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (0.6.12)\n","Requirement already satisfied: torch_scatter in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.9)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (2.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (4.63.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torch-geometric-temporal) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2018.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-geometric-temporal) (3.10.0.2)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.4)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.11.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (1.0.2)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (0.1.8)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (6.1.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (3.0.7)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric->torch-geometric-temporal) (2.23.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric->torch-geometric-temporal) (2.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (57.4.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (0.6.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric->torch-geometric-temporal) (4.11.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib->torch_geometric->torch-geometric-temporal) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.0.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.1.0)\n","Building wheels for collected packages: torch-geometric-temporal\n","  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.51.0-py3-none-any.whl size=83569 sha256=723983340947ed4ce042e418ce35500c024bd9f74da0c0771f49d22ed1f80107\n","  Stored in directory: /root/.cache/pip/wheels/a5/26/64/465700aa43b21fccca9ae446b407de2389f0ba16114e84db8d\n","Successfully built torch-geometric-temporal\n","Installing collected packages: torch-geometric-temporal\n","Successfully installed torch-geometric-temporal-0.51.0\n","Collecting ogb\n","  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.63.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n","Collecting outdated>=0.2.0\n","  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.5)\n","Collecting littleutils\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=5c6736fb8481bedcf6469ce757d493316851a227cc4c9a04307439f4bcb9a6bc\n","  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n","PyTorch has version 1.10.0+cu111\n","Collecting epiweeks\n","  Downloading epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n","Installing collected packages: epiweeks\n","Successfully installed epiweeks-2.1.4\n","Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n"]}],"source":["###################################################################################################################################\n","\"\"\"\n","Ensure we have correct version of Pytorch Geometric before importing or installing anything else\n","\"\"\"\n","# Code in this cell is courtesy of: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7 \n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","import os\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric \n","!pip install torch-geometric-temporal # added for GConvGRU\n","!pip install ogb\n","\n","print(\"PyTorch has version {}\".format(torch.__version__))\n","\n","########################################################################################################################################\n","\n","\"\"\"\n","Download and Import any other libraries we will use later on\n","\"\"\"\n","\n","! pip install epiweeks\n","! pip install haversine\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","\n","from torch_geometric.data import Data\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv \n","from torch_geometric_temporal.nn.recurrent import GConvGRU\n","# from preprocess_data import get_preprocessed_data\n","\n","##############################################################################################################################################\n"]},{"cell_type":"code","source":["import time\n","time_start = time.time()"],"metadata":{"id":"t0Y3iC2eSF-e","executionInfo":{"status":"ok","timestamp":1646754647767,"user_tz":360,"elapsed":27,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables\n","\"\"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","inputLayer_num_features = 36 \n","hiddenLayer1_num_features = 24\n","hiddenLayer2_num_features = 24\n","outputLayer_num_features = 15 # pred_window, redeclared from preprocessing_data\n","chebyshev_filter_size = 2\n","history_window = 6 # history_window, redeclared from preprocessing_data \n","preprocessed_data_relative_file_path = 'data/preprocessed_data_v3_reorder_input_active_cases.pickle'\n","save_model_relative_path = './saved_models/v3_2_reorder_skip_and_active_1000_epochs_timed'\n","numEpochs = 1000\n","save_predictions_relative_path = './saved_models/v3_2_archived_output_1000_epochs_timed.pickle'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zXf082K6UCy","executionInfo":{"status":"ok","timestamp":1646754647900,"user_tz":360,"elapsed":156,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"7051897c-f3ef-48ec-f0e2-0ea69e9c77b8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Get and unpack preprocessed data\n","\"\"\"\n","\n","# # Get preprocessed data and unpackage variables needed during training/testing/validation\n","# preprocessed_data = get_preprocessed_data()\n","# # Save preprocessed data as pickle so we don't have to consecutively re-download the same data (time consuming) when re-running all cells in this notebook\n","# with open(preprocessed_data_relative_file_path, 'wb') as handle:\n","#     pickle.dump(preprocessed_data, handle)\n","\n","\n","# Load in saved preprocessed_data\n","preprocessed_data = None\n","with open(preprocessed_data_relative_file_path, 'rb') as handle:\n","    preprocessed_data = pickle.load(handle)\n","\n","# Unpack preprocessed data\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","edge_index = preprocessed_data['edge_index']\n","\n","train_x = training_variables['train_x_smoothed']\n","train_y = training_variables['train_y_active_smoothed']\n","\n","val_x = validation_variables['val_x_smoothed']\n","val_y = validation_variables['val_y_active_unsmoothed']\n","\n","test_x = testing_variables['test_x_smoothed']\n","test_y = testing_variables['test_y_active_unsmoothed']"],"metadata":{"id":"oWdP4J6ONPLU","executionInfo":{"status":"ok","timestamp":1646754648485,"user_tz":360,"elapsed":590,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put preprocessed data into \"Data\" objects and put \"Data\" objects into tensors on the \"device\"\n","\"\"\"\n","training_data = []\n","validation_data = []\n","testing_data = []\n","for i in range(len(train_x)):\n","  toAppend = Data(x=train_x[i], y=train_y[i], edge_index=edge_index).to(device)\n","  training_data.append(toAppend)\n","for i in range(len(val_x)):\n","  toAppend = Data(x=val_x[i], y=val_y[i], edge_index=edge_index).to(device)\n","  validation_data.append(toAppend)\n","for i in range(len(test_x)):\n","  toAppend = Data(x=test_x[i], y=test_y[i], edge_index=edge_index).to(device)\n","  testing_data.append(toAppend)"],"metadata":{"id":"dM5W6OFmPuI3","executionInfo":{"status":"ok","timestamp":1646754648486,"user_tz":360,"elapsed":7,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Define network architecture, optimizer, and loss function\n","Reference: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html \n","\"\"\"\n","\n","class GCN(torch.nn.Module):\n","    # def __init__(self):\n","    #     super().__init__()\n","    #     self.conv1 = GCNConv(inputLayer_num_features, hiddenLayer1_num_features)\n","    #     self.conv2 = GCNConv(hiddenLayer1_num_features, outputLayer_num_features)\n","\n","    def __init__(self):\n","      super().__init__()\n","      self.conv1 = GConvGRU(in_channels=inputLayer_num_features, out_channels=hiddenLayer1_num_features, K=chebyshev_filter_size)\n","      self.conv2 = GConvGRU(in_channels=hiddenLayer1_num_features, out_channels=hiddenLayer2_num_features, K=chebyshev_filter_size)\n","      self.linear1 = Linear(hiddenLayer2_num_features, outputLayer_num_features)\n","      self.linear2 = Linear(history_window, outputLayer_num_features)\n","      self.linear3 = Linear(outputLayer_num_features, outputLayer_num_features)\n","\n","    def forward(self, data):\n","      x, edge_index = data.x, data.edge_index\n","      x = self.conv1(x, edge_index)\n","      x = F.elu(x)\n","      x = self.conv2(x, edge_index)\n","      x = F.elu(x)\n","      x = self.linear1(x) + self.linear2(data.x[:, 0:history_window])\n","\n","      x = F.elu(x)\n","      x = self.linear3(x)\n","\n","      return x\n","\n","model = GCN().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","criterion = torch.nn.MSELoss()"],"metadata":{"id":"arZhwUZBZ0Zj","executionInfo":{"status":"ok","timestamp":1646754648487,"user_tz":360,"elapsed":7,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"ZFKTgiyDqskX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646754648596,"user_tz":360,"elapsed":116,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"05917f7d-d23b-491e-9fe8-940de987b5d4"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GCN(\n","  (conv1): GConvGRU(\n","    (conv_x_z): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(36, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (conv2): GConvGRU(\n","    (conv_x_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_z): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_r): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_x_h): ChebConv(24, 24, K=2, normalization=sym)\n","    (conv_h_h): ChebConv(24, 24, K=2, normalization=sym)\n","  )\n","  (linear1): Linear(in_features=24, out_features=15, bias=True)\n","  (linear2): Linear(in_features=6, out_features=15, bias=True)\n","  (linear3): Linear(in_features=15, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["\"\"\"\n","Training\n","\"\"\"\n","min_loss = 1e20\n","\n","def train():\n","  total_loss = 0.0\n","  for data in training_data:\n","    model.train()\n","    optimizer.zero_grad()\n","    predicted_output = model(data)\n","    labeled_output = data.y.to(device)\n","    loss = criterion(predicted_output, labeled_output)\n","    loss.backward()\n","    total_loss += loss.item()\n","    optimizer.step()\n","  return total_loss\n","\n","def validation():\n","  model.eval()\n","  totalLoss = 0.0\n","  for data_validation in validation_data:\n","    validation_prediction_output = model(data_validation)\n","    labeled_validation_output = data_validation.y.to(device)\n","    loss = criterion(validation_prediction_output, labeled_validation_output)\n","    totalLoss += loss.item()\n","  return totalLoss\n","\n","for epoch in range(numEpochs):\n","  training_loss = train()\n","  val_loss = validation()\n","  if val_loss < min_loss:\n","    min_loss = val_loss\n","    state = {\n","        'state': model.state_dict(),\n","        'optimizer': optimizer.state_dict()\n","    }\n","    torch.save(state, save_model_relative_path)\n","    print(\"==================================================================\")\n","    print(\"Saved best model\")\n","  print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, training_loss, val_loss))"],"metadata":{"id":"vHIlH0VdVKdK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646754992845,"user_tz":360,"elapsed":344254,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"3c161b51-ca07-43fd-89dc-5deb0223f174"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================================\n","Saved best model\n","Epoch 0, Loss 24902800278.00, Val loss 196377024.00\n","==================================================================\n","Saved best model\n","Epoch 1, Loss 1755771014.00, Val loss 81956992.00\n","==================================================================\n","Saved best model\n","Epoch 2, Loss 865720121.00, Val loss 59362140.00\n","==================================================================\n","Saved best model\n","Epoch 3, Loss 818720700.00, Val loss 54691400.00\n","Epoch 4, Loss 815864263.00, Val loss 56262372.00\n","==================================================================\n","Saved best model\n","Epoch 5, Loss 812462576.00, Val loss 54298016.00\n","==================================================================\n","Saved best model\n","Epoch 6, Loss 820676069.00, Val loss 51858152.00\n","==================================================================\n","Saved best model\n","Epoch 7, Loss 832379217.00, Val loss 49701212.00\n","==================================================================\n","Saved best model\n","Epoch 8, Loss 837847508.00, Val loss 47367720.00\n","==================================================================\n","Saved best model\n","Epoch 9, Loss 843760047.00, Val loss 45461028.00\n","==================================================================\n","Saved best model\n","Epoch 10, Loss 846811487.00, Val loss 43979068.00\n","==================================================================\n","Saved best model\n","Epoch 11, Loss 846240063.00, Val loss 42764964.00\n","==================================================================\n","Saved best model\n","Epoch 12, Loss 842584663.00, Val loss 41698108.00\n","==================================================================\n","Saved best model\n","Epoch 13, Loss 836691822.00, Val loss 40710920.00\n","==================================================================\n","Saved best model\n","Epoch 14, Loss 828714526.00, Val loss 39835612.00\n","==================================================================\n","Saved best model\n","Epoch 15, Loss 819153039.00, Val loss 39053776.00\n","==================================================================\n","Saved best model\n","Epoch 16, Loss 807892151.00, Val loss 38382940.00\n","==================================================================\n","Saved best model\n","Epoch 17, Loss 795049344.00, Val loss 37807092.00\n","==================================================================\n","Saved best model\n","Epoch 18, Loss 780710089.00, Val loss 37294712.00\n","==================================================================\n","Saved best model\n","Epoch 19, Loss 764905655.00, Val loss 36805980.00\n","==================================================================\n","Saved best model\n","Epoch 20, Loss 747678387.50, Val loss 36309064.00\n","==================================================================\n","Saved best model\n","Epoch 21, Loss 729103956.00, Val loss 35771980.00\n","==================================================================\n","Saved best model\n","Epoch 22, Loss 709296272.50, Val loss 35162336.00\n","==================================================================\n","Saved best model\n","Epoch 23, Loss 688447201.50, Val loss 34472336.00\n","==================================================================\n","Saved best model\n","Epoch 24, Loss 666721534.50, Val loss 33710400.00\n","==================================================================\n","Saved best model\n","Epoch 25, Loss 644415644.50, Val loss 32931254.00\n","==================================================================\n","Saved best model\n","Epoch 26, Loss 621802263.50, Val loss 32257550.00\n","==================================================================\n","Saved best model\n","Epoch 27, Loss 599049296.50, Val loss 31887612.00\n","Epoch 28, Loss 577133327.00, Val loss 32028822.00\n","Epoch 29, Loss 555853666.00, Val loss 32780702.00\n","Epoch 30, Loss 535720281.00, Val loss 34116040.00\n","Epoch 31, Loss 516015742.00, Val loss 35858412.00\n","Epoch 32, Loss 497670821.00, Val loss 37607892.00\n","Epoch 33, Loss 480827483.50, Val loss 39177516.00\n","Epoch 34, Loss 465415786.00, Val loss 40364096.00\n","Epoch 35, Loss 452722053.00, Val loss 41076116.00\n","Epoch 36, Loss 442341138.50, Val loss 41270040.00\n","Epoch 37, Loss 434604527.00, Val loss 40862376.00\n","Epoch 38, Loss 429569258.25, Val loss 39731660.00\n","Epoch 39, Loss 428140516.00, Val loss 37852412.00\n","Epoch 40, Loss 427752577.75, Val loss 35383200.00\n","Epoch 41, Loss 428084948.00, Val loss 33035172.00\n","==================================================================\n","Saved best model\n","Epoch 42, Loss 428016068.00, Val loss 31078482.00\n","==================================================================\n","Saved best model\n","Epoch 43, Loss 427447804.00, Val loss 29546280.00\n","==================================================================\n","Saved best model\n","Epoch 44, Loss 425491874.75, Val loss 28220484.00\n","==================================================================\n","Saved best model\n","Epoch 45, Loss 422985229.75, Val loss 27119590.00\n","==================================================================\n","Saved best model\n","Epoch 46, Loss 419208683.75, Val loss 26225658.00\n","==================================================================\n","Saved best model\n","Epoch 47, Loss 415673238.25, Val loss 25243886.00\n","==================================================================\n","Saved best model\n","Epoch 48, Loss 411055143.50, Val loss 24768274.00\n","==================================================================\n","Saved best model\n","Epoch 49, Loss 406235966.50, Val loss 24236194.00\n","==================================================================\n","Saved best model\n","Epoch 50, Loss 402281675.75, Val loss 24086770.00\n","==================================================================\n","Saved best model\n","Epoch 51, Loss 400222863.00, Val loss 23853528.00\n","==================================================================\n","Saved best model\n","Epoch 52, Loss 399049171.75, Val loss 23851706.00\n","==================================================================\n","Saved best model\n","Epoch 53, Loss 399994956.75, Val loss 23769678.00\n","==================================================================\n","Saved best model\n","Epoch 54, Loss 401724225.50, Val loss 23728596.00\n","==================================================================\n","Saved best model\n","Epoch 55, Loss 403101378.00, Val loss 23717684.00\n","==================================================================\n","Saved best model\n","Epoch 56, Loss 404097628.25, Val loss 23697266.00\n","Epoch 57, Loss 404477593.00, Val loss 23699278.00\n","Epoch 58, Loss 404489253.25, Val loss 23728298.00\n","==================================================================\n","Saved best model\n","Epoch 59, Loss 404282918.50, Val loss 23649530.00\n","==================================================================\n","Saved best model\n","Epoch 60, Loss 402903802.50, Val loss 23643556.00\n","==================================================================\n","Saved best model\n","Epoch 61, Loss 401587166.25, Val loss 23566044.00\n","Epoch 62, Loss 399940141.50, Val loss 23567738.00\n","Epoch 63, Loss 398305380.75, Val loss 23567588.00\n","==================================================================\n","Saved best model\n","Epoch 64, Loss 397308715.75, Val loss 23562802.00\n","==================================================================\n","Saved best model\n","Epoch 65, Loss 395971706.00, Val loss 23533848.00\n","Epoch 66, Loss 394454905.50, Val loss 23537498.00\n","==================================================================\n","Saved best model\n","Epoch 67, Loss 393233447.25, Val loss 23518474.00\n","==================================================================\n","Saved best model\n","Epoch 68, Loss 391706038.75, Val loss 23509338.00\n","Epoch 69, Loss 390073009.25, Val loss 23513362.00\n","Epoch 70, Loss 388508859.50, Val loss 23546026.00\n","==================================================================\n","Saved best model\n","Epoch 71, Loss 387235662.75, Val loss 23507262.00\n","Epoch 72, Loss 385362725.50, Val loss 23530888.00\n","Epoch 73, Loss 383772397.50, Val loss 23542152.00\n","Epoch 74, Loss 382111803.75, Val loss 23545510.00\n","Epoch 75, Loss 380219709.25, Val loss 23567488.00\n","Epoch 76, Loss 378430343.50, Val loss 23596326.00\n","Epoch 77, Loss 376636245.00, Val loss 23623924.00\n","Epoch 78, Loss 374965452.50, Val loss 23643874.00\n","Epoch 79, Loss 373126901.50, Val loss 23660418.00\n","Epoch 80, Loss 3563992520.00, Val loss 31372516.00\n","Epoch 81, Loss 909805854.00, Val loss 32256336.00\n","Epoch 82, Loss 524072007.00, Val loss 37763680.00\n","Epoch 83, Loss 484522532.50, Val loss 36073024.00\n","Epoch 84, Loss 468556774.00, Val loss 37162532.00\n","Epoch 85, Loss 1333226099.50, Val loss 29909854.00\n","Epoch 86, Loss 538115860.00, Val loss 33087742.00\n","Epoch 87, Loss 461760396.50, Val loss 37815652.00\n","Epoch 88, Loss 439336482.00, Val loss 36714072.00\n","Epoch 89, Loss 430786387.00, Val loss 37179580.00\n","Epoch 90, Loss 422339446.00, Val loss 37046036.00\n","Epoch 91, Loss 415866172.00, Val loss 36811756.00\n","Epoch 92, Loss 410775293.50, Val loss 36421024.00\n","Epoch 93, Loss 406762679.50, Val loss 35957352.00\n","Epoch 94, Loss 405496851.25, Val loss 35399784.00\n","Epoch 95, Loss 4881723622.00, Val loss 50874692.00\n","Epoch 96, Loss 621758689.50, Val loss 29959404.00\n","Epoch 97, Loss 528537542.00, Val loss 29304942.00\n","Epoch 98, Loss 499139916.75, Val loss 29216278.00\n","Epoch 99, Loss 485797061.00, Val loss 29573454.00\n","Epoch 100, Loss 476597803.50, Val loss 29907146.00\n","Epoch 101, Loss 468983589.00, Val loss 30337680.00\n","Epoch 102, Loss 461840026.50, Val loss 30865234.00\n","Epoch 103, Loss 454966986.50, Val loss 31427254.00\n","Epoch 104, Loss 448243615.50, Val loss 31982876.00\n","Epoch 105, Loss 441723305.00, Val loss 32514844.00\n","Epoch 106, Loss 435451637.00, Val loss 32995422.00\n","Epoch 107, Loss 429449145.00, Val loss 33417578.00\n","Epoch 108, Loss 423745490.50, Val loss 33759400.00\n","Epoch 109, Loss 418461339.00, Val loss 34033288.00\n","Epoch 110, Loss 413316142.50, Val loss 34218888.00\n","Epoch 111, Loss 408640392.00, Val loss 34341748.00\n","Epoch 112, Loss 404322072.00, Val loss 34391912.00\n","Epoch 113, Loss 400363359.00, Val loss 34374812.00\n","Epoch 114, Loss 396656548.50, Val loss 34304296.00\n","Epoch 115, Loss 393180767.25, Val loss 34193680.00\n","Epoch 116, Loss 390908335.00, Val loss 33985404.00\n","Epoch 117, Loss 387478882.00, Val loss 33889884.00\n","Epoch 118, Loss 385994849.00, Val loss 33524856.00\n","Epoch 119, Loss 382771277.25, Val loss 33481534.00\n","Epoch 120, Loss 381722167.25, Val loss 32969476.00\n","Epoch 121, Loss 378876033.75, Val loss 33007060.00\n","Epoch 122, Loss 377876024.50, Val loss 32384882.00\n","Epoch 123, Loss 375645549.25, Val loss 32427752.00\n","Epoch 124, Loss 377815272.25, Val loss 31621902.00\n","Epoch 125, Loss 373940417.25, Val loss 31904116.00\n","Epoch 126, Loss 375322445.50, Val loss 31008354.00\n","Epoch 127, Loss 371518695.00, Val loss 31178198.00\n","Epoch 128, Loss 373728624.75, Val loss 30365954.00\n","Epoch 129, Loss 369309402.00, Val loss 30345660.00\n","Epoch 130, Loss 375712311.25, Val loss 29651028.00\n","Epoch 131, Loss 368222871.25, Val loss 29465916.00\n","Epoch 132, Loss 376214454.00, Val loss 28982912.00\n","Epoch 133, Loss 367007668.00, Val loss 28349392.00\n","Epoch 134, Loss 376097083.25, Val loss 28224822.00\n","Epoch 135, Loss 364936874.75, Val loss 27143140.00\n","Epoch 136, Loss 375671071.25, Val loss 27428640.00\n","Epoch 137, Loss 362942446.25, Val loss 25908402.00\n","Epoch 138, Loss 375627384.50, Val loss 26604932.00\n","Epoch 139, Loss 360452681.00, Val loss 24725204.00\n","Epoch 140, Loss 375544859.00, Val loss 25826508.00\n","Epoch 141, Loss 358112028.50, Val loss 23627894.00\n","Epoch 142, Loss 368689891.75, Val loss 24861988.00\n","==================================================================\n","Saved best model\n","Epoch 143, Loss 354358827.75, Val loss 22589632.00\n","Epoch 144, Loss 367814435.25, Val loss 24153992.00\n","==================================================================\n","Saved best model\n","Epoch 145, Loss 346809181.00, Val loss 22085504.00\n","Epoch 146, Loss 352003215.75, Val loss 22924096.00\n","==================================================================\n","Saved best model\n","Epoch 147, Loss 339951779.50, Val loss 21286682.00\n","Epoch 148, Loss 349805428.00, Val loss 22514302.00\n","==================================================================\n","Saved best model\n","Epoch 149, Loss 331982934.25, Val loss 20646914.00\n","Epoch 150, Loss 350290357.75, Val loss 22413556.00\n","==================================================================\n","Saved best model\n","Epoch 151, Loss 323197162.00, Val loss 20220406.00\n","Epoch 152, Loss 341696091.25, Val loss 21982776.00\n","==================================================================\n","Saved best model\n","Epoch 153, Loss 313704355.00, Val loss 20008058.00\n","Epoch 154, Loss 344468249.75, Val loss 22001192.00\n","Epoch 155, Loss 305705878.50, Val loss 20008148.00\n","Epoch 156, Loss 327327230.25, Val loss 21216832.00\n","Epoch 157, Loss 306478664.00, Val loss 20553016.00\n","Epoch 158, Loss 292199010.25, Val loss 21015376.00\n","Epoch 159, Loss 308176506.00, Val loss 20510126.00\n","Epoch 160, Loss 288673795.00, Val loss 21575804.00\n","Epoch 161, Loss 298731297.25, Val loss 20794958.00\n","Epoch 162, Loss 288213951.00, Val loss 21733266.00\n","Epoch 163, Loss 291065418.75, Val loss 21658102.00\n","Epoch 164, Loss 274741377.25, Val loss 23403940.00\n","Epoch 165, Loss 286395840.75, Val loss 23080676.00\n","Epoch 166, Loss 279330008.00, Val loss 23756078.00\n","Epoch 167, Loss 281334854.25, Val loss 23938918.00\n","Epoch 168, Loss 279095826.75, Val loss 24178686.00\n","Epoch 169, Loss 273031588.50, Val loss 24577514.00\n","Epoch 170, Loss 276088645.00, Val loss 24486210.00\n","Epoch 171, Loss 266270363.00, Val loss 24817520.00\n","Epoch 172, Loss 274383241.75, Val loss 24377050.00\n","Epoch 173, Loss 256211962.50, Val loss 24937236.00\n","Epoch 174, Loss 270614656.75, Val loss 24453890.00\n","Epoch 175, Loss 247625331.25, Val loss 25119804.00\n","Epoch 176, Loss 258527170.00, Val loss 24966372.00\n","Epoch 177, Loss 247458658.00, Val loss 25092766.00\n","Epoch 178, Loss 242965092.00, Val loss 25214088.00\n","Epoch 179, Loss 242347046.25, Val loss 25177784.00\n","Epoch 180, Loss 247747987.75, Val loss 24970710.00\n","Epoch 181, Loss 233382706.50, Val loss 24867132.00\n","Epoch 182, Loss 223419841.75, Val loss 24486834.00\n","Epoch 183, Loss 236637944.00, Val loss 24528048.00\n","Epoch 184, Loss 224047754.25, Val loss 24169470.00\n","Epoch 185, Loss 230428056.75, Val loss 24116718.00\n","Epoch 186, Loss 218845384.00, Val loss 23675996.00\n","Epoch 187, Loss 212790239.00, Val loss 22657990.00\n","==================================================================\n","Saved best model\n","Epoch 188, Loss 208189463.88, Val loss 19134058.00\n","Epoch 189, Loss 227705061.00, Val loss 20243774.00\n","Epoch 190, Loss 237782012.25, Val loss 22395276.00\n","Epoch 191, Loss 208536056.62, Val loss 22040586.00\n","==================================================================\n","Saved best model\n","Epoch 192, Loss 211039407.75, Val loss 17634488.00\n","Epoch 193, Loss 331114143.75, Val loss 23979270.00\n","==================================================================\n","Saved best model\n","Epoch 194, Loss 443788092.75, Val loss 15701055.00\n","Epoch 195, Loss 226803813.88, Val loss 24355562.00\n","==================================================================\n","Saved best model\n","Epoch 196, Loss 413651279.50, Val loss 14654716.00\n","Epoch 197, Loss 434546168.00, Val loss 28713094.00\n","Epoch 198, Loss 995979058.25, Val loss 31920952.00\n","Epoch 199, Loss 332099929.00, Val loss 21630978.00\n","==================================================================\n","Saved best model\n","Epoch 200, Loss 417897189.75, Val loss 14558933.00\n","Epoch 201, Loss 472900002.00, Val loss 19721918.00\n","Epoch 202, Loss 622572392.75, Val loss 15216128.00\n","Epoch 203, Loss 359206142.00, Val loss 19291722.00\n","Epoch 204, Loss 296321587.50, Val loss 18645364.00\n","Epoch 205, Loss 259704891.00, Val loss 17350680.00\n","Epoch 206, Loss 263880216.75, Val loss 15749006.00\n","Epoch 207, Loss 262547184.38, Val loss 14749717.00\n","==================================================================\n","Saved best model\n","Epoch 208, Loss 256105025.62, Val loss 14258958.00\n","==================================================================\n","Saved best model\n","Epoch 209, Loss 254528679.38, Val loss 13784646.00\n","==================================================================\n","Saved best model\n","Epoch 210, Loss 241979340.25, Val loss 13781119.00\n","==================================================================\n","Saved best model\n","Epoch 211, Loss 238771832.75, Val loss 13471324.00\n","==================================================================\n","Saved best model\n","Epoch 212, Loss 231092951.38, Val loss 13405710.00\n","==================================================================\n","Saved best model\n","Epoch 213, Loss 226084226.00, Val loss 13227082.00\n","Epoch 214, Loss 217966608.62, Val loss 13272681.00\n","Epoch 215, Loss 211515244.38, Val loss 13294605.00\n","Epoch 216, Loss 204027057.50, Val loss 13640122.00\n","Epoch 217, Loss 197306814.12, Val loss 13859042.00\n","Epoch 218, Loss 191471149.88, Val loss 14396549.00\n","Epoch 219, Loss 188079556.62, Val loss 14803562.00\n","Epoch 220, Loss 184600902.50, Val loss 15238653.00\n","Epoch 221, Loss 182756043.62, Val loss 15959320.00\n","Epoch 222, Loss 182339082.38, Val loss 15764408.00\n","Epoch 223, Loss 179753489.12, Val loss 17371734.00\n","Epoch 224, Loss 184350509.50, Val loss 16253189.00\n","Epoch 225, Loss 177291880.62, Val loss 18048286.00\n","Epoch 226, Loss 184144536.00, Val loss 17388338.00\n","Epoch 227, Loss 177482842.88, Val loss 19169452.00\n","Epoch 228, Loss 179768568.00, Val loss 18433190.00\n","Epoch 229, Loss 178579278.00, Val loss 19309860.00\n","Epoch 230, Loss 180191867.38, Val loss 18913528.00\n","Epoch 231, Loss 178093681.12, Val loss 19729816.00\n","Epoch 232, Loss 178560403.12, Val loss 19596452.00\n","Epoch 233, Loss 179367014.75, Val loss 19711130.00\n","Epoch 234, Loss 181881940.12, Val loss 19632768.00\n","Epoch 235, Loss 179162213.12, Val loss 19593356.00\n","Epoch 236, Loss 186226995.12, Val loss 19547462.00\n","Epoch 237, Loss 183776315.12, Val loss 19441096.00\n","Epoch 238, Loss 201064205.50, Val loss 19215210.00\n","Epoch 239, Loss 205588475.88, Val loss 19370478.00\n","Epoch 240, Loss 367595495.00, Val loss 14303105.00\n","Epoch 241, Loss 190022532.50, Val loss 20584480.00\n","Epoch 242, Loss 229789809.00, Val loss 14222327.00\n","Epoch 243, Loss 350350582.75, Val loss 28499886.00\n","Epoch 244, Loss 790606765.75, Val loss 30100438.00\n","==================================================================\n","Saved best model\n","Epoch 245, Loss 686802756.62, Val loss 13147374.00\n","Epoch 246, Loss 712957470.50, Val loss 16191174.00\n","Epoch 247, Loss 394846787.25, Val loss 23157048.00\n","Epoch 248, Loss 234656673.50, Val loss 20566592.00\n","Epoch 249, Loss 228349139.50, Val loss 18796530.00\n","Epoch 250, Loss 231600411.75, Val loss 17172952.00\n","Epoch 251, Loss 234996154.00, Val loss 15820293.00\n","Epoch 252, Loss 239499760.00, Val loss 14791625.00\n","Epoch 253, Loss 237279385.50, Val loss 14267445.00\n","Epoch 254, Loss 239380369.25, Val loss 13835960.00\n","Epoch 255, Loss 239020609.25, Val loss 13531454.00\n","Epoch 256, Loss 238954108.62, Val loss 13282782.00\n","==================================================================\n","Saved best model\n","Epoch 257, Loss 238135402.88, Val loss 13091147.00\n","==================================================================\n","Saved best model\n","Epoch 258, Loss 237368675.62, Val loss 12928734.00\n","==================================================================\n","Saved best model\n","Epoch 259, Loss 236430392.12, Val loss 12792589.00\n","Epoch 260, Loss 227975702.75, Val loss 12901272.00\n","==================================================================\n","Saved best model\n","Epoch 261, Loss 237567047.75, Val loss 12553597.00\n","Epoch 262, Loss 230511348.88, Val loss 12580183.00\n","Epoch 263, Loss 226785186.00, Val loss 12599043.00\n","==================================================================\n","Saved best model\n","Epoch 264, Loss 233921943.12, Val loss 12339859.00\n","Epoch 265, Loss 228527978.62, Val loss 12361809.00\n","Epoch 266, Loss 224302656.38, Val loss 12421464.00\n","==================================================================\n","Saved best model\n","Epoch 267, Loss 233278221.75, Val loss 12148561.00\n","Epoch 268, Loss 225597386.12, Val loss 12247922.00\n","==================================================================\n","Saved best model\n","Epoch 269, Loss 234186918.25, Val loss 12013538.00\n","Epoch 270, Loss 224185372.12, Val loss 12186205.00\n","Epoch 271, Loss 224685980.88, Val loss 12167366.00\n","Epoch 272, Loss 229054295.25, Val loss 12038474.00\n","Epoch 273, Loss 226523992.50, Val loss 12073043.00\n","==================================================================\n","Saved best model\n","Epoch 274, Loss 230971348.88, Val loss 11966482.00\n","Epoch 275, Loss 225519486.88, Val loss 12094103.00\n","==================================================================\n","Saved best model\n","Epoch 276, Loss 233898474.88, Val loss 11934073.00\n","Epoch 277, Loss 223035586.62, Val loss 12226462.00\n","==================================================================\n","Saved best model\n","Epoch 278, Loss 241620519.62, Val loss 11879282.00\n","Epoch 279, Loss 225227173.75, Val loss 12262727.00\n","Epoch 280, Loss 229724821.50, Val loss 12272427.00\n","Epoch 281, Loss 225377809.62, Val loss 12464435.00\n","Epoch 282, Loss 238166633.00, Val loss 12286669.00\n","Epoch 283, Loss 231565304.00, Val loss 12520141.00\n","Epoch 284, Loss 231937087.50, Val loss 12627398.00\n","Epoch 285, Loss 225152679.00, Val loss 12901008.00\n","Epoch 286, Loss 234134718.00, Val loss 12744313.00\n","Epoch 287, Loss 226481969.00, Val loss 12935119.00\n","Epoch 288, Loss 229983080.00, Val loss 12912377.00\n","Epoch 289, Loss 229949029.75, Val loss 12904612.00\n","Epoch 290, Loss 219888057.75, Val loss 13282676.00\n","Epoch 291, Loss 237357781.75, Val loss 12876906.00\n","Epoch 292, Loss 216112298.75, Val loss 13552204.00\n","Epoch 293, Loss 239774488.25, Val loss 13033666.00\n","Epoch 294, Loss 214356118.12, Val loss 14012810.00\n","Epoch 295, Loss 256524772.50, Val loss 13105107.00\n","Epoch 296, Loss 237512241.38, Val loss 14874854.00\n","Epoch 297, Loss 353812384.50, Val loss 13773559.00\n","Epoch 298, Loss 426526075.62, Val loss 16855404.00\n","Epoch 299, Loss 484816951.00, Val loss 27814004.00\n","Epoch 300, Loss 424605226.00, Val loss 17699842.00\n","Epoch 301, Loss 324757706.75, Val loss 28326300.00\n","Epoch 302, Loss 226110419.50, Val loss 18128532.00\n","Epoch 303, Loss 209460410.00, Val loss 22729982.00\n","Epoch 304, Loss 186182890.88, Val loss 18926854.00\n","Epoch 305, Loss 189857771.50, Val loss 18698504.00\n","Epoch 306, Loss 190508620.25, Val loss 17979588.00\n","Epoch 307, Loss 191301099.00, Val loss 17123118.00\n","Epoch 308, Loss 193809928.00, Val loss 16439899.00\n","Epoch 309, Loss 195765813.12, Val loss 15868308.00\n","Epoch 310, Loss 198461213.50, Val loss 15418087.00\n","Epoch 311, Loss 199820847.50, Val loss 15129255.00\n","Epoch 312, Loss 201794585.50, Val loss 14852024.00\n","Epoch 313, Loss 209681225.75, Val loss 14379832.00\n","Epoch 314, Loss 205284937.75, Val loss 14317040.00\n","Epoch 315, Loss 206768102.25, Val loss 14280848.00\n","Epoch 316, Loss 211474509.00, Val loss 14073369.00\n","Epoch 317, Loss 207240335.25, Val loss 14124985.00\n","Epoch 318, Loss 215414055.00, Val loss 13804172.00\n","Epoch 319, Loss 203948002.25, Val loss 14230183.00\n","Epoch 320, Loss 211442113.50, Val loss 14044738.00\n","Epoch 321, Loss 224121131.25, Val loss 14723744.00\n","Epoch 322, Loss 246600807.25, Val loss 14421152.00\n","Epoch 323, Loss 220150524.00, Val loss 15802472.00\n","Epoch 324, Loss 342516836.75, Val loss 14794140.00\n","Epoch 325, Loss 359543596.62, Val loss 16958042.00\n","Epoch 326, Loss 511214453.75, Val loss 27243064.00\n","Epoch 327, Loss 410387200.62, Val loss 18207264.00\n","Epoch 328, Loss 302189914.25, Val loss 27499762.00\n","Epoch 329, Loss 209501593.75, Val loss 18423426.00\n","Epoch 330, Loss 198673662.75, Val loss 22351526.00\n","Epoch 331, Loss 179476455.62, Val loss 19063100.00\n","Epoch 332, Loss 183344355.88, Val loss 18885174.00\n","Epoch 333, Loss 182321507.12, Val loss 17943360.00\n","Epoch 334, Loss 184814096.38, Val loss 17449110.00\n","Epoch 335, Loss 185345989.25, Val loss 16810600.00\n","Epoch 336, Loss 187982706.88, Val loss 16315762.00\n","Epoch 337, Loss 193938551.75, Val loss 15713822.00\n","Epoch 338, Loss 193209326.00, Val loss 15225404.00\n","Epoch 339, Loss 193487512.88, Val loss 15033609.00\n","Epoch 340, Loss 196475483.50, Val loss 14875271.00\n","Epoch 341, Loss 200815217.75, Val loss 14607837.00\n","Epoch 342, Loss 199249312.75, Val loss 14500167.00\n","Epoch 343, Loss 204035948.00, Val loss 14299966.00\n","Epoch 344, Loss 199273254.75, Val loss 14387988.00\n","Epoch 345, Loss 207529571.75, Val loss 14164298.00\n","Epoch 346, Loss 196749684.75, Val loss 14480073.00\n","Epoch 347, Loss 216060211.50, Val loss 14018072.00\n","Epoch 348, Loss 196305356.62, Val loss 14516547.00\n","Epoch 349, Loss 211067000.75, Val loss 14256595.00\n","Epoch 350, Loss 195101821.75, Val loss 14593410.00\n","Epoch 351, Loss 204830098.00, Val loss 14384532.00\n","Epoch 352, Loss 197415476.25, Val loss 14524323.00\n","Epoch 353, Loss 203170587.25, Val loss 14270043.00\n","Epoch 354, Loss 197776417.25, Val loss 14434065.00\n","Epoch 355, Loss 200378961.25, Val loss 14305221.00\n","Epoch 356, Loss 205474150.50, Val loss 14080348.00\n","Epoch 357, Loss 200691277.25, Val loss 14268029.00\n","Epoch 358, Loss 202053182.75, Val loss 14225380.00\n","Epoch 359, Loss 204090486.50, Val loss 14229961.00\n","Epoch 360, Loss 202196550.25, Val loss 14251502.00\n","Epoch 361, Loss 194780598.00, Val loss 14594802.00\n","Epoch 362, Loss 208149315.25, Val loss 14248260.00\n","Epoch 363, Loss 191577765.88, Val loss 14856688.00\n","Epoch 364, Loss 220439198.75, Val loss 14111698.00\n","Epoch 365, Loss 205387099.38, Val loss 15405647.00\n","Epoch 366, Loss 296406898.75, Val loss 14582142.00\n","Epoch 367, Loss 398169845.00, Val loss 17333812.00\n","Epoch 368, Loss 493931992.00, Val loss 28844988.00\n","Epoch 369, Loss 570723777.50, Val loss 20110082.00\n","Epoch 370, Loss 384557750.50, Val loss 24469048.00\n","Epoch 371, Loss 307659144.00, Val loss 22163490.00\n","Epoch 372, Loss 205908773.25, Val loss 22034454.00\n","Epoch 373, Loss 188144176.00, Val loss 20718476.00\n","Epoch 374, Loss 178518808.25, Val loss 21422040.00\n","Epoch 375, Loss 172865934.75, Val loss 19872472.00\n","Epoch 376, Loss 173656051.62, Val loss 19706770.00\n","Epoch 377, Loss 172211026.25, Val loss 19285230.00\n","Epoch 378, Loss 171722824.88, Val loss 18606460.00\n","Epoch 379, Loss 172554303.62, Val loss 18174616.00\n","Epoch 380, Loss 173074534.00, Val loss 17634678.00\n","Epoch 381, Loss 178128573.25, Val loss 17184520.00\n","Epoch 382, Loss 176466562.75, Val loss 16483942.00\n","Epoch 383, Loss 178723931.25, Val loss 16256051.00\n","Epoch 384, Loss 179192531.88, Val loss 15979583.00\n","Epoch 385, Loss 187510990.25, Val loss 15750198.00\n","Epoch 386, Loss 181206336.62, Val loss 15635696.00\n","Epoch 387, Loss 185844804.00, Val loss 15554827.00\n","Epoch 388, Loss 184105634.12, Val loss 15353863.00\n","Epoch 389, Loss 189917472.00, Val loss 15232414.00\n","Epoch 390, Loss 183773038.38, Val loss 15206334.00\n","Epoch 391, Loss 196280352.75, Val loss 15058590.00\n","Epoch 392, Loss 184954958.12, Val loss 15231995.00\n","Epoch 393, Loss 199213203.25, Val loss 14829454.00\n","Epoch 394, Loss 184382809.50, Val loss 15112557.00\n","Epoch 395, Loss 219072065.50, Val loss 14392965.00\n","Epoch 396, Loss 193704271.25, Val loss 15381516.00\n","Epoch 397, Loss 265085556.25, Val loss 14952174.00\n","Epoch 398, Loss 259723963.00, Val loss 16103291.00\n","Epoch 399, Loss 422086439.25, Val loss 21130682.00\n","Epoch 400, Loss 416969060.62, Val loss 17172086.00\n","Epoch 401, Loss 373853729.50, Val loss 28340004.00\n","Epoch 402, Loss 280058232.50, Val loss 17688934.00\n","Epoch 403, Loss 219099080.00, Val loss 23991508.00\n","Epoch 404, Loss 179132825.12, Val loss 18293276.00\n","Epoch 405, Loss 177030029.38, Val loss 20911942.00\n","Epoch 406, Loss 166172112.12, Val loss 18655014.00\n","Epoch 407, Loss 169787953.62, Val loss 18563570.00\n","Epoch 408, Loss 168555463.75, Val loss 17750748.00\n","Epoch 409, Loss 171439830.00, Val loss 17571144.00\n","Epoch 410, Loss 173312036.62, Val loss 17041888.00\n","Epoch 411, Loss 174523274.12, Val loss 16486764.00\n","Epoch 412, Loss 177820300.50, Val loss 16089933.00\n","Epoch 413, Loss 178734025.00, Val loss 15775334.00\n","Epoch 414, Loss 182154731.00, Val loss 15594320.00\n","Epoch 415, Loss 181576297.50, Val loss 15472937.00\n","Epoch 416, Loss 184935813.75, Val loss 15366378.00\n","Epoch 417, Loss 181934401.62, Val loss 15325938.00\n","Epoch 418, Loss 187359371.50, Val loss 15269765.00\n","Epoch 419, Loss 180713983.12, Val loss 15292865.00\n","Epoch 420, Loss 192547064.50, Val loss 15083445.00\n","Epoch 421, Loss 181683042.88, Val loss 15180211.00\n","Epoch 422, Loss 195676142.50, Val loss 14982248.00\n","Epoch 423, Loss 180453486.12, Val loss 15225529.00\n","Epoch 424, Loss 204394845.25, Val loss 14805685.00\n","Epoch 425, Loss 180308186.50, Val loss 15363094.00\n","Epoch 426, Loss 220883919.50, Val loss 15079294.00\n","Epoch 427, Loss 193654805.75, Val loss 15651269.00\n","Epoch 428, Loss 281355320.50, Val loss 16104440.00\n","Epoch 429, Loss 283994205.50, Val loss 16171924.00\n","Epoch 430, Loss 434419275.25, Val loss 24540228.00\n","Epoch 431, Loss 410725365.50, Val loss 16976088.00\n","Epoch 432, Loss 359136142.00, Val loss 27553674.00\n","Epoch 433, Loss 263936418.25, Val loss 17965474.00\n","Epoch 434, Loss 208098955.00, Val loss 23245672.00\n","Epoch 435, Loss 175157204.00, Val loss 18495162.00\n","Epoch 436, Loss 173362765.50, Val loss 20760754.00\n","Epoch 437, Loss 164100000.25, Val loss 18705400.00\n","Epoch 438, Loss 167553418.12, Val loss 18739048.00\n","Epoch 439, Loss 166138046.12, Val loss 17904948.00\n","Epoch 440, Loss 168537016.50, Val loss 17864228.00\n","Epoch 441, Loss 169731178.88, Val loss 17298720.00\n","Epoch 442, Loss 170645921.25, Val loss 16805704.00\n","Epoch 443, Loss 170536836.25, Val loss 16488567.00\n","Epoch 444, Loss 178988158.88, Val loss 16305751.00\n","Epoch 445, Loss 172705630.00, Val loss 16006077.00\n","Epoch 446, Loss 177919223.25, Val loss 15994683.00\n","Epoch 447, Loss 176342173.88, Val loss 15836609.00\n","Epoch 448, Loss 181161831.38, Val loss 15700165.00\n","Epoch 449, Loss 176653538.75, Val loss 15581944.00\n","Epoch 450, Loss 185914919.00, Val loss 15441060.00\n","Epoch 451, Loss 179643815.75, Val loss 15354389.00\n","Epoch 452, Loss 185477858.25, Val loss 15241733.00\n","Epoch 453, Loss 182856323.50, Val loss 15238137.00\n","Epoch 454, Loss 181912445.88, Val loss 15223838.00\n","Epoch 455, Loss 182266333.00, Val loss 15267589.00\n","Epoch 456, Loss 189945376.50, Val loss 14978350.00\n","Epoch 457, Loss 179781029.50, Val loss 15163514.00\n","Epoch 458, Loss 193635938.25, Val loss 14947360.00\n","Epoch 459, Loss 176803064.38, Val loss 15257041.00\n","Epoch 460, Loss 213617277.75, Val loss 14722255.00\n","Epoch 461, Loss 189995204.88, Val loss 15552864.00\n","Epoch 462, Loss 284750823.50, Val loss 15869332.00\n","Epoch 463, Loss 316332546.00, Val loss 16219458.00\n","Epoch 464, Loss 504652690.25, Val loss 28683282.00\n","Epoch 465, Loss 508969561.25, Val loss 18653644.00\n","Epoch 466, Loss 327375223.00, Val loss 24401662.00\n","Epoch 467, Loss 232358530.75, Val loss 20547100.00\n","Epoch 468, Loss 186659362.50, Val loss 22255656.00\n","Epoch 469, Loss 172728714.88, Val loss 20216816.00\n","Epoch 470, Loss 171215649.38, Val loss 21308156.00\n","Epoch 471, Loss 165477331.00, Val loss 19750168.00\n","Epoch 472, Loss 167019885.25, Val loss 20118736.00\n","Epoch 473, Loss 164758812.25, Val loss 19328190.00\n","Epoch 474, Loss 165746641.12, Val loss 19032454.00\n","Epoch 475, Loss 166025633.38, Val loss 18573288.00\n","Epoch 476, Loss 166909674.75, Val loss 18135808.00\n","Epoch 477, Loss 168060478.12, Val loss 17719910.00\n","Epoch 478, Loss 169335311.88, Val loss 17317636.00\n","Epoch 479, Loss 170683040.62, Val loss 16949500.00\n","Epoch 480, Loss 172079125.50, Val loss 16621186.00\n","Epoch 481, Loss 173384240.12, Val loss 16338742.00\n","Epoch 482, Loss 174576118.62, Val loss 16095552.00\n","Epoch 483, Loss 175671006.88, Val loss 15859368.00\n","Epoch 484, Loss 176693905.88, Val loss 15721707.00\n","Epoch 485, Loss 177629470.38, Val loss 15617163.00\n","Epoch 486, Loss 177823524.00, Val loss 15567451.00\n","Epoch 487, Loss 178341178.50, Val loss 15438238.00\n","Epoch 488, Loss 178056453.00, Val loss 15420256.00\n","Epoch 489, Loss 179320938.25, Val loss 15333801.00\n","Epoch 490, Loss 177543155.88, Val loss 15327543.00\n","Epoch 491, Loss 181434759.62, Val loss 15271280.00\n","Epoch 492, Loss 182055598.62, Val loss 15184595.00\n","Epoch 493, Loss 183256627.00, Val loss 15090977.00\n","Epoch 494, Loss 182037358.25, Val loss 15031514.00\n","Epoch 495, Loss 177136532.50, Val loss 15028632.00\n","Epoch 496, Loss 186555045.25, Val loss 15124044.00\n","Epoch 497, Loss 175872294.75, Val loss 15074538.00\n","Epoch 498, Loss 191714501.50, Val loss 15098732.00\n","Epoch 499, Loss 172850399.88, Val loss 15014232.00\n","Epoch 500, Loss 207624298.75, Val loss 15341370.00\n","Epoch 501, Loss 188663335.00, Val loss 14997332.00\n","Epoch 502, Loss 307743063.50, Val loss 17396190.00\n","Epoch 503, Loss 390929595.25, Val loss 16138967.00\n","Epoch 504, Loss 563987915.75, Val loss 31657206.00\n","Epoch 505, Loss 526088105.50, Val loss 19993988.00\n","Epoch 506, Loss 247543585.00, Val loss 20777682.00\n","Epoch 507, Loss 198530741.25, Val loss 21240604.00\n","Epoch 508, Loss 172112630.38, Val loss 20889434.00\n","Epoch 509, Loss 169490463.75, Val loss 20092922.00\n","Epoch 510, Loss 164960491.31, Val loss 20535142.00\n","Epoch 511, Loss 161941578.38, Val loss 19543360.00\n","Epoch 512, Loss 162275035.69, Val loss 19691000.00\n","Epoch 513, Loss 160580415.31, Val loss 19031816.00\n","Epoch 514, Loss 161326073.38, Val loss 18815340.00\n","Epoch 515, Loss 161188639.56, Val loss 18367628.00\n","Epoch 516, Loss 162049865.69, Val loss 18019994.00\n","Epoch 517, Loss 162776329.56, Val loss 17635350.00\n","Epoch 518, Loss 163966658.25, Val loss 17291858.00\n","Epoch 519, Loss 165041875.25, Val loss 16952338.00\n","Epoch 520, Loss 166424422.12, Val loss 16658315.00\n","Epoch 521, Loss 167533045.75, Val loss 16383095.00\n","Epoch 522, Loss 168904103.62, Val loss 16157580.00\n","Epoch 523, Loss 169777279.25, Val loss 15952069.00\n","Epoch 524, Loss 171094117.12, Val loss 15798801.00\n","Epoch 525, Loss 171401175.25, Val loss 15613525.00\n","Epoch 526, Loss 172859320.38, Val loss 15555363.00\n","Epoch 527, Loss 172633835.75, Val loss 15488515.00\n","Epoch 528, Loss 173923487.62, Val loss 15403087.00\n","Epoch 529, Loss 172675923.62, Val loss 15365004.00\n","Epoch 530, Loss 176097872.25, Val loss 15327468.00\n","Epoch 531, Loss 177476659.50, Val loss 15227294.00\n","Epoch 532, Loss 177180721.00, Val loss 15118349.00\n","Epoch 533, Loss 179278058.00, Val loss 15056175.00\n","Epoch 534, Loss 176545207.00, Val loss 14981635.00\n","Epoch 535, Loss 174623373.88, Val loss 15021455.00\n","Epoch 536, Loss 177333550.00, Val loss 15098040.00\n","Epoch 537, Loss 179180586.25, Val loss 15066505.00\n","Epoch 538, Loss 175950411.25, Val loss 14990851.00\n","Epoch 539, Loss 184167663.50, Val loss 14978531.00\n","Epoch 540, Loss 170822789.12, Val loss 14913753.00\n","Epoch 541, Loss 200256243.75, Val loss 15098497.00\n","Epoch 542, Loss 182300349.62, Val loss 14793060.00\n","Epoch 543, Loss 281158057.50, Val loss 16856776.00\n","Epoch 544, Loss 353857063.75, Val loss 15332827.00\n","Epoch 545, Loss 484090385.00, Val loss 29326312.00\n","Epoch 546, Loss 524978706.75, Val loss 19307358.00\n","Epoch 547, Loss 305004508.75, Val loss 21328146.00\n","Epoch 548, Loss 250267471.75, Val loss 21636586.00\n","Epoch 549, Loss 183400165.38, Val loss 20422856.00\n","Epoch 550, Loss 180783723.75, Val loss 20312104.00\n","Epoch 551, Loss 165914593.31, Val loss 20581130.00\n","Epoch 552, Loss 163488511.12, Val loss 19505458.00\n","Epoch 553, Loss 161762338.62, Val loss 20040202.00\n","Epoch 554, Loss 158792948.75, Val loss 19000222.00\n","Epoch 555, Loss 159961058.38, Val loss 19211522.00\n","Epoch 556, Loss 158154516.38, Val loss 18486830.00\n","Epoch 557, Loss 159716190.81, Val loss 18390946.00\n","Epoch 558, Loss 159128605.56, Val loss 17870920.00\n","Epoch 559, Loss 160989474.50, Val loss 17656578.00\n","Epoch 560, Loss 160855792.75, Val loss 17231148.00\n","Epoch 561, Loss 162957140.12, Val loss 17019906.00\n","Epoch 562, Loss 162873242.00, Val loss 16654555.00\n","Epoch 563, Loss 165447158.50, Val loss 16493764.00\n","Epoch 564, Loss 164523546.00, Val loss 16166497.00\n","Epoch 565, Loss 168445502.88, Val loss 16077418.00\n","Epoch 566, Loss 165573223.00, Val loss 15821028.00\n","Epoch 567, Loss 172597021.88, Val loss 15863291.00\n","Epoch 568, Loss 164878873.50, Val loss 15535134.00\n","Epoch 569, Loss 182508837.75, Val loss 15812578.00\n","Epoch 570, Loss 166302285.94, Val loss 15310568.00\n","Epoch 571, Loss 196939981.75, Val loss 15942487.00\n","Epoch 572, Loss 170098913.56, Val loss 15170073.00\n","Epoch 573, Loss 228763326.75, Val loss 16812328.00\n","Epoch 574, Loss 208545385.25, Val loss 15048519.00\n","Epoch 575, Loss 316822666.75, Val loss 20811030.00\n","Epoch 576, Loss 314876410.75, Val loss 15447027.00\n","Epoch 577, Loss 359968124.00, Val loss 26891716.00\n","Epoch 578, Loss 299563770.00, Val loss 16754638.00\n","Epoch 579, Loss 246969895.50, Val loss 23993554.00\n","Epoch 580, Loss 191470252.75, Val loss 17698678.00\n","Epoch 581, Loss 176795696.62, Val loss 21446084.00\n","Epoch 582, Loss 159766852.00, Val loss 18121014.00\n","Epoch 583, Loss 162754152.06, Val loss 19780904.00\n","Epoch 584, Loss 155841655.75, Val loss 18097126.00\n","Epoch 585, Loss 160038426.62, Val loss 18626076.00\n","Epoch 586, Loss 156701975.25, Val loss 17654264.00\n","Epoch 587, Loss 161172860.25, Val loss 17839686.00\n","Epoch 588, Loss 158075151.00, Val loss 17038508.00\n","Epoch 589, Loss 164704654.50, Val loss 17246706.00\n","Epoch 590, Loss 159444276.00, Val loss 16438726.00\n","Epoch 591, Loss 171660261.88, Val loss 16809310.00\n","Epoch 592, Loss 159954614.81, Val loss 15861802.00\n","Epoch 593, Loss 174855406.25, Val loss 16609062.00\n","Epoch 594, Loss 161501957.00, Val loss 15887134.00\n","Epoch 595, Loss 175423367.25, Val loss 16120716.00\n","Epoch 596, Loss 163139786.88, Val loss 15551068.00\n","Epoch 597, Loss 177891271.75, Val loss 15834279.00\n","Epoch 598, Loss 163988499.00, Val loss 15381596.00\n","Epoch 599, Loss 182463068.00, Val loss 15712679.00\n","Epoch 600, Loss 163719231.50, Val loss 15242566.00\n","Epoch 601, Loss 183247738.25, Val loss 15788419.00\n","Epoch 602, Loss 163383497.00, Val loss 15339005.00\n","Epoch 603, Loss 206067162.00, Val loss 15959061.00\n","Epoch 604, Loss 182237329.12, Val loss 15031962.00\n","Epoch 605, Loss 269027765.25, Val loss 18096280.00\n","Epoch 606, Loss 273421945.75, Val loss 15179219.00\n","Epoch 607, Loss 393370501.75, Val loss 26080574.00\n","Epoch 608, Loss 376342212.50, Val loss 16721138.00\n","Epoch 609, Loss 309267737.25, Val loss 24896588.00\n","Epoch 610, Loss 243645038.50, Val loss 18482898.00\n","Epoch 611, Loss 195018618.00, Val loss 21821782.00\n","Epoch 612, Loss 174776698.00, Val loss 18637842.00\n","Epoch 613, Loss 165948266.12, Val loss 20660670.00\n","Epoch 614, Loss 157568291.88, Val loss 18492162.00\n","Epoch 615, Loss 159474677.44, Val loss 19564852.00\n","Epoch 616, Loss 154765441.75, Val loss 18223710.00\n","Epoch 617, Loss 158068329.06, Val loss 18632934.00\n","Epoch 618, Loss 155188038.38, Val loss 17744692.00\n","Epoch 619, Loss 159034855.50, Val loss 17900802.00\n","Epoch 620, Loss 156336103.25, Val loss 17171612.00\n","Epoch 621, Loss 161469376.50, Val loss 17276310.00\n","Epoch 622, Loss 157667950.81, Val loss 16614825.00\n","Epoch 623, Loss 165662893.62, Val loss 16868838.00\n","Epoch 624, Loss 158483331.25, Val loss 16151099.00\n","Epoch 625, Loss 172772430.25, Val loss 16636981.00\n","Epoch 626, Loss 159464316.19, Val loss 15816770.00\n","Epoch 627, Loss 177619087.00, Val loss 16296687.00\n","Epoch 628, Loss 160652591.88, Val loss 15536299.00\n","Epoch 629, Loss 178492446.50, Val loss 16204751.00\n","Epoch 630, Loss 160783076.56, Val loss 15550469.00\n","Epoch 631, Loss 181162803.75, Val loss 16187552.00\n","Epoch 632, Loss 161405570.81, Val loss 15514112.00\n","Epoch 633, Loss 192844061.75, Val loss 16356834.00\n","Epoch 634, Loss 169816317.62, Val loss 15346752.00\n","Epoch 635, Loss 243101614.00, Val loss 17582238.00\n","Epoch 636, Loss 238540927.75, Val loss 15196474.00\n","Epoch 637, Loss 366049331.75, Val loss 24096958.00\n","Epoch 638, Loss 367208687.00, Val loss 16416318.00\n","Epoch 639, Loss 335214174.00, Val loss 25786764.00\n","Epoch 640, Loss 280629318.50, Val loss 18384474.00\n","Epoch 641, Loss 212909169.50, Val loss 22098928.00\n","Epoch 642, Loss 178307838.50, Val loss 18680868.00\n","Epoch 643, Loss 165274799.94, Val loss 20724620.00\n","Epoch 644, Loss 156428372.38, Val loss 18622080.00\n","Epoch 645, Loss 157617552.06, Val loss 19589210.00\n","Epoch 646, Loss 153529425.94, Val loss 18434890.00\n","Epoch 647, Loss 155732257.56, Val loss 18620560.00\n","Epoch 648, Loss 154271472.69, Val loss 18015696.00\n","Epoch 649, Loss 155952433.31, Val loss 17843596.00\n","Epoch 650, Loss 155832705.94, Val loss 17467462.00\n","Epoch 651, Loss 157531297.88, Val loss 17260942.00\n","Epoch 652, Loss 157768647.19, Val loss 16963108.00\n","Epoch 653, Loss 159368412.19, Val loss 16755073.00\n","Epoch 654, Loss 160217462.62, Val loss 16558828.00\n","Epoch 655, Loss 161011553.12, Val loss 16342798.00\n","Epoch 656, Loss 163966528.00, Val loss 16281914.00\n","Epoch 657, Loss 160006652.50, Val loss 15901831.00\n","Epoch 658, Loss 168988431.88, Val loss 16153785.00\n","Epoch 659, Loss 163387428.75, Val loss 15744551.00\n","Epoch 660, Loss 165200364.25, Val loss 15706238.00\n","Epoch 661, Loss 169588962.00, Val loss 15673817.00\n","Epoch 662, Loss 166775390.88, Val loss 15481176.00\n","Epoch 663, Loss 164676114.12, Val loss 15399281.00\n","Epoch 664, Loss 169200503.00, Val loss 15569395.00\n","Epoch 665, Loss 169712395.25, Val loss 15461079.00\n","Epoch 666, Loss 167338851.88, Val loss 15398593.00\n","Epoch 667, Loss 167762013.62, Val loss 15387995.00\n","Epoch 668, Loss 172244294.62, Val loss 15439813.00\n","Epoch 669, Loss 166289073.88, Val loss 15284670.00\n","Epoch 670, Loss 169235954.25, Val loss 15411124.00\n","Epoch 671, Loss 171363800.50, Val loss 15360748.00\n","Epoch 672, Loss 165922385.12, Val loss 15307243.00\n","Epoch 673, Loss 172675476.75, Val loss 15386357.00\n","Epoch 674, Loss 163773942.62, Val loss 15325311.00\n","Epoch 675, Loss 182451645.50, Val loss 15480936.00\n","Epoch 676, Loss 165893430.50, Val loss 15202136.00\n","Epoch 677, Loss 248378602.00, Val loss 16586143.00\n","Epoch 678, Loss 322123531.00, Val loss 15523312.00\n","Epoch 679, Loss 506013178.75, Val loss 29293528.00\n","Epoch 680, Loss 622497526.00, Val loss 21691990.00\n","Epoch 681, Loss 292293451.50, Val loss 18030750.00\n","Epoch 682, Loss 258886562.75, Val loss 23452780.00\n","Epoch 683, Loss 174669982.38, Val loss 18920686.00\n","Epoch 684, Loss 179078111.00, Val loss 21052128.00\n","Epoch 685, Loss 159139214.81, Val loss 19910024.00\n","Epoch 686, Loss 162258294.25, Val loss 20080902.00\n","Epoch 687, Loss 156189421.88, Val loss 19890614.00\n","Epoch 688, Loss 155854007.00, Val loss 19354140.00\n","Epoch 689, Loss 154475375.75, Val loss 19550850.00\n","Epoch 690, Loss 152471389.25, Val loss 18843352.00\n","Epoch 691, Loss 153234775.06, Val loss 18895330.00\n","Epoch 692, Loss 152202159.81, Val loss 18421046.00\n","Epoch 693, Loss 153226007.31, Val loss 18224554.00\n","Epoch 694, Loss 153100924.06, Val loss 17898974.00\n","Epoch 695, Loss 154028430.38, Val loss 17645126.00\n","Epoch 696, Loss 154782652.81, Val loss 17417226.00\n","Epoch 697, Loss 155261483.62, Val loss 17116282.00\n","Epoch 698, Loss 157201964.38, Val loss 17009186.00\n","Epoch 699, Loss 156330951.06, Val loss 16656704.00\n","Epoch 700, Loss 160890959.38, Val loss 16721471.00\n","Epoch 701, Loss 156485826.62, Val loss 16239201.00\n","Epoch 702, Loss 168652095.12, Val loss 16617526.00\n","Epoch 703, Loss 156686866.81, Val loss 15901586.00\n","Epoch 704, Loss 177541585.25, Val loss 16466481.00\n","Epoch 705, Loss 158385064.06, Val loss 15596232.00\n","Epoch 706, Loss 191459621.50, Val loss 16806658.00\n","Epoch 707, Loss 167719114.00, Val loss 15498730.00\n","Epoch 708, Loss 237456828.75, Val loss 18264680.00\n","Epoch 709, Loss 227876412.50, Val loss 15436644.00\n","Epoch 710, Loss 344376432.25, Val loss 24069072.00\n","Epoch 711, Loss 341267245.25, Val loss 16636325.00\n","Epoch 712, Loss 311995291.50, Val loss 25013752.00\n","Epoch 713, Loss 265017666.25, Val loss 18498352.00\n","Epoch 714, Loss 205423631.00, Val loss 21765864.00\n","Epoch 715, Loss 176889986.50, Val loss 18675502.00\n","Epoch 716, Loss 163935149.00, Val loss 20627932.00\n","Epoch 717, Loss 155716839.62, Val loss 18481972.00\n","Epoch 718, Loss 156476565.88, Val loss 19714456.00\n","Epoch 719, Loss 151388318.75, Val loss 18236150.00\n","Epoch 720, Loss 154651514.38, Val loss 18867172.00\n","Epoch 721, Loss 151273834.25, Val loss 17861954.00\n","Epoch 722, Loss 155287159.06, Val loss 18198416.00\n","Epoch 723, Loss 152115811.75, Val loss 17361410.00\n","Epoch 724, Loss 158143184.75, Val loss 17714714.00\n","Epoch 725, Loss 152581274.12, Val loss 16782974.00\n","Epoch 726, Loss 162713400.75, Val loss 17405254.00\n","Epoch 727, Loss 154048350.25, Val loss 16479531.00\n","Epoch 728, Loss 161659936.50, Val loss 16748058.00\n","Epoch 729, Loss 155601257.81, Val loss 16200280.00\n","Epoch 730, Loss 165987418.12, Val loss 16560175.00\n","Epoch 731, Loss 156585443.38, Val loss 15980695.00\n","Epoch 732, Loss 169866282.00, Val loss 16331152.00\n","Epoch 733, Loss 156455487.44, Val loss 15746613.00\n","Epoch 734, Loss 182857929.00, Val loss 16364060.00\n","Epoch 735, Loss 161088500.12, Val loss 15486461.00\n","Epoch 736, Loss 209362007.50, Val loss 17197666.00\n","Epoch 737, Loss 190222861.12, Val loss 15440061.00\n","Epoch 738, Loss 282837352.25, Val loss 20590798.00\n","Epoch 739, Loss 292024324.25, Val loss 15971323.00\n","Epoch 740, Loss 357128316.25, Val loss 26381598.00\n","Epoch 741, Loss 337151449.25, Val loss 17926210.00\n","Epoch 742, Loss 252210063.25, Val loss 22555262.00\n","Epoch 743, Loss 212700622.00, Val loss 19127220.00\n","Epoch 744, Loss 180037465.50, Val loss 20982374.00\n","Epoch 745, Loss 164554164.38, Val loss 18704410.00\n","Epoch 746, Loss 158677418.31, Val loss 20176124.00\n","Epoch 747, Loss 152992291.50, Val loss 18453032.00\n","Epoch 748, Loss 154605493.00, Val loss 19340702.00\n","Epoch 749, Loss 150983108.62, Val loss 18168258.00\n","Epoch 750, Loss 154300145.12, Val loss 18606888.00\n","Epoch 751, Loss 151614718.12, Val loss 17705546.00\n","Epoch 752, Loss 156939152.25, Val loss 18127290.00\n","Epoch 753, Loss 151397208.62, Val loss 17039430.00\n","Epoch 754, Loss 160411699.88, Val loss 17817818.00\n","Epoch 755, Loss 151927453.62, Val loss 16680670.00\n","Epoch 756, Loss 163504954.50, Val loss 17440506.00\n","Epoch 757, Loss 152872757.94, Val loss 16419354.00\n","Epoch 758, Loss 163015189.75, Val loss 16863092.00\n","Epoch 759, Loss 154094980.56, Val loss 16158267.00\n","Epoch 760, Loss 170868763.75, Val loss 16849252.00\n","Epoch 761, Loss 154441809.19, Val loss 15913370.00\n","Epoch 762, Loss 178814569.00, Val loss 16758560.00\n","Epoch 763, Loss 157106339.25, Val loss 15679035.00\n","Epoch 764, Loss 189252594.00, Val loss 17119856.00\n","Epoch 765, Loss 164183650.25, Val loss 15638314.00\n","Epoch 766, Loss 221489793.75, Val loss 18265694.00\n","Epoch 767, Loss 204243000.62, Val loss 15541828.00\n","Epoch 768, Loss 303158174.50, Val loss 22471488.00\n","Epoch 769, Loss 300174161.25, Val loss 16243224.00\n","Epoch 770, Loss 308156060.00, Val loss 25336254.00\n","Epoch 771, Loss 273646386.75, Val loss 17889490.00\n","Epoch 772, Loss 227841306.25, Val loss 22423916.00\n","Epoch 773, Loss 197211330.75, Val loss 18489034.00\n","Epoch 774, Loss 174455557.38, Val loss 20998326.00\n","Epoch 775, Loss 159255087.00, Val loss 18307040.00\n","Epoch 776, Loss 158011163.25, Val loss 19965314.00\n","Epoch 777, Loss 151838164.38, Val loss 18129682.00\n","Epoch 778, Loss 155362037.56, Val loss 19186000.00\n","Epoch 779, Loss 150291769.38, Val loss 17763004.00\n","Epoch 780, Loss 155997290.94, Val loss 18595734.00\n","Epoch 781, Loss 150373404.44, Val loss 17305602.00\n","Epoch 782, Loss 159279737.38, Val loss 18179234.00\n","Epoch 783, Loss 150427111.44, Val loss 16913272.00\n","Epoch 784, Loss 159827379.25, Val loss 17484424.00\n","Epoch 785, Loss 152264354.88, Val loss 16552939.00\n","Epoch 786, Loss 169518271.12, Val loss 17441776.00\n","Epoch 787, Loss 153180805.56, Val loss 16125221.00\n","Epoch 788, Loss 172482547.75, Val loss 17318864.00\n","Epoch 789, Loss 153832513.62, Val loss 16067019.00\n","Epoch 790, Loss 189161075.25, Val loss 17454510.00\n","Epoch 791, Loss 166526795.25, Val loss 15683384.00\n","Epoch 792, Loss 223339388.50, Val loss 18842968.00\n","Epoch 793, Loss 203877327.00, Val loss 15668631.00\n","Epoch 794, Loss 282881022.50, Val loss 22504810.00\n","Epoch 795, Loss 268135627.00, Val loss 16235004.00\n","Epoch 796, Loss 279097672.50, Val loss 24552806.00\n","Epoch 797, Loss 246083169.75, Val loss 17385238.00\n","Epoch 798, Loss 227819075.00, Val loss 22774958.00\n","Epoch 799, Loss 197218506.00, Val loss 17905296.00\n","Epoch 800, Loss 180947384.25, Val loss 21266242.00\n","Epoch 801, Loss 162141162.25, Val loss 17899522.00\n","Epoch 802, Loss 162197273.88, Val loss 20142912.00\n","Epoch 803, Loss 153435146.25, Val loss 17765818.00\n","Epoch 804, Loss 158857253.88, Val loss 19415172.00\n","Epoch 805, Loss 150986006.50, Val loss 17460692.00\n","Epoch 806, Loss 159971189.00, Val loss 18862102.00\n","Epoch 807, Loss 149442552.75, Val loss 17170926.00\n","Epoch 808, Loss 158985102.00, Val loss 18040176.00\n","Epoch 809, Loss 150816729.56, Val loss 16778322.00\n","Epoch 810, Loss 166381052.38, Val loss 17928610.00\n","Epoch 811, Loss 151074350.31, Val loss 16395176.00\n","Epoch 812, Loss 164866141.62, Val loss 17513444.00\n","Epoch 813, Loss 151275853.38, Val loss 16405851.00\n","Epoch 814, Loss 169075822.50, Val loss 17196014.00\n","Epoch 815, Loss 152701322.50, Val loss 16056359.00\n","Epoch 816, Loss 173948699.25, Val loss 17230028.00\n","Epoch 817, Loss 154070769.38, Val loss 16024086.00\n","Epoch 818, Loss 184036039.50, Val loss 17467816.00\n","Epoch 819, Loss 162765801.00, Val loss 15845239.00\n","Epoch 820, Loss 221009674.25, Val loss 18706854.00\n","Epoch 821, Loss 210233603.25, Val loss 15729987.00\n","Epoch 822, Loss 310240863.00, Val loss 23332678.00\n","Epoch 823, Loss 313825837.75, Val loss 16732556.00\n","Epoch 824, Loss 314439338.25, Val loss 24953694.00\n","Epoch 825, Loss 283555594.25, Val loss 18804692.00\n","Epoch 826, Loss 215747833.50, Val loss 21364822.00\n","Epoch 827, Loss 193155460.50, Val loss 19206166.00\n","Epoch 828, Loss 167487081.38, Val loss 20468644.00\n","Epoch 829, Loss 158610257.00, Val loss 18728974.00\n","Epoch 830, Loss 155140639.00, Val loss 19812996.00\n","Epoch 831, Loss 151782826.50, Val loss 18389606.00\n","Epoch 832, Loss 153532752.31, Val loss 19272038.00\n","Epoch 833, Loss 149670152.12, Val loss 17949992.00\n","Epoch 834, Loss 154319004.69, Val loss 18823438.00\n","Epoch 835, Loss 149205039.19, Val loss 17504622.00\n","Epoch 836, Loss 157073880.00, Val loss 18449058.00\n","Epoch 837, Loss 148595665.62, Val loss 17124276.00\n","Epoch 838, Loss 156956228.50, Val loss 17762470.00\n","Epoch 839, Loss 149997880.75, Val loss 16778046.00\n","Epoch 840, Loss 164185368.50, Val loss 17681336.00\n","Epoch 841, Loss 150352881.62, Val loss 16399546.00\n","Epoch 842, Loss 171154066.50, Val loss 17489214.00\n","Epoch 843, Loss 152847599.69, Val loss 16108900.00\n","Epoch 844, Loss 177118032.00, Val loss 17605870.00\n","Epoch 845, Loss 156006567.00, Val loss 16025939.00\n","Epoch 846, Loss 192298132.50, Val loss 18123414.00\n","Epoch 847, Loss 170661787.75, Val loss 15873137.00\n","Epoch 848, Loss 233703600.50, Val loss 19920596.00\n","Epoch 849, Loss 221955296.25, Val loss 15916801.00\n","Epoch 850, Loss 292198598.00, Val loss 23894504.00\n","Epoch 851, Loss 280610228.50, Val loss 16907310.00\n","Epoch 852, Loss 272007993.00, Val loss 23945454.00\n","Epoch 853, Loss 244180993.25, Val loss 18271982.00\n","Epoch 854, Loss 210130465.50, Val loss 21729012.00\n","Epoch 855, Loss 189022370.50, Val loss 18519474.00\n","Epoch 856, Loss 171404335.00, Val loss 20704842.00\n","Epoch 857, Loss 158932889.00, Val loss 18244886.00\n","Epoch 858, Loss 157548592.75, Val loss 19924540.00\n","Epoch 859, Loss 148939756.62, Val loss 18039906.00\n","Epoch 860, Loss 152435867.00, Val loss 18956672.00\n","Epoch 861, Loss 148283316.69, Val loss 17809030.00\n","Epoch 862, Loss 153029865.44, Val loss 18435736.00\n","Epoch 863, Loss 148448089.81, Val loss 17380006.00\n","Epoch 864, Loss 156093555.06, Val loss 18120488.00\n","Epoch 865, Loss 149107722.81, Val loss 16925998.00\n","Epoch 866, Loss 162804469.38, Val loss 17997062.00\n","Epoch 867, Loss 149302720.06, Val loss 16549834.00\n","Epoch 868, Loss 168230422.88, Val loss 17721370.00\n","Epoch 869, Loss 151306592.31, Val loss 16249677.00\n","Epoch 870, Loss 171684281.88, Val loss 17707370.00\n","Epoch 871, Loss 152444127.81, Val loss 16197850.00\n","Epoch 872, Loss 179330508.75, Val loss 17871114.00\n","Epoch 873, Loss 158528135.88, Val loss 16049097.00\n","Epoch 874, Loss 202899099.75, Val loss 18713028.00\n","Epoch 875, Loss 185355070.38, Val loss 15899802.00\n","Epoch 876, Loss 260667525.00, Val loss 21536688.00\n","Epoch 877, Loss 256734144.25, Val loss 16301191.00\n","Epoch 878, Loss 294921973.25, Val loss 24729166.00\n","Epoch 879, Loss 278766803.25, Val loss 17743966.00\n","Epoch 880, Loss 247166714.25, Val loss 22606372.00\n","Epoch 881, Loss 222602032.75, Val loss 18775894.00\n","Epoch 882, Loss 191314815.88, Val loss 21015424.00\n","Epoch 883, Loss 176065482.75, Val loss 18595178.00\n","Epoch 884, Loss 163675404.00, Val loss 20351598.00\n","Epoch 885, Loss 153626385.25, Val loss 18245978.00\n","Epoch 886, Loss 154015547.38, Val loss 19554914.00\n","Epoch 887, Loss 149130166.62, Val loss 17982318.00\n","Epoch 888, Loss 153132540.75, Val loss 18994446.00\n","Epoch 889, Loss 148114473.88, Val loss 17594134.00\n","Epoch 890, Loss 154965373.38, Val loss 18597892.00\n","Epoch 891, Loss 148522434.50, Val loss 17167390.00\n","Epoch 892, Loss 160109270.25, Val loss 18368892.00\n","Epoch 893, Loss 148124700.19, Val loss 16771799.00\n","Epoch 894, Loss 163167088.12, Val loss 17957384.00\n","Epoch 895, Loss 149139497.44, Val loss 16492761.00\n","Epoch 896, Loss 169719763.50, Val loss 17772316.00\n","Epoch 897, Loss 152197668.81, Val loss 16201312.00\n","Epoch 898, Loss 175526192.50, Val loss 17937292.00\n","Epoch 899, Loss 154866918.50, Val loss 16151425.00\n","Epoch 900, Loss 187528865.75, Val loss 18355906.00\n","Epoch 901, Loss 167052176.12, Val loss 16024685.00\n","Epoch 902, Loss 220161352.00, Val loss 19813278.00\n","Epoch 903, Loss 207122868.75, Val loss 16084376.00\n","Epoch 904, Loss 271676726.00, Val loss 23021758.00\n","Epoch 905, Loss 265075697.75, Val loss 16901092.00\n","Epoch 906, Loss 275381174.25, Val loss 23843482.00\n","Epoch 907, Loss 256952665.25, Val loss 18344254.00\n","Epoch 908, Loss 220765013.25, Val loss 21614250.00\n","Epoch 909, Loss 203427290.75, Val loss 18851756.00\n","Epoch 910, Loss 176538298.12, Val loss 20530868.00\n","Epoch 911, Loss 165940623.62, Val loss 18508248.00\n","Epoch 912, Loss 159834847.88, Val loss 19994738.00\n","Epoch 913, Loss 151394443.88, Val loss 18098156.00\n","Epoch 914, Loss 153410619.19, Val loss 19295256.00\n","Epoch 915, Loss 148332768.75, Val loss 17822884.00\n","Epoch 916, Loss 153131550.94, Val loss 18809518.00\n","Epoch 917, Loss 147590876.00, Val loss 17448216.00\n","Epoch 918, Loss 155418053.62, Val loss 18473884.00\n","Epoch 919, Loss 146656983.94, Val loss 17099304.00\n","Epoch 920, Loss 155198342.12, Val loss 17855482.00\n","Epoch 921, Loss 148082338.81, Val loss 16823516.00\n","Epoch 922, Loss 162104774.75, Val loss 17814822.00\n","Epoch 923, Loss 148537266.06, Val loss 16465542.00\n","Epoch 924, Loss 169657289.12, Val loss 17702624.00\n","Epoch 925, Loss 151992968.69, Val loss 16185312.00\n","Epoch 926, Loss 178025239.25, Val loss 18000394.00\n","Epoch 927, Loss 157551280.25, Val loss 16119656.00\n","Epoch 928, Loss 198043058.00, Val loss 18807090.00\n","Epoch 929, Loss 180708236.88, Val loss 16035446.00\n","Epoch 930, Loss 246136773.75, Val loss 21212314.00\n","Epoch 931, Loss 242249267.00, Val loss 16446065.00\n","Epoch 932, Loss 283627062.25, Val loss 24238186.00\n","Epoch 933, Loss 275794685.75, Val loss 17834994.00\n","Epoch 934, Loss 253307845.25, Val loss 22471326.00\n","Epoch 935, Loss 237744979.25, Val loss 19078246.00\n","Epoch 936, Loss 198530113.25, Val loss 20692186.00\n","Epoch 937, Loss 188538737.50, Val loss 19021860.00\n","Epoch 938, Loss 167606647.88, Val loss 20170944.00\n","Epoch 939, Loss 160572940.88, Val loss 18514386.00\n","Epoch 940, Loss 156254918.88, Val loss 19769526.00\n","Epoch 941, Loss 152632403.75, Val loss 18111388.00\n","Epoch 942, Loss 154602280.56, Val loss 19425182.00\n","Epoch 943, Loss 149534958.00, Val loss 17723088.00\n","Epoch 944, Loss 155561833.06, Val loss 19078066.00\n","Epoch 945, Loss 146530715.88, Val loss 17381602.00\n","Epoch 946, Loss 154233095.38, Val loss 18403370.00\n","Epoch 947, Loss 147521616.44, Val loss 17124274.00\n","Epoch 948, Loss 159280331.88, Val loss 18254262.00\n","Epoch 949, Loss 147411004.38, Val loss 16759322.00\n","Epoch 950, Loss 163368678.25, Val loss 17970982.00\n","Epoch 951, Loss 148862317.06, Val loss 16488502.00\n","Epoch 952, Loss 164818986.25, Val loss 17869268.00\n","Epoch 953, Loss 149042412.31, Val loss 16461543.00\n","Epoch 954, Loss 176175867.12, Val loss 17914042.00\n","Epoch 955, Loss 157982642.50, Val loss 16138810.00\n","Epoch 956, Loss 198742882.25, Val loss 18885924.00\n","Epoch 957, Loss 182273048.12, Val loss 16116504.00\n","Epoch 958, Loss 246723747.50, Val loss 21503172.00\n","Epoch 959, Loss 243144547.00, Val loss 16616525.00\n","Epoch 960, Loss 278368845.00, Val loss 24028480.00\n","Epoch 961, Loss 273696027.25, Val loss 17984436.00\n","Epoch 962, Loss 249079526.75, Val loss 22302488.00\n","Epoch 963, Loss 236553003.25, Val loss 19156836.00\n","Epoch 964, Loss 197132025.62, Val loss 20514538.00\n","Epoch 965, Loss 189295760.75, Val loss 19098488.00\n","Epoch 966, Loss 167421103.25, Val loss 20057318.00\n","Epoch 967, Loss 161666903.25, Val loss 18578204.00\n","Epoch 968, Loss 156292680.25, Val loss 19753608.00\n","Epoch 969, Loss 153447810.00, Val loss 18163404.00\n","Epoch 970, Loss 154666771.69, Val loss 19475052.00\n","Epoch 971, Loss 150080042.00, Val loss 17772264.00\n","Epoch 972, Loss 155659556.19, Val loss 19182892.00\n","Epoch 973, Loss 146622194.75, Val loss 17423698.00\n","Epoch 974, Loss 154350563.06, Val loss 18562588.00\n","Epoch 975, Loss 147543593.31, Val loss 17174446.00\n","Epoch 976, Loss 159644931.75, Val loss 18446582.00\n","Epoch 977, Loss 147356326.62, Val loss 16747509.00\n","Epoch 978, Loss 165200581.75, Val loss 18429014.00\n","Epoch 979, Loss 148923441.12, Val loss 16607254.00\n","Epoch 980, Loss 165764005.38, Val loss 18162422.00\n","Epoch 981, Loss 149654472.94, Val loss 16514007.00\n","Epoch 982, Loss 179643230.00, Val loss 18326824.00\n","Epoch 983, Loss 162232642.38, Val loss 16203523.00\n","Epoch 984, Loss 206233742.75, Val loss 19703068.00\n","Epoch 985, Loss 191625833.62, Val loss 16306000.00\n","Epoch 986, Loss 249511257.75, Val loss 22250480.00\n","Epoch 987, Loss 247083303.25, Val loss 16902094.00\n","Epoch 988, Loss 277093645.00, Val loss 23802004.00\n","Epoch 989, Loss 271745137.75, Val loss 18366822.00\n","Epoch 990, Loss 234399605.75, Val loss 21582602.00\n","Epoch 991, Loss 223916796.00, Val loss 19287932.00\n","Epoch 992, Loss 182484560.50, Val loss 20112020.00\n","Epoch 993, Loss 177289829.12, Val loss 19010454.00\n","Epoch 994, Loss 161874100.62, Val loss 19867876.00\n","Epoch 995, Loss 157071302.12, Val loss 18467756.00\n","Epoch 996, Loss 154101389.12, Val loss 19567242.00\n","Epoch 997, Loss 151352258.75, Val loss 18097346.00\n","Epoch 998, Loss 153628140.25, Val loss 19289028.00\n","Epoch 999, Loss 146525757.69, Val loss 17674472.00\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Testing and plotting results\n","\"\"\"\n","\n","predicted_output = None\n","labeled_output = None\n","for data in testing_data:\n","  model.eval()\n","  predicted_output = model(data)\n","  labeled_output = data.y.to(device)\n","  loss = criterion(predicted_output, labeled_output)\n","  print(loss.item())\n","\n","state_num = 29\n","\n","print(predicted_output[state_num])\n","print(labeled_output[state_num])\n","\n","plt.plot(labeled_output[state_num].cpu().detach().numpy(),c='r', label='Ground truth')\n","plt.plot(predicted_output[state_num].cpu().detach().numpy(),c='b', label='Prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Wj7dvnzpaP-J","colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"status":"ok","timestamp":1646754993032,"user_tz":360,"elapsed":207,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"ad9b4e62-d98b-40bb-b29b-432d8f1be1e5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["273689056.0\n","tensor([2594.1128, 2745.8848, 2900.8577, 3055.9751, 3218.1821, 3389.6340,\n","        3558.7051, 3724.3987, 3889.5034, 4049.0896, 4210.6846, 4362.2734,\n","        4494.1250, 4623.9326, 4756.2520], grad_fn=<SelectBackward0>)\n","tensor([2908., 2871., 3187., 3222., 3421., 3617., 3844., 3908., 3987., 4006.,\n","        4123., 4544., 4193., 4722., 4866.])\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["archived_output = {\n","     # 'loc_list':loc_list, \n","    # 'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_number_infected_tensor':predicted_output, # (52, 15)\n","    # 'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_number_infected_tensor':labeled_output # (52, 15)\n","}\n","\n","# Save archived_output as pickle for use later\n","with open(save_predictions_relative_path, 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open(save_predictions_relative_path, 'rb') as handle:\n","#     archived_output = pickle.load(handle)\n","\n","\n","# print(model_predictions_number_infected['Alabama'])\n","# print(ground_truth_number_infected['Alabama'])"],"metadata":{"id":"s3-8Yge6CAWM","executionInfo":{"status":"ok","timestamp":1646754993033,"user_tz":360,"elapsed":14,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["time_end = time.time()\n","time_seconds = time_end - time_start\n","(t_min, t_sec) = divmod(time_seconds,60)\n","(t_hour,t_min) = divmod(t_min,60) \n","print('Time passed: {} hours:{} minutes:{} seconds'.format(t_hour,t_min,t_sec))\n","# Time passed: 0.0 hours:6.0 minutes:50.026517391204834 seconds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFdb-o9NR6co","executionInfo":{"status":"ok","timestamp":1646754993036,"user_tz":360,"elapsed":17,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"ef50e04b-3ddf-43d4-dbd4-c4eb32cf4f4b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Time passed: 0.0 hours:5.0 minutes:45.13369083404541 seconds\n"]}]}]}