{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_data2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOGL9Qw9t5Md93KnnCuXwEm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Remove use of unreported data from data preprocessing\n","\"\"\""],"metadata":{"id":"bjtYC1leoEN1","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1646356262862,"user_tz":360,"elapsed":33,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"6ef2b135-ec5f-4d89-b1bb-e0dcd6954f6e"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nRemove use of unreported data from data preprocessing\\n'"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"2. STAN-Without Missing Data\"\n","! pip install epiweeks\n","! pip install haversine\n","! pip install dgl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7yDiZvV2jd2R","executionInfo":{"status":"ok","timestamp":1646356291844,"user_tz":360,"elapsed":28997,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"370b19f1-c253-45b3-f0a7-745f40ccf348"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/2. STAN-Without Missing Data\n","Collecting epiweeks\n","  Downloading epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n","Installing collected packages: epiweeks\n","Successfully installed epiweeks-2.1.4\n","Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n","Collecting dgl\n","  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.5)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n","Installing collected packages: dgl\n","Successfully installed dgl-0.6.1\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Import libraries needed\n","\"\"\"\n","from data_downloader import GenerateTrainingData\n","from utils import gravity_law_commute_dist\n","import pickle\n","import pandas as pd\n","import dgl\n","import numpy as np"],"metadata":{"id":"xyi3R865jOGA","executionInfo":{"status":"ok","timestamp":1646356300705,"user_tz":360,"elapsed":8886,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"67b2df3e-1ce6-4633-b34c-439c59dd9c82"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]},{"output_type":"stream","name":"stderr","text":["Using backend: pytorch\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables used to preprocess data\n","\"\"\"\n","START_DATE = '2020-04-12'\n","END_DATE = '2022-01-24'\n","valid_window = 25\n","test_window = 25\n","history_window=6\n","pred_window=15\n","slide_step=5"],"metadata":{"id":"3aRrgY3MfbR2","executionInfo":{"status":"ok","timestamp":1646356300706,"user_tz":360,"elapsed":7,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uYf0Z0LfSNa","executionInfo":{"status":"ok","timestamp":1646356381537,"user_tz":360,"elapsed":80836,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"0a9c2bea-d771-47de-8b5d-9c8ef0891b6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Finish download\n"]}],"source":["\"\"\"\n","Download JHU data and merge it with population data\n","\"\"\"\n","# Download data\n","GenerateTrainingData().download_jhu_data(START_DATE, END_DATE)\n","\n","#Merge population data with downloaded data\n","raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n","pop_data = pd.read_csv('./uszips.csv')\n","pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')\n"]},{"cell_type":"code","source":["\"\"\"\n","Create graph in DGL library based on similarities between locations \n","\"\"\"\n","# Generate location similarity\n","loc_list = list(raw_data['state'].unique())\n","loc_dist_map = {}\n","for each_loc in loc_list:\n","    loc_dist_map[each_loc] = {}\n","    for each_loc2 in loc_list:\n","        lat1 = raw_data[raw_data['state']==each_loc]['latitude'].unique()[0]\n","        lng1 = raw_data[raw_data['state']==each_loc]['longitude'].unique()[0]\n","        pop1 = raw_data[raw_data['state']==each_loc]['population'].unique()[0]\n","        \n","        lat2 = raw_data[raw_data['state']==each_loc2]['latitude'].unique()[0]\n","        lng2 = raw_data[raw_data['state']==each_loc2]['longitude'].unique()[0]\n","        pop2 = raw_data[raw_data['state']==each_loc2]['population'].unique()[0]\n","        \n","        loc_dist_map[each_loc][each_loc2] = gravity_law_commute_dist(lat1, lng1, pop1, lat2, lng2, pop2, r=0.5)\n","\n","#Generate Graph\n","dist_threshold = 18\n","for each_loc in loc_dist_map:\n","    loc_dist_map[each_loc] = {k: v for k, v in sorted(loc_dist_map[each_loc].items(), key=lambda item: item[1], reverse=True)}\n","adj_map = {}\n","for each_loc in loc_dist_map:\n","    adj_map[each_loc] = []\n","    for i, each_loc2 in enumerate(loc_dist_map[each_loc]):\n","        if loc_dist_map[each_loc][each_loc2] > dist_threshold:\n","            if i <= 3:\n","                adj_map[each_loc].append(each_loc2)\n","            else:\n","                break\n","        else:\n","            if i <= 1:\n","                adj_map[each_loc].append(each_loc2)\n","            else:\n","                break\n","rows = []\n","cols = []\n","for each_loc in adj_map:\n","    for each_loc2 in adj_map[each_loc]:\n","        rows.append(loc_list.index(each_loc))\n","        cols.append(loc_list.index(each_loc2))\n","\n","g = dgl.graph((rows, cols))"],"metadata":{"id":"4zWMmuGFfwZb","executionInfo":{"status":"ok","timestamp":1646356442327,"user_tz":360,"elapsed":60809,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Preprocess data by separating it into different groups\n","\"\"\"\n","# Preprocess features\n","confirmed_cases = []\n","death_cases = []\n","static_feat = []\n","\n","for i, each_loc in enumerate(loc_list):\n","    confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","    death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","    static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","    \n","confirmed_cases = np.array(confirmed_cases)\n","death_cases = np.array(death_cases)\n","static_feat = np.array(static_feat)[:, 0, :]\n","\n","\n","# Calculate change in # cases and # deaths from previous day\n","daily_change_in_confirmed = np.concatenate((np.zeros((confirmed_cases.shape[0], 1), dtype=np.float32), np.diff(confirmed_cases)), axis=-1)\n","daily_change_in_deaths = np.concatenate((np.zeros((death_cases.shape[0], 1), dtype=np.float32), np.diff(death_cases)), axis=-1)"],"metadata":{"id":"UKHyvb_fg4HE","executionInfo":{"status":"ok","timestamp":1646356592504,"user_tz":360,"elapsed":636,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put data together into 1 big numpy array\n","\"\"\"\n","dynamic_feat = np.concatenate((np.expand_dims(confirmed_cases, axis=-1),\n","                               np.expand_dims(death_cases, axis=-1),\n","                               np.expand_dims(daily_change_in_confirmed, axis=-1), \n","                               np.expand_dims(daily_change_in_deaths, axis=-1)\n","                               ), axis=-1)"],"metadata":{"id":"dOMvjHswg8Jg","executionInfo":{"status":"ok","timestamp":1646356593936,"user_tz":360,"elapsed":114,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Separate data into training, testing, and validation sets\n","\"\"\"\n","\n","#Split train-test\n","train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n","val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n","test_feat = dynamic_feat[:, -test_window:, :]\n","\n","# Helper function for creating each set of data used\n","def prepare_data(data):\n","  # Data shape n_loc, timestep, n_feat\n","  # Reshape to n_loc, t, history_window*n_feat\n","  n_loc = data.shape[0]\n","  timestep = data.shape[1]\n","  n_feat = data.shape[2]\n","\n","  x = []\n","  y_confirmed = []\n","  y_deaths = []\n","  y_confirmed_change = []\n","  y_deaths_change = []\n","\n","  for i in range(0, timestep, slide_step):\n","      if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","          break\n","      x.append(data[:, i:i+history_window, :].reshape((n_loc, history_window*n_feat)))\n","      y_confirmed.append(data[:, i+history_window:i+history_window+pred_window, 0].reshape((n_loc, pred_window)))\n","      y_deaths.append(data[:, i+history_window:i+history_window+pred_window, 1].reshape((n_loc, pred_window)))\n","      y_confirmed_change.append(data[:, i+history_window:i+history_window+pred_window, 2].reshape((n_loc, pred_window)))\n","      y_deaths_change.append(data[:, i+history_window:i+history_window+pred_window, 3].reshape((n_loc, pred_window)))\n","\n","  # Change shape from (# timesteps, # states/locations, # features) to (# states/locations, # timesteps, # features)\n","  x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\n","  y_confirmed = np.array(y_confirmed, dtype=np.float32).transpose((1, 0, 2))\n","  y_deaths = np.array(y_deaths, dtype=np.float32).transpose((1, 0, 2))\n","  y_confirmed_change = np.array(y_confirmed_change, dtype=np.float32).transpose((1, 0, 2))\n","  y_deaths_change = np.array(y_deaths_change, dtype=np.float32).transpose((1, 0, 2))\n","\n","  return x, y_confirmed, y_deaths, y_confirmed_change, y_deaths_change\n","\n","train_x, train_y_confirmed, train_y_deaths, train_y_confirmed_change, train_y_deaths_change = prepare_data(train_feat)\n","val_x, val_y_confirmed, val_y_deaths, val_y_confirmed_change, val_y_deaths_change = prepare_data(val_feat)\n","test_x, test_y_confirmed, test_y_deaths, test_y_confirmed_change, test_y_deaths_change = prepare_data(test_feat)"],"metadata":{"id":"QE_pnGeYhg8v","executionInfo":{"status":"ok","timestamp":1646356598428,"user_tz":360,"elapsed":148,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Package/organize preprocessed data together into a dictionary called \"preprocessed_data\"\n","\"\"\"\n","training_variables = {'train_x':train_x, \n","                      'train_y_confirmed':train_y_confirmed,\n","                      'train_y_deaths':train_y_deaths,\n","                      'train_y_confirmed_change':train_y_confirmed_change,\n","                      'train_y_deaths_change':train_y_deaths_change\n","                      }\n","\n","validation_variables = {'val_x':val_x, \n","                        'val_y_confirmed':val_y_confirmed,\n","                        'val_y_deaths':val_y_deaths,\n","                        'val_y_confirmed_change':val_y_confirmed_change,\n","                        'val_y_deaths_change':val_y_deaths_change\n","                        }\n","\n","testing_variables = {'test_x':test_x, \n","                     'test_y_confirmed':test_y_confirmed,\n","                     'test_y_deaths':test_y_deaths,\n","                     'test_y_confirmed_change':test_y_confirmed_change,\n","                     'test_y_deaths_change':test_y_deaths_change\n","                     }\n","\n","preprocessed_data = {\n","    'training_variables':training_variables,\n","    'validation_variables':validation_variables,\n","    'testing_variables':testing_variables,\n","    'static_feat':static_feat,\n","    'loc_list':loc_list,\n","    'graph':g\n","}"],"metadata":{"id":"XoBnRNlznfut","executionInfo":{"status":"ok","timestamp":1646356716201,"user_tz":360,"elapsed":119,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put above code into 1 file and 1 function\n","\"\"\"\n","%%writefile preprocess_data_library2.py\n","\n","\"\"\"\n","Import libraries needed\n","\"\"\"\n","from data_downloader import GenerateTrainingData\n","from utils import gravity_law_commute_dist\n","import pickle\n","import pandas as pd\n","import dgl\n","import numpy as np\n","##############################################################################################################################################\n","\"\"\"\n","Declare global variables used to preprocess data\n","\"\"\"\n","START_DATE = '2020-04-12'\n","END_DATE = '2022-01-24'\n","valid_window = 25\n","test_window = 25\n","history_window=6\n","pred_window=15\n","slide_step=5\n","##############################################################################################################################################\n","\n","def get_preprocessed_data():\n","    \"\"\"\n","    Download JHU data and merge it with population data\n","    \"\"\"\n","    # Download data\n","    GenerateTrainingData().download_jhu_data(START_DATE, END_DATE)\n","\n","    #Merge population data with downloaded data\n","    raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n","    pop_data = pd.read_csv('./uszips.csv')\n","    pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","    raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')\n","    ##############################################################################################################################################\n","    \"\"\"\n","    Create graph in DGL library based on similarities between locations \n","    \"\"\"\n","    # Generate location similarity\n","    loc_list = list(raw_data['state'].unique())\n","    loc_dist_map = {}\n","    for each_loc in loc_list:\n","        loc_dist_map[each_loc] = {}\n","        for each_loc2 in loc_list:\n","            lat1 = raw_data[raw_data['state']==each_loc]['latitude'].unique()[0]\n","            lng1 = raw_data[raw_data['state']==each_loc]['longitude'].unique()[0]\n","            pop1 = raw_data[raw_data['state']==each_loc]['population'].unique()[0]\n","            \n","            lat2 = raw_data[raw_data['state']==each_loc2]['latitude'].unique()[0]\n","            lng2 = raw_data[raw_data['state']==each_loc2]['longitude'].unique()[0]\n","            pop2 = raw_data[raw_data['state']==each_loc2]['population'].unique()[0]\n","            \n","            loc_dist_map[each_loc][each_loc2] = gravity_law_commute_dist(lat1, lng1, pop1, lat2, lng2, pop2, r=0.5)\n","\n","    #Generate Graph\n","    dist_threshold = 18\n","    for each_loc in loc_dist_map:\n","        loc_dist_map[each_loc] = {k: v for k, v in sorted(loc_dist_map[each_loc].items(), key=lambda item: item[1], reverse=True)}\n","    adj_map = {}\n","    for each_loc in loc_dist_map:\n","        adj_map[each_loc] = []\n","        for i, each_loc2 in enumerate(loc_dist_map[each_loc]):\n","            if loc_dist_map[each_loc][each_loc2] > dist_threshold:\n","                if i <= 3:\n","                    adj_map[each_loc].append(each_loc2)\n","                else:\n","                    break\n","            else:\n","                if i <= 1:\n","                    adj_map[each_loc].append(each_loc2)\n","                else:\n","                    break\n","    rows = []\n","    cols = []\n","    for each_loc in adj_map:\n","        for each_loc2 in adj_map[each_loc]:\n","            rows.append(loc_list.index(each_loc))\n","            cols.append(loc_list.index(each_loc2))\n","\n","    g = dgl.graph((rows, cols))\n","    ##############################################################################################################################################\n","    \"\"\"\n","    Preprocess data by separating it into different groups\n","    \"\"\"\n","    # Preprocess features\n","    confirmed_cases = []\n","    death_cases = []\n","    static_feat = []\n","\n","    for i, each_loc in enumerate(loc_list):\n","        confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","        death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","        static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","        \n","    confirmed_cases = np.array(confirmed_cases)\n","    death_cases = np.array(death_cases)\n","    static_feat = np.array(static_feat)[:, 0, :]\n","\n","\n","    # Calculate change in # cases and # deaths from previous day\n","    daily_change_in_confirmed = np.concatenate((np.zeros((confirmed_cases.shape[0], 1), dtype=np.float32), np.diff(confirmed_cases)), axis=-1)\n","    daily_change_in_deaths = np.concatenate((np.zeros((death_cases.shape[0], 1), dtype=np.float32), np.diff(death_cases)), axis=-1)\n","    ##############################################################################################################################################\n","    \"\"\"\n","    Put data together into 1 big numpy array\n","    \"\"\"\n","    dynamic_feat = np.concatenate((np.expand_dims(confirmed_cases, axis=-1),\n","                                  np.expand_dims(death_cases, axis=-1),\n","                                  np.expand_dims(daily_change_in_confirmed, axis=-1), \n","                                  np.expand_dims(daily_change_in_deaths, axis=-1)\n","                                  ), axis=-1)\n","\n","    ##############################################################################################################################################\n","    \"\"\"\n","    Separate data into training, testing, and validation sets\n","    \"\"\"\n","\n","    #Split train-test\n","    train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n","    val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n","    test_feat = dynamic_feat[:, -test_window:, :]\n","\n","    # Helper function for creating each set of data used\n","    def prepare_data(data):\n","      # Data shape n_loc, timestep, n_feat\n","      # Reshape to n_loc, t, history_window*n_feat\n","      n_loc = data.shape[0]\n","      timestep = data.shape[1]\n","      n_feat = data.shape[2]\n","\n","      x = []\n","      y_confirmed = []\n","      y_deaths = []\n","      y_confirmed_change = []\n","      y_deaths_change = []\n","\n","      for i in range(0, timestep, slide_step):\n","          if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","              break\n","          x.append(data[:, i:i+history_window, :].reshape((n_loc, history_window*n_feat)))\n","          y_confirmed.append(data[:, i+history_window:i+history_window+pred_window, 0].reshape((n_loc, pred_window)))\n","          y_deaths.append(data[:, i+history_window:i+history_window+pred_window, 1].reshape((n_loc, pred_window)))\n","          y_confirmed_change.append(data[:, i+history_window:i+history_window+pred_window, 2].reshape((n_loc, pred_window)))\n","          y_deaths_change.append(data[:, i+history_window:i+history_window+pred_window, 3].reshape((n_loc, pred_window)))\n","\n","      # Change shape from (# timesteps, # states/locations, # features) to (# states/locations, # timesteps, # features)\n","      x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\n","      y_confirmed = np.array(y_confirmed, dtype=np.float32).transpose((1, 0, 2))\n","      y_deaths = np.array(y_deaths, dtype=np.float32).transpose((1, 0, 2))\n","      y_confirmed_change = np.array(y_confirmed_change, dtype=np.float32).transpose((1, 0, 2))\n","      y_deaths_change = np.array(y_deaths_change, dtype=np.float32).transpose((1, 0, 2))\n","\n","      return x, y_confirmed, y_deaths, y_confirmed_change, y_deaths_change\n","\n","    train_x, train_y_confirmed, train_y_deaths, train_y_confirmed_change, train_y_deaths_change = prepare_data(train_feat)\n","    val_x, val_y_confirmed, val_y_deaths, val_y_confirmed_change, val_y_deaths_change = prepare_data(val_feat)\n","    test_x, test_y_confirmed, test_y_deaths, test_y_confirmed_change, test_y_deaths_change = prepare_data(test_feat)\n","    ##############################################################################################################################################\n","    \"\"\"\n","    Package/organize preprocessed data together into a dictionary called \"preprocessed_data\"\n","    \"\"\"\n","    training_variables = {'train_x':train_x, \n","                          'train_y_confirmed':train_y_confirmed,\n","                          'train_y_deaths':train_y_deaths,\n","                          'train_y_confirmed_change':train_y_confirmed_change,\n","                          'train_y_deaths_change':train_y_deaths_change\n","                          }\n","\n","    validation_variables = {'val_x':val_x, \n","                            'val_y_confirmed':val_y_confirmed,\n","                            'val_y_deaths':val_y_deaths,\n","                            'val_y_confirmed_change':val_y_confirmed_change,\n","                            'val_y_deaths_change':val_y_deaths_change\n","                            }\n","\n","    testing_variables = {'test_x':test_x, \n","                        'test_y_confirmed':test_y_confirmed,\n","                        'test_y_deaths':test_y_deaths,\n","                        'test_y_confirmed_change':test_y_confirmed_change,\n","                        'test_y_deaths_change':test_y_deaths_change\n","                        }\n","\n","    preprocessed_data = {\n","        'training_variables':training_variables,\n","        'validation_variables':validation_variables,\n","        'testing_variables':testing_variables,\n","        'static_feat':static_feat,\n","        'loc_list':loc_list,\n","        'graph':g\n","    }\n","\n","    return preprocessed_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuApCB1YrAc-","executionInfo":{"status":"ok","timestamp":1646356808546,"user_tz":360,"elapsed":399,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"2370fd35-7593-4553-ecff-34ec8023f100"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting preprocess_data_library2.py\n"]}]},{"cell_type":"code","source":["%%writefile model2.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import warnings\n","#warnings.filterwarnings('ignore')\n","\n","class GATLayer(nn.Module):\n","    def __init__(self, g, in_dim, out_dim):\n","        super(GATLayer, self).__init__()\n","        self.g = g\n","        self.fc = nn.Linear(in_dim, out_dim)\n","        self.attn_fc = nn.Linear(2 * out_dim, 1)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        gain = nn.init.calculate_gain('relu')\n","        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n","        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n","\n","    def edge_attention(self, edges):\n","        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n","        a = self.attn_fc(z2)\n","        return {'e': F.leaky_relu(a)}\n","\n","    def message_func(self, edges):\n","        return {'z': edges.src['z'], 'e': edges.data['e']}\n","\n","    def reduce_func(self, nodes):\n","        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n","        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n","        return {'h': h}\n","\n","    def forward(self, h):\n","        z = self.fc(h)\n","        self.g.ndata['z'] = z\n","        self.g.apply_edges(self.edge_attention)\n","        self.g.update_all(self.message_func, self.reduce_func)\n","        return self.g.ndata.pop('h')\n","\n","class MultiHeadGATLayer(nn.Module):\n","    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n","        super(MultiHeadGATLayer, self).__init__()\n","        self.heads = nn.ModuleList()\n","        for i in range(num_heads):\n","            self.heads.append(GATLayer(g, in_dim, out_dim))\n","        self.merge = merge\n","\n","    def forward(self, h):\n","        head_outs = [attn_head(h) for attn_head in self.heads]\n","        if self.merge == 'cat':\n","            return torch.cat(head_outs, dim=1)\n","        else:\n","            return torch.mean(torch.stack(head_outs))\n","        \n","class STAN_v2(nn.Module):\n","    def __init__(self, g, in_dim, hidden_dim1, hidden_dim2, gru_dim, num_heads, pred_window, device):\n","        super(STAN_v2, self).__init__()\n","        self.g = g\n","        \n","        self.layer1 = MultiHeadGATLayer(self.g, in_dim, hidden_dim1, num_heads)\n","        self.layer2 = MultiHeadGATLayer(self.g, hidden_dim1 * num_heads, hidden_dim2, 1)\n","\n","        self.pred_window = pred_window\n","        self.gru = nn.GRUCell(hidden_dim2, gru_dim)\n","    \n","        self.nn_cumulative_confirmed = nn.Linear(gru_dim, pred_window)\n","        self.nn_cumulative_deaths = nn.Linear(gru_dim, pred_window)\n","        \n","        self.hidden_dim2 = hidden_dim2\n","        self.gru_dim = gru_dim\n","        self.device = device\n","\n","    def forward(self, dynamic, h=None):\n","        num_loc, timestep, n_feat = dynamic.size()\n","\n","        if h is None:\n","            h = torch.zeros(1, self.gru_dim).to(self.device)\n","            gain = nn.init.calculate_gain('relu')\n","            nn.init.xavier_normal_(h, gain=gain)  \n","\n","        cumulative_confirmed = []\n","        cumulative_deaths = []\n","\n","\n","        for each_step in range(timestep):        \n","            cur_h = self.layer1(dynamic[:, each_step, :])\n","            cur_h = F.elu(cur_h)\n","            cur_h = self.layer2(cur_h)\n","            cur_h = F.elu(cur_h)\n","            \n","            cur_h = torch.max(cur_h, 0)[0].reshape(1, self.hidden_dim2)\n","            \n","            h = self.gru(cur_h, h)\n","            \n","            predicted_confirmed = self.nn_cumulative_confirmed(h)\n","            predicted_deaths = self.nn_cumulative_deaths(h)\n","            \n","            cumulative_confirmed.append(predicted_confirmed)\n","            cumulative_deaths.append(predicted_deaths)\n","\n","        cumulative_confirmed = torch.stack(cumulative_confirmed).to(self.device).permute(1,0,2)\n","        cumulative_deaths = torch.stack(cumulative_deaths).to(self.device).permute(1,0,2)\n","\n","        return cumulative_confirmed, cumulative_deaths, h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXL0uIdt1ayY","executionInfo":{"status":"ok","timestamp":1646279035355,"user_tz":360,"elapsed":163,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"20347aca-2afb-488d-acbe-14244958cf0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting model2.py\n"]}]}]}