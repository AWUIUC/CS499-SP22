{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_data_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfLwauxEbsJLEGxTFCWh1+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"2. STAN-Without Missing Data\"\n","! pip install epiweeks\n","! pip install haversine\n","! pip install dgl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7yDiZvV2jd2R","executionInfo":{"status":"ok","timestamp":1646268793628,"user_tz":360,"elapsed":26962,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"259d0ccf-b5ba-4b64-eb2f-1440f8e17915"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/2. STAN-Without Missing Data\n","Collecting epiweeks\n","  Downloading epiweeks-2.1.4-py3-none-any.whl (5.9 kB)\n","Installing collected packages: epiweeks\n","Successfully installed epiweeks-2.1.4\n","Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n","Collecting dgl\n","  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 14.8 MB/s \n","\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.5)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n","Installing collected packages: dgl\n","Successfully installed dgl-0.6.1\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Import libraries needed\n","\"\"\"\n","from data_downloader import GenerateTrainingData\n","from utils import gravity_law_commute_dist\n","import pickle\n","import pandas as pd\n","import dgl\n","import numpy as np"],"metadata":{"id":"xyi3R865jOGA","executionInfo":{"status":"ok","timestamp":1646268801503,"user_tz":360,"elapsed":7897,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"709f9f0d-a515-41bc-a40e-25b5aa830e17"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]},{"output_type":"stream","name":"stderr","text":["Using backend: pytorch\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Declare global variables used to preprocess data\n","\"\"\"\n","START_DATE = '2020-04-12'\n","END_DATE = '2022-01-24'\n","valid_window = 25\n","test_window = 25\n","history_window=6\n","pred_window=15\n","slide_step=5"],"metadata":{"id":"3aRrgY3MfbR2","executionInfo":{"status":"ok","timestamp":1646268986579,"user_tz":360,"elapsed":135,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uYf0Z0LfSNa","executionInfo":{"status":"ok","timestamp":1646269041642,"user_tz":360,"elapsed":47029,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"086cac5f-b12c-4135-e312-b455a451ad87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Finish download\n"]}],"source":["\"\"\"\n","Download JHU data and merge it with population data\n","\"\"\"\n","# Download data\n","GenerateTrainingData().download_jhu_data(START_DATE, END_DATE)\n","\n","#Merge population data with downloaded data\n","raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n","pop_data = pd.read_csv('./uszips.csv')\n","pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')\n"]},{"cell_type":"code","source":["\"\"\"\n","Create graph in DGL library based on similarities between locations \n","\"\"\"\n","# Generate location similarity\n","loc_list = list(raw_data['state'].unique())\n","loc_dist_map = {}\n","for each_loc in loc_list:\n","    loc_dist_map[each_loc] = {}\n","    for each_loc2 in loc_list:\n","        lat1 = raw_data[raw_data['state']==each_loc]['latitude'].unique()[0]\n","        lng1 = raw_data[raw_data['state']==each_loc]['longitude'].unique()[0]\n","        pop1 = raw_data[raw_data['state']==each_loc]['population'].unique()[0]\n","        \n","        lat2 = raw_data[raw_data['state']==each_loc2]['latitude'].unique()[0]\n","        lng2 = raw_data[raw_data['state']==each_loc2]['longitude'].unique()[0]\n","        pop2 = raw_data[raw_data['state']==each_loc2]['population'].unique()[0]\n","        \n","        loc_dist_map[each_loc][each_loc2] = gravity_law_commute_dist(lat1, lng1, pop1, lat2, lng2, pop2, r=0.5)\n","\n","#Generate Graph\n","dist_threshold = 18\n","for each_loc in loc_dist_map:\n","    loc_dist_map[each_loc] = {k: v for k, v in sorted(loc_dist_map[each_loc].items(), key=lambda item: item[1], reverse=True)}\n","adj_map = {}\n","for each_loc in loc_dist_map:\n","    adj_map[each_loc] = []\n","    for i, each_loc2 in enumerate(loc_dist_map[each_loc]):\n","        if loc_dist_map[each_loc][each_loc2] > dist_threshold:\n","            if i <= 3:\n","                adj_map[each_loc].append(each_loc2)\n","            else:\n","                break\n","        else:\n","            if i <= 1:\n","                adj_map[each_loc].append(each_loc2)\n","            else:\n","                break\n","rows = []\n","cols = []\n","for each_loc in adj_map:\n","    for each_loc2 in adj_map[each_loc]:\n","        rows.append(loc_list.index(each_loc))\n","        cols.append(loc_list.index(each_loc2))\n","\n","g = dgl.graph((rows, cols))"],"metadata":{"id":"4zWMmuGFfwZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Preprocess data by separating it into different groups\n","\"\"\"\n","# Preprocess features\n","active_cases = []\n","confirmed_cases = []\n","new_cases = []\n","death_cases = []\n","static_feat = []\n","\n","for i, each_loc in enumerate(loc_list):\n","    active_cases.append(raw_data[raw_data['state'] == each_loc]['active'])\n","    confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","    new_cases.append(raw_data[raw_data['state'] == each_loc]['new_cases'])\n","    death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","    static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","    \n","active_cases = np.array(active_cases)\n","confirmed_cases = np.array(confirmed_cases)\n","death_cases = np.array(death_cases)\n","new_cases = np.array(new_cases)\n","static_feat = np.array(static_feat)[:, 0, :]\n","recovered_cases = confirmed_cases - active_cases - death_cases\n","susceptible_cases = np.expand_dims(static_feat[:, 0], -1) - active_cases - recovered_cases\n","\n","# Batch_feat: new_cases(dI), dR, dS\n","#dI = np.array(new_cases)\n","dI = np.concatenate((np.zeros((active_cases.shape[0],1), dtype=np.float32), np.diff(active_cases)), axis=-1)\n","dR = np.concatenate((np.zeros((recovered_cases.shape[0],1), dtype=np.float32), np.diff(recovered_cases)), axis=-1)\n","dS = np.concatenate((np.zeros((susceptible_cases.shape[0],1), dtype=np.float32), np.diff(susceptible_cases)), axis=-1)"],"metadata":{"id":"UKHyvb_fg4HE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Normalize data\n","\"\"\"\n","#Build normalizer\n","normalizer = {'S':{}, 'I':{}, 'R':{}, 'dS':{}, 'dI':{}, 'dR':{}}\n","\n","for i, each_loc in enumerate(loc_list):\n","    normalizer['S'][each_loc] = (np.mean(susceptible_cases[i]), np.std(susceptible_cases[i]))\n","    normalizer['I'][each_loc] = (np.mean(active_cases[i]), np.std(active_cases[i]))\n","    normalizer['R'][each_loc] = (np.mean(recovered_cases[i]), np.std(recovered_cases[i]))\n","    normalizer['dI'][each_loc] = (np.mean(dI[i]), np.std(dI[i]))\n","    normalizer['dR'][each_loc] = (np.mean(dR[i]), np.std(dR[i]))\n","    normalizer['dS'][each_loc] = (np.mean(dS[i]), np.std(dS[i]))\n","\n","dynamic_feat = np.concatenate((np.expand_dims(dI, axis=-1), np.expand_dims(dR, axis=-1), np.expand_dims(dS, axis=-1)), axis=-1)\n","    \n","#Normalize\n","for i, each_loc in enumerate(loc_list):\n","    dynamic_feat[i, :, 0] = (dynamic_feat[i, :, 0] - normalizer['dI'][each_loc][0]) / normalizer['dI'][each_loc][1]\n","    dynamic_feat[i, :, 1] = (dynamic_feat[i, :, 1] - normalizer['dR'][each_loc][0]) / normalizer['dR'][each_loc][1]\n","    dynamic_feat[i, :, 2] = (dynamic_feat[i, :, 2] - normalizer['dS'][each_loc][0]) / normalizer['dS'][each_loc][1]\n","dI_mean = []\n","dI_std = []\n","dR_mean = []\n","dR_std = []\n","for i, each_loc in enumerate(loc_list):\n","    dI_mean.append(normalizer['dI'][each_loc][0])\n","    dR_mean.append(normalizer['dR'][each_loc][0])\n","    dI_std.append(normalizer['dI'][each_loc][1])\n","    dR_std.append(normalizer['dR'][each_loc][1])\n","dI_mean = np.array(dI_mean)\n","dI_std = np.array(dI_std)\n","dR_mean = np.array(dR_mean)\n","dR_std = np.array(dR_std)"],"metadata":{"id":"dOMvjHswg8Jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Separate data into training, testing, and validation sets\n","\"\"\"\n","# Helper function for creating each set of data used\n","def prepare_data(data, sum_I, sum_R, history_window=5, pred_window=15, slide_step=5):\n","    # Data shape n_loc, timestep, n_feat\n","    # Reshape to n_loc, t, history_window*n_feat\n","    n_loc = data.shape[0]\n","    timestep = data.shape[1]\n","    n_feat = data.shape[2]\n","    \n","    x = []\n","    y_I = []\n","    y_R = []\n","    last_I = []\n","    last_R = []\n","    concat_I = []\n","    concat_R = []\n","    for i in range(0, timestep, slide_step):\n","        if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","            break\n","        x.append(data[:, i:i+history_window, :].reshape((n_loc, history_window*n_feat)))\n","        \n","        concat_I.append(data[:, i+history_window-1, 0])\n","        concat_R.append(data[:, i+history_window-1, 1])\n","        last_I.append(sum_I[:, i+history_window-1])\n","        last_R.append(sum_R[:, i+history_window-1])\n","\n","        y_I.append(data[:, i+history_window:i+history_window+pred_window, 0])\n","        y_R.append(data[:, i+history_window:i+history_window+pred_window, 1])\n","\n","    x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\n","    last_I = np.array(last_I, dtype=np.float32).transpose((1, 0))\n","    last_R = np.array(last_R, dtype=np.float32).transpose((1, 0))\n","    concat_I = np.array(concat_I, dtype=np.float32).transpose((1, 0))\n","    concat_R = np.array(concat_R, dtype=np.float32).transpose((1, 0))\n","    y_I = np.array(y_I, dtype=np.float32).transpose((1, 0, 2))\n","    y_R = np.array(y_R, dtype=np.float32).transpose((1, 0, 2))\n","    return x, last_I, last_R, concat_I, concat_R, y_I, y_R\n","\n","#Split train-test\n","train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n","val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n","test_feat = dynamic_feat[:, -test_window:, :]\n","\n","train_x, train_I, train_R, train_cI, train_cR, train_yI, train_yR = prepare_data(train_feat, active_cases[:, :-valid_window-test_window], recovered_cases[:, :-valid_window-test_window], history_window, pred_window, slide_step)\n","val_x, val_I, val_R, val_cI, val_cR, val_yI, val_yR = prepare_data(val_feat, active_cases[:, -valid_window-test_window:-test_window], recovered_cases[:, -valid_window-test_window:-test_window], history_window, pred_window, slide_step)\n","test_x, test_I, test_R, test_cI, test_cR, test_yI, test_yR = prepare_data(test_feat, active_cases[:, -test_window:], recovered_cases[:, -test_window:], history_window, pred_window, slide_step)"],"metadata":{"id":"QE_pnGeYhg8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Package/organize preprocessed data together into a dictionary called \"preprocessed_data\"\n","\"\"\"\n","training_variables = {'train_x':train_x, 'train_I':train_I, 'train_R':train_R, \n","                      'train_cI':train_cI, 'train_cR':train_cR, \n","                      'train_yI':train_yI, 'train_yR':train_yR}\n","validation_variables = {'val_x':val_x, 'val_I':val_I, 'val_R':val_R, \n","                        'val_cI':val_cI, 'val_cR':val_cR,\n","                        'val_yI':val_yI, 'val_yR':val_yR}\n","testing_variables = {'test_x':test_x, 'test_I':test_I, 'test_R':test_R, \n","                     'test_cI':test_cI, 'test_cR':test_cR,\n","                     'test_yI':test_yI, 'test_yR':test_yR}\n","normalization_variables = {'dI_mean':dI_mean, 'dI_std':dI_std, 'dR_mean':dR_mean, 'dR_std':dR_std}\n","\n","preprocessed_data = {\n","    'training_variables':training_variables,\n","    'validation_variables':validation_variables,\n","    'testing_variables':testing_variables,\n","    'normalization_variables':normalization_variables,\n","    'static_feat':static_feat,\n","    'loc_list':loc_list,\n","    'graph':g,\n","    'active_cases':active_cases\n","}"],"metadata":{"id":"XoBnRNlznfut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Put above code into 1 file and 1 function\n","\"\"\"\n","\n","%%writefile preprocess_data_library.py\n","\n","\"\"\"\n","Import libraries needed\n","\"\"\"\n","from data_downloader import GenerateTrainingData\n","from utils import gravity_law_commute_dist\n","import pickle\n","import pandas as pd\n","import dgl\n","import numpy as np\n","\n","\"\"\"\n","Declare global variables used to preprocess data\n","\"\"\"\n","START_DATE = '2020-05-01'\n","END_DATE = '2020-12-01'\n","valid_window = 25\n","test_window = 25\n","history_window=6\n","pred_window=15\n","slide_step=5\n","\n","def get_preprocessed_data():\n","  \"\"\"\n","  Download JHU data and merge it with population data\n","  \"\"\"\n","  # Download data\n","  GenerateTrainingData().download_jhu_data(START_DATE, END_DATE)\n","\n","  #Merge population data with downloaded data\n","  raw_data = pickle.load(open('./data/state_covid_data.pickle','rb'))\n","  pop_data = pd.read_csv('./uszips.csv')\n","  pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","  raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')\n","\n","  \"\"\"\n","  Create graph in DGL library based on similarities between locations \n","  \"\"\"\n","  # Generate location similarity\n","  loc_list = list(raw_data['state'].unique())\n","  loc_dist_map = {}\n","  for each_loc in loc_list:\n","      loc_dist_map[each_loc] = {}\n","      for each_loc2 in loc_list:\n","          lat1 = raw_data[raw_data['state']==each_loc]['latitude'].unique()[0]\n","          lng1 = raw_data[raw_data['state']==each_loc]['longitude'].unique()[0]\n","          pop1 = raw_data[raw_data['state']==each_loc]['population'].unique()[0]\n","          \n","          lat2 = raw_data[raw_data['state']==each_loc2]['latitude'].unique()[0]\n","          lng2 = raw_data[raw_data['state']==each_loc2]['longitude'].unique()[0]\n","          pop2 = raw_data[raw_data['state']==each_loc2]['population'].unique()[0]\n","          \n","          loc_dist_map[each_loc][each_loc2] = gravity_law_commute_dist(lat1, lng1, pop1, lat2, lng2, pop2, r=0.5)\n","\n","  #Generate Graph\n","  dist_threshold = 18\n","  for each_loc in loc_dist_map:\n","      loc_dist_map[each_loc] = {k: v for k, v in sorted(loc_dist_map[each_loc].items(), key=lambda item: item[1], reverse=True)}\n","  adj_map = {}\n","  for each_loc in loc_dist_map:\n","      adj_map[each_loc] = []\n","      for i, each_loc2 in enumerate(loc_dist_map[each_loc]):\n","          if loc_dist_map[each_loc][each_loc2] > dist_threshold:\n","              if i <= 3:\n","                  adj_map[each_loc].append(each_loc2)\n","              else:\n","                  break\n","          else:\n","              if i <= 1:\n","                  adj_map[each_loc].append(each_loc2)\n","              else:\n","                  break\n","  rows = []\n","  cols = []\n","  for each_loc in adj_map:\n","      for each_loc2 in adj_map[each_loc]:\n","          rows.append(loc_list.index(each_loc))\n","          cols.append(loc_list.index(each_loc2))\n","\n","  g = dgl.graph((rows, cols))\n","\n","  \"\"\"\n","  Preprocess data by separating it into different groups\n","  \"\"\"\n","  # Preprocess features\n","  active_cases = []\n","  confirmed_cases = []\n","  new_cases = []\n","  death_cases = []\n","  static_feat = []\n","\n","  for i, each_loc in enumerate(loc_list):\n","      active_cases.append(raw_data[raw_data['state'] == each_loc]['active'])\n","      confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","      new_cases.append(raw_data[raw_data['state'] == each_loc]['new_cases'])\n","      death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","      static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","      \n","  active_cases = np.array(active_cases)\n","  confirmed_cases = np.array(confirmed_cases)\n","  death_cases = np.array(death_cases)\n","  new_cases = np.array(new_cases)\n","  static_feat = np.array(static_feat)[:, 0, :]\n","  recovered_cases = confirmed_cases - active_cases - death_cases\n","  susceptible_cases = np.expand_dims(static_feat[:, 0], -1) - active_cases - recovered_cases\n","\n","  # Batch_feat: new_cases(dI), dR, dS\n","  #dI = np.array(new_cases)\n","  dI = np.concatenate((np.zeros((active_cases.shape[0],1), dtype=np.float32), np.diff(active_cases)), axis=-1)\n","  dR = np.concatenate((np.zeros((recovered_cases.shape[0],1), dtype=np.float32), np.diff(recovered_cases)), axis=-1)\n","  dS = np.concatenate((np.zeros((susceptible_cases.shape[0],1), dtype=np.float32), np.diff(susceptible_cases)), axis=-1)\n","\n","  \"\"\"\n","  Normalize data\n","  \"\"\"\n","  #Build normalizer\n","  normalizer = {'S':{}, 'I':{}, 'R':{}, 'dS':{}, 'dI':{}, 'dR':{}}\n","\n","  for i, each_loc in enumerate(loc_list):\n","      normalizer['S'][each_loc] = (np.mean(susceptible_cases[i]), np.std(susceptible_cases[i]))\n","      normalizer['I'][each_loc] = (np.mean(active_cases[i]), np.std(active_cases[i]))\n","      normalizer['R'][each_loc] = (np.mean(recovered_cases[i]), np.std(recovered_cases[i]))\n","      normalizer['dI'][each_loc] = (np.mean(dI[i]), np.std(dI[i]))\n","      normalizer['dR'][each_loc] = (np.mean(dR[i]), np.std(dR[i]))\n","      normalizer['dS'][each_loc] = (np.mean(dS[i]), np.std(dS[i]))\n","\n","  dynamic_feat = np.concatenate((np.expand_dims(dI, axis=-1), np.expand_dims(dR, axis=-1), np.expand_dims(dS, axis=-1)), axis=-1)\n","      \n","  #Normalize\n","  for i, each_loc in enumerate(loc_list):\n","      dynamic_feat[i, :, 0] = (dynamic_feat[i, :, 0] - normalizer['dI'][each_loc][0]) / normalizer['dI'][each_loc][1]\n","      dynamic_feat[i, :, 1] = (dynamic_feat[i, :, 1] - normalizer['dR'][each_loc][0]) / normalizer['dR'][each_loc][1]\n","      dynamic_feat[i, :, 2] = (dynamic_feat[i, :, 2] - normalizer['dS'][each_loc][0]) / normalizer['dS'][each_loc][1]\n","  dI_mean = []\n","  dI_std = []\n","  dR_mean = []\n","  dR_std = []\n","  for i, each_loc in enumerate(loc_list):\n","      dI_mean.append(normalizer['dI'][each_loc][0])\n","      dR_mean.append(normalizer['dR'][each_loc][0])\n","      dI_std.append(normalizer['dI'][each_loc][1])\n","      dR_std.append(normalizer['dR'][each_loc][1])\n","  dI_mean = np.array(dI_mean)\n","  dI_std = np.array(dI_std)\n","  dR_mean = np.array(dR_mean)\n","  dR_std = np.array(dR_std)\n","\n","  \"\"\"\n","  Separate data into training, testing, and validation sets\n","  \"\"\"\n","  # Helper function for creating each set of data used\n","  def prepare_data(data, sum_I, sum_R, history_window=5, pred_window=15, slide_step=5):\n","      # Data shape n_loc, timestep, n_feat\n","      # Reshape to n_loc, t, history_window*n_feat\n","      n_loc = data.shape[0]\n","      timestep = data.shape[1]\n","      n_feat = data.shape[2]\n","      \n","      x = []\n","      y_I = []\n","      y_R = []\n","      last_I = []\n","      last_R = []\n","      concat_I = []\n","      concat_R = []\n","      for i in range(0, timestep, slide_step):\n","          if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","              break\n","          x.append(data[:, i:i+history_window, :].reshape((n_loc, history_window*n_feat)))\n","          \n","          concat_I.append(data[:, i+history_window-1, 0])\n","          concat_R.append(data[:, i+history_window-1, 1])\n","          last_I.append(sum_I[:, i+history_window-1])\n","          last_R.append(sum_R[:, i+history_window-1])\n","\n","          y_I.append(data[:, i+history_window:i+history_window+pred_window, 0])\n","          y_R.append(data[:, i+history_window:i+history_window+pred_window, 1])\n","\n","      x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\n","      last_I = np.array(last_I, dtype=np.float32).transpose((1, 0))\n","      last_R = np.array(last_R, dtype=np.float32).transpose((1, 0))\n","      concat_I = np.array(concat_I, dtype=np.float32).transpose((1, 0))\n","      concat_R = np.array(concat_R, dtype=np.float32).transpose((1, 0))\n","      y_I = np.array(y_I, dtype=np.float32).transpose((1, 0, 2))\n","      y_R = np.array(y_R, dtype=np.float32).transpose((1, 0, 2))\n","      return x, last_I, last_R, concat_I, concat_R, y_I, y_R\n","\n","  #Split train-test\n","  train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n","  val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n","  test_feat = dynamic_feat[:, -test_window:, :]\n","\n","  train_x, train_I, train_R, train_cI, train_cR, train_yI, train_yR = prepare_data(train_feat, active_cases[:, :-valid_window-test_window], recovered_cases[:, :-valid_window-test_window], history_window, pred_window, slide_step)\n","  val_x, val_I, val_R, val_cI, val_cR, val_yI, val_yR = prepare_data(val_feat, active_cases[:, -valid_window-test_window:-test_window], recovered_cases[:, -valid_window-test_window:-test_window], history_window, pred_window, slide_step)\n","  test_x, test_I, test_R, test_cI, test_cR, test_yI, test_yR = prepare_data(test_feat, active_cases[:, -test_window:], recovered_cases[:, -test_window:], history_window, pred_window, slide_step)\n","\n","  \"\"\"\n","  Package/organize preprocessed data together into a dictionary called \"preprocessed_data\"\n","  \"\"\"\n","  training_variables = {'train_x':train_x, 'train_I':train_I, 'train_R':train_R, \n","                        'train_cI':train_cI, 'train_cR':train_cR, \n","                        'train_yI':train_yI, 'train_yR':train_yR}\n","  validation_variables = {'val_x':val_x, 'val_I':val_I, 'val_R':val_R, \n","                          'val_cI':val_cI, 'val_cR':val_cR,\n","                          'val_yI':val_yI, 'val_yR':val_yR}\n","  testing_variables = {'test_x':test_x, 'test_I':test_I, 'test_R':test_R, \n","                      'test_cI':test_cI, 'test_cR':test_cR,\n","                      'test_yI':test_yI, 'test_yR':test_yR}\n","  normalization_variables = {'dI_mean':dI_mean, 'dI_std':dI_std, 'dR_mean':dR_mean, 'dR_std':dR_std}\n","\n","  preprocessed_data = {\n","      'training_variables':training_variables,\n","      'validation_variables':validation_variables,\n","      'testing_variables':testing_variables,\n","      'normalization_variables':normalization_variables,\n","      'static_feat':static_feat,\n","      'loc_list':loc_list,\n","      'graph':g,\n","      'active_cases':active_cases\n","  }\n","\n","  return preprocessed_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuApCB1YrAc-","executionInfo":{"status":"ok","timestamp":1646247873068,"user_tz":360,"elapsed":138,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"df1f3fa9-dbff-48b7-90d9-00f95482b158"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting preprocess_data_library.py\n"]}]}]}