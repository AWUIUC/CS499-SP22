{"cells":[{"cell_type":"code","source":["\"\"\"\n","Originally from train_stan2.ipynb\n","train_stan2.ipynb --> train_stan4.ipynb: changed model to only predict future # confirmed cases (as oppsoed to both # confirmed cases and # deaths)\n","On initial look (when epochs = 50) it looks like model performs equally compared to without changes, but when I trained for more epochs I discovered that\n","the number of predicted cases gradually became more accurate and larger (but still orders of magnitude smaller than what it should be)\n","\n","Summary: \n","Tried to only predict # confirmed cases (2 output linear layers --> 1 output linear layer) and kept # epochs same (at 50) --> didn't change results too much\n","Increased epochs --> changed results significantly, but not quickly enough\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"NJvRzO7BMQvc","executionInfo":{"status":"ok","timestamp":1646335165279,"user_tz":360,"elapsed":188,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"fd99914d-20f6-409a-98c7-0859dbca553d"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nOriginally from train_stan2.ipynb\\n'"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"3. Experiments\"/\"2. STAN-Without Missing Data\"\n","! pip install epiweeks\n","! pip install haversine\n","! pip install dgl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cCASTSUr9A5","executionInfo":{"status":"ok","timestamp":1646335174993,"user_tz":360,"elapsed":9557,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"127572ec-a1ae-400f-cf48-3e85cdb2cf6d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/3. Experiments/2. STAN-Without Missing Data\n","Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n","Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.6.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.5)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Import libraries needed\n","\"\"\"\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from preprocess_data_library2 import get_preprocessed_data\n","from model4 import STAN_v4\n","from torch import nn"],"metadata":{"id":"hx5w_ew_rw6N","executionInfo":{"status":"ok","timestamp":1646335174995,"user_tz":360,"elapsed":19,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Get preprocessed data and unpackage variables needed during training/testing/validation\n","\n","preprocessed_data = get_preprocessed_data()\n","\n","training_variables = preprocessed_data['training_variables']\n","validation_variables = preprocessed_data['validation_variables']\n","testing_variables = preprocessed_data['testing_variables']\n","\n","static_feat = preprocessed_data['static_feat']\n","loc_list = preprocessed_data['loc_list']\n","g = preprocessed_data['graph']\n","\n","train_x = training_variables['train_x']\n","train_y_confirmed = training_variables['train_y_confirmed']\n","train_y_deaths = training_variables['train_y_deaths']\n","\n","val_x = validation_variables['val_x']\n","val_y_confirmed = validation_variables['val_y_confirmed']\n","val_y_deaths = validation_variables['val_y_deaths']\n","\n","test_x = testing_variables['test_x']\n","test_y_confirmed = testing_variables['test_y_confirmed']\n","test_y_deaths = testing_variables['test_y_deaths']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-lShqX5vCW9","executionInfo":{"status":"ok","timestamp":1646335319027,"user_tz":360,"elapsed":144047,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"93a8affa-8183-4df1-b644-924799f7ad4f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Finish download\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"scrolled":true,"id":"NiY1eYyGfODt","executionInfo":{"status":"ok","timestamp":1646335319181,"user_tz":360,"elapsed":29,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[],"source":["history_window = 6 # Redeclared from preprocess_data_library\n","pred_window = 15 # Redeclared from preprocess_data_library\n","slide_step = 5 # Redeclared from preprocess_data_library\n","\n","#Build STAN model\n","\n","in_dim = 4*history_window\n","hidden_dim1 = 32\n","hidden_dim2 = 32\n","gru_dim = 32\n","num_heads = 1\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","g = g.to(device)\n","model = STAN_v4(g, in_dim, hidden_dim1, hidden_dim2, gru_dim, num_heads, pred_window, device).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n","criterion = nn.MSELoss()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"TNuoJxRbfODu","outputId":"ae808a74-801b-4f24-acd3-3a2b1cfcd30c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646335319183,"user_tz":360,"elapsed":29,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["STAN_v4(\n","  (layer1): MultiHeadGATLayer(\n","    (heads): ModuleList(\n","      (0): GATLayer(\n","        (fc): Linear(in_features=24, out_features=32, bias=True)\n","        (attn_fc): Linear(in_features=64, out_features=1, bias=True)\n","      )\n","    )\n","  )\n","  (layer2): MultiHeadGATLayer(\n","    (heads): ModuleList(\n","      (0): GATLayer(\n","        (fc): Linear(in_features=32, out_features=32, bias=True)\n","        (attn_fc): Linear(in_features=64, out_features=1, bias=True)\n","      )\n","    )\n","  )\n","  (gru): GRUCell(32, 32)\n","  (nn_cumulative_confirmed): Linear(in_features=32, out_features=15, bias=True)\n",")"]},"metadata":{},"execution_count":10}],"source":["model"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"7ayb-cJsfODu","executionInfo":{"status":"ok","timestamp":1646335319185,"user_tz":360,"elapsed":26,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[],"source":["train_x = torch.tensor(train_x).to(device)\n","train_y_confirmed = torch.tensor(train_y_confirmed).to(device)\n","train_y_deaths = torch.tensor(train_y_deaths).to(device)\n","\n","val_x = torch.tensor(val_x).to(device)\n","val_y_confirmed= torch.tensor(val_y_confirmed).to(device)\n","val_y_deaths = torch.tensor(val_y_deaths).to(device)\n","\n","test_x = torch.tensor(test_x).to(device)\n","test_y_confirmed = torch.tensor(test_y_confirmed).to(device)\n","test_y_deaths = torch.tensor(test_y_deaths).to(device)\n","\n","N = torch.tensor(static_feat[:, 0], dtype=torch.float32).to(device).unsqueeze(-1)"]},{"cell_type":"code","execution_count":21,"metadata":{"scrolled":true,"id":"aeK2cfTMfODv","outputId":"1b5fbb19-04b0-42a0-8f59-190d3a928e28","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646336074802,"user_tz":360,"elapsed":328616,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1, 15])) that is different to the input size (torch.Size([15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["-----Save best model-----\n","Epoch 0, Loss 160352468992.00, Val loss 685776961536.00\n","-----Save best model-----\n","Epoch 1, Loss 160352403456.00, Val loss 685776896000.00\n","-----Save best model-----\n","Epoch 2, Loss 160352305152.00, Val loss 685776437248.00\n","-----Save best model-----\n","Epoch 3, Loss 160352174080.00, Val loss 685776044032.00\n","-----Save best model-----\n","Epoch 4, Loss 160352026624.00, Val loss 685775454208.00\n","-----Save best model-----\n","Epoch 5, Loss 160352075776.00, Val loss 685775323136.00\n","-----Save best model-----\n","Epoch 6, Loss 160351715328.00, Val loss 685775126528.00\n","-----Save best model-----\n","Epoch 7, Loss 160351387648.00, Val loss 685774340096.00\n","-----Save best model-----\n","Epoch 8, Loss 160351338496.00, Val loss 685773946880.00\n","-----Save best model-----\n","Epoch 9, Loss 160351207424.00, Val loss 685773619200.00\n","-----Save best model-----\n","Epoch 10, Loss 160351305728.00, Val loss 685773029376.00\n","-----Save best model-----\n","Epoch 11, Loss 160350928896.00, Val loss 685772439552.00\n","Epoch 12, Loss 160350879744.00, Val loss 685773029376.00\n","-----Save best model-----\n","Epoch 13, Loss 160350781440.00, Val loss 685772046336.00\n","-----Save best model-----\n","Epoch 14, Loss 160350486528.00, Val loss 685771849728.00\n","-----Save best model-----\n","Epoch 15, Loss 160350289920.00, Val loss 685771390976.00\n","-----Save best model-----\n","Epoch 16, Loss 160350142464.00, Val loss 685771259904.00\n","-----Save best model-----\n","Epoch 17, Loss 160349995008.00, Val loss 685770473472.00\n","-----Save best model-----\n","Epoch 18, Loss 160350011392.00, Val loss 685769752576.00\n","-----Save best model-----\n","Epoch 19, Loss 160349765632.00, Val loss 685769490432.00\n","-----Save best model-----\n","Epoch 20, Loss 160349601792.00, Val loss 685769097216.00\n","-----Save best model-----\n","Epoch 21, Loss 160349306880.00, Val loss 685768704000.00\n","Epoch 22, Loss 160349224960.00, Val loss 685768704000.00\n","-----Save best model-----\n","Epoch 23, Loss 160349061120.00, Val loss 685767983104.00\n","-----Save best model-----\n","Epoch 24, Loss 160348913664.00, Val loss 685767786496.00\n","-----Save best model-----\n","Epoch 25, Loss 160348864512.00, Val loss 685767262208.00\n","-----Save best model-----\n","Epoch 26, Loss 160348536832.00, Val loss 685766868992.00\n","-----Save best model-----\n","Epoch 27, Loss 160348438528.00, Val loss 685766213632.00\n","-----Save best model-----\n","Epoch 28, Loss 160348438528.00, Val loss 685765885952.00\n","-----Save best model-----\n","Epoch 29, Loss 160348127232.00, Val loss 685765230592.00\n","-----Save best model-----\n","Epoch 30, Loss 160348127232.00, Val loss 685765099520.00\n","-----Save best model-----\n","Epoch 31, Loss 160348078080.00, Val loss 685764902912.00\n","-----Save best model-----\n","Epoch 32, Loss 160347750400.00, Val loss 685764444160.00\n","-----Save best model-----\n","Epoch 33, Loss 160347537408.00, Val loss 685764050944.00\n","-----Save best model-----\n","Epoch 34, Loss 160347275264.00, Val loss 685763788800.00\n","-----Save best model-----\n","Epoch 35, Loss 160347144192.00, Val loss 685763264512.00\n","-----Save best model-----\n","Epoch 36, Loss 160347062272.00, Val loss 685762609152.00\n","Epoch 37, Loss 160346898432.00, Val loss 685762674688.00\n","-----Save best model-----\n","Epoch 38, Loss 160346652672.00, Val loss 685762347008.00\n","-----Save best model-----\n","Epoch 39, Loss 160346652672.00, Val loss 685761953792.00\n","-----Save best model-----\n","Epoch 40, Loss 160346488832.00, Val loss 685760970752.00\n","-----Save best model-----\n","Epoch 41, Loss 160346308608.00, Val loss 685760577536.00\n","-----Save best model-----\n","Epoch 42, Loss 160346193920.00, Val loss 685760512000.00\n","-----Save best model-----\n","Epoch 43, Loss 160346210304.00, Val loss 685760118784.00\n","-----Save best model-----\n","Epoch 44, Loss 160345849856.00, Val loss 685759660032.00\n","-----Save best model-----\n","Epoch 45, Loss 160345636864.00, Val loss 685759266816.00\n","-----Save best model-----\n","Epoch 46, Loss 160345571328.00, Val loss 685758611456.00\n","-----Save best model-----\n","Epoch 47, Loss 160345473024.00, Val loss 685758349312.00\n","-----Save best model-----\n","Epoch 48, Loss 160345210880.00, Val loss 685758283776.00\n","-----Save best model-----\n","Epoch 49, Loss 160345227264.00, Val loss 685757431808.00\n","Epoch 50, Loss 160344866816.00, Val loss 685757562880.00\n","Epoch 51, Loss 160344997888.00, Val loss 685757497344.00\n","-----Save best model-----\n","Epoch 52, Loss 160344506368.00, Val loss 685756514304.00\n","-----Save best model-----\n","Epoch 53, Loss 160344309760.00, Val loss 685756121088.00\n","-----Save best model-----\n","Epoch 54, Loss 160344358912.00, Val loss 685755203584.00\n","Epoch 55, Loss 160344047616.00, Val loss 685755662336.00\n","-----Save best model-----\n","Epoch 56, Loss 160343949312.00, Val loss 685754679296.00\n","Epoch 57, Loss 160343900160.00, Val loss 685754679296.00\n","-----Save best model-----\n","Epoch 58, Loss 160343867392.00, Val loss 685754023936.00\n","-----Save best model-----\n","Epoch 59, Loss 160343752704.00, Val loss 685753696256.00\n","-----Save best model-----\n","Epoch 60, Loss 160343425024.00, Val loss 685753237504.00\n","Epoch 61, Loss 160343212032.00, Val loss 685753434112.00\n","-----Save best model-----\n","Epoch 62, Loss 160343179264.00, Val loss 685752516608.00\n","-----Save best model-----\n","Epoch 63, Loss 160342884352.00, Val loss 685751926784.00\n","-----Save best model-----\n","Epoch 64, Loss 160342835200.00, Val loss 685751599104.00\n","Epoch 65, Loss 160342573056.00, Val loss 685751795712.00\n","-----Save best model-----\n","Epoch 66, Loss 160342556672.00, Val loss 685751074816.00\n","-----Save best model-----\n","Epoch 67, Loss 160342343680.00, Val loss 685750616064.00\n","-----Save best model-----\n","Epoch 68, Loss 160342327296.00, Val loss 685749436416.00\n","Epoch 69, Loss 160342114304.00, Val loss 685749764096.00\n","-----Save best model-----\n","Epoch 70, Loss 160341770240.00, Val loss 685749239808.00\n","-----Save best model-----\n","Epoch 71, Loss 160341573632.00, Val loss 685748387840.00\n","Epoch 72, Loss 160341737472.00, Val loss 685748781056.00\n","-----Save best model-----\n","Epoch 73, Loss 160341475328.00, Val loss 685747470336.00\n","Epoch 74, Loss 160341393408.00, Val loss 685747470336.00\n","-----Save best model-----\n","Epoch 75, Loss 160341032960.00, Val loss 685747339264.00\n","-----Save best model-----\n","Epoch 76, Loss 160341049344.00, Val loss 685747142656.00\n","-----Save best model-----\n","Epoch 77, Loss 160340803584.00, Val loss 685746814976.00\n","-----Save best model-----\n","Epoch 78, Loss 160340606976.00, Val loss 685746028544.00\n","-----Save best model-----\n","Epoch 79, Loss 160340541440.00, Val loss 685745831936.00\n","-----Save best model-----\n","Epoch 80, Loss 160340508672.00, Val loss 685745307648.00\n","-----Save best model-----\n","Epoch 81, Loss 160340246528.00, Val loss 685744979968.00\n","Epoch 82, Loss 160340197376.00, Val loss 685745307648.00\n","-----Save best model-----\n","Epoch 83, Loss 160339935232.00, Val loss 685743931392.00\n","-----Save best model-----\n","Epoch 84, Loss 160339623936.00, Val loss 685743800320.00\n","Epoch 85, Loss 160339558400.00, Val loss 685743800320.00\n","-----Save best model-----\n","Epoch 86, Loss 160339509248.00, Val loss 685742817280.00\n","Epoch 87, Loss 160339394560.00, Val loss 685743210496.00\n","-----Save best model-----\n","Epoch 88, Loss 160339165184.00, Val loss 685742424064.00\n","-----Save best model-----\n","Epoch 89, Loss 160338935808.00, Val loss 685742030848.00\n","-----Save best model-----\n","Epoch 90, Loss 160338821120.00, Val loss 685740851200.00\n","-----Save best model-----\n","Epoch 91, Loss 160338886656.00, Val loss 685740720128.00\n","-----Save best model-----\n","Epoch 92, Loss 160338509824.00, Val loss 685739933696.00\n","Epoch 93, Loss 160338362368.00, Val loss 685740457984.00\n","-----Save best model-----\n","Epoch 94, Loss 160338149376.00, Val loss 685739606016.00\n","Epoch 95, Loss 160338149376.00, Val loss 685739802624.00\n","-----Save best model-----\n","Epoch 96, Loss 160337854464.00, Val loss 685738622976.00\n","Epoch 97, Loss 160337805312.00, Val loss 685738622976.00\n","-----Save best model-----\n","Epoch 98, Loss 160337657856.00, Val loss 685737836544.00\n","Epoch 99, Loss 160337379328.00, Val loss 685737967616.00\n","Epoch 100, Loss 160337002496.00, Val loss 685737967616.00\n","-----Save best model-----\n","Epoch 101, Loss 160337117184.00, Val loss 685736984576.00\n","-----Save best model-----\n","Epoch 102, Loss 160337002496.00, Val loss 685736656896.00\n","Epoch 103, Loss 160336805888.00, Val loss 685736853504.00\n","-----Save best model-----\n","Epoch 104, Loss 160336707584.00, Val loss 685735346176.00\n","-----Save best model-----\n","Epoch 105, Loss 160336330752.00, Val loss 685735084032.00\n","Epoch 106, Loss 160336412672.00, Val loss 685735280640.00\n","-----Save best model-----\n","Epoch 107, Loss 160336363520.00, Val loss 685734821888.00\n","-----Save best model-----\n","Epoch 108, Loss 160336166912.00, Val loss 685734559744.00\n","-----Save best model-----\n","Epoch 109, Loss 160336068608.00, Val loss 685733052416.00\n","-----Save best model-----\n","Epoch 110, Loss 160335626240.00, Val loss 685732986880.00\n","Epoch 111, Loss 160335773696.00, Val loss 685733773312.00\n","-----Save best model-----\n","Epoch 112, Loss 160335511552.00, Val loss 685732790272.00\n","-----Save best model-----\n","Epoch 113, Loss 160335446016.00, Val loss 685732134912.00\n","-----Save best model-----\n","Epoch 114, Loss 160335314944.00, Val loss 685731938304.00\n","-----Save best model-----\n","Epoch 115, Loss 160334905344.00, Val loss 685731545088.00\n","Epoch 116, Loss 160334938112.00, Val loss 685731545088.00\n","-----Save best model-----\n","Epoch 117, Loss 160334823424.00, Val loss 685730037760.00\n","Epoch 118, Loss 160334430208.00, Val loss 685730562048.00\n","Epoch 119, Loss 160334299136.00, Val loss 685730234368.00\n","-----Save best model-----\n","Epoch 120, Loss 160334168064.00, Val loss 685729644544.00\n","-----Save best model-----\n","Epoch 121, Loss 160334069760.00, Val loss 685729251328.00\n","-----Save best model-----\n","Epoch 122, Loss 160333856768.00, Val loss 685728989184.00\n","-----Save best model-----\n","Epoch 123, Loss 160334053376.00, Val loss 685728530432.00\n","-----Save best model-----\n","Epoch 124, Loss 160333824000.00, Val loss 685727154176.00\n","Epoch 125, Loss 160333873152.00, Val loss 685727350784.00\n","-----Save best model-----\n","Epoch 126, Loss 160333398016.00, Val loss 685726957568.00\n","-----Save best model-----\n","Epoch 127, Loss 160333283328.00, Val loss 685725843456.00\n","Epoch 128, Loss 160333037568.00, Val loss 685726105600.00\n","Epoch 129, Loss 160332644352.00, Val loss 685725974528.00\n","-----Save best model-----\n","Epoch 130, Loss 160332578816.00, Val loss 685725384704.00\n","-----Save best model-----\n","Epoch 131, Loss 160332578816.00, Val loss 685725319168.00\n","-----Save best model-----\n","Epoch 132, Loss 160332480512.00, Val loss 685724401664.00\n","-----Save best model-----\n","Epoch 133, Loss 160332447744.00, Val loss 685724205056.00\n","Epoch 134, Loss 160332152832.00, Val loss 685724336128.00\n","-----Save best model-----\n","Epoch 135, Loss 160331988992.00, Val loss 685723877376.00\n","-----Save best model-----\n","Epoch 136, Loss 160331694080.00, Val loss 685722173440.00\n","Epoch 137, Loss 160331710464.00, Val loss 685722501120.00\n","Epoch 138, Loss 160331530240.00, Val loss 685723025408.00\n","-----Save best model-----\n","Epoch 139, Loss 160331284480.00, Val loss 685721714688.00\n","-----Save best model-----\n","Epoch 140, Loss 160331202560.00, Val loss 685721518080.00\n","Epoch 141, Loss 160330956800.00, Val loss 685721845760.00\n","-----Save best model-----\n","Epoch 142, Loss 160330661888.00, Val loss 685721059328.00\n","-----Save best model-----\n","Epoch 143, Loss 160330743808.00, Val loss 685720272896.00\n","-----Save best model-----\n","Epoch 144, Loss 160330792960.00, Val loss 685719814144.00\n","Epoch 145, Loss 160330514432.00, Val loss 685719814144.00\n","-----Save best model-----\n","Epoch 146, Loss 160330235904.00, Val loss 685719420928.00\n","-----Save best model-----\n","Epoch 147, Loss 160330088448.00, Val loss 685718372352.00\n","-----Save best model-----\n","Epoch 148, Loss 160329924608.00, Val loss 685717716992.00\n","-----Save best model-----\n","Epoch 149, Loss 160330006528.00, Val loss 685717651456.00\n","Epoch 150, Loss 160329859072.00, Val loss 685717651456.00\n","-----Save best model-----\n","Epoch 151, Loss 160329416704.00, Val loss 685717585920.00\n","-----Save best model-----\n","Epoch 152, Loss 160329744384.00, Val loss 685717192704.00\n","-----Save best model-----\n","Epoch 153, Loss 160329302016.00, Val loss 685715816448.00\n","Epoch 154, Loss 160329138176.00, Val loss 685716209664.00\n","Epoch 155, Loss 160329072640.00, Val loss 685716537344.00\n","-----Save best model-----\n","Epoch 156, Loss 160328810496.00, Val loss 685714898944.00\n","Epoch 157, Loss 160328876032.00, Val loss 685714898944.00\n","-----Save best model-----\n","Epoch 158, Loss 160328744960.00, Val loss 685714243584.00\n","Epoch 159, Loss 160328384512.00, Val loss 685714374656.00\n","-----Save best model-----\n","Epoch 160, Loss 160328368128.00, Val loss 685713195008.00\n","Epoch 161, Loss 160328056832.00, Val loss 685713588224.00\n","-----Save best model-----\n","Epoch 162, Loss 160328138752.00, Val loss 685712801792.00\n","Epoch 163, Loss 160327499776.00, Val loss 685713063936.00\n","-----Save best model-----\n","Epoch 164, Loss 160327417856.00, Val loss 685712080896.00\n","-----Save best model-----\n","Epoch 165, Loss 160327401472.00, Val loss 685711097856.00\n","Epoch 166, Loss 160327548928.00, Val loss 685711163392.00\n","-----Save best model-----\n","Epoch 167, Loss 160326844416.00, Val loss 685711032320.00\n","-----Save best model-----\n","Epoch 168, Loss 160326746112.00, Val loss 685710573568.00\n","Epoch 169, Loss 160326877184.00, Val loss 685710573568.00\n","-----Save best model-----\n","Epoch 170, Loss 160326664192.00, Val loss 685709918208.00\n","-----Save best model-----\n","Epoch 171, Loss 160326418432.00, Val loss 685708476416.00\n","Epoch 172, Loss 160326205440.00, Val loss 685708869632.00\n","-----Save best model-----\n","Epoch 173, Loss 160326172672.00, Val loss 685707886592.00\n","-----Save best model-----\n","Epoch 174, Loss 160325828608.00, Val loss 685707755520.00\n","Epoch 175, Loss 160325844992.00, Val loss 685707755520.00\n","-----Save best model-----\n","Epoch 176, Loss 160325664768.00, Val loss 685706903552.00\n","-----Save best model-----\n","Epoch 177, Loss 160325894144.00, Val loss 685706706944.00\n","-----Save best model-----\n","Epoch 178, Loss 160325697536.00, Val loss 685706117120.00\n","Epoch 179, Loss 160325484544.00, Val loss 685706182656.00\n","Epoch 180, Loss 160325009408.00, Val loss 685706248192.00\n","-----Save best model-----\n","Epoch 181, Loss 160324665344.00, Val loss 685704871936.00\n","-----Save best model-----\n","Epoch 182, Loss 160324747264.00, Val loss 685704413184.00\n","Epoch 183, Loss 160324796416.00, Val loss 685705068544.00\n","-----Save best model-----\n","Epoch 184, Loss 160324222976.00, Val loss 685704347648.00\n","-----Save best model-----\n","Epoch 185, Loss 160324894720.00, Val loss 685704282112.00\n","-----Save best model-----\n","Epoch 186, Loss 160324075520.00, Val loss 685703102464.00\n","-----Save best model-----\n","Epoch 187, Loss 160324042752.00, Val loss 685702774784.00\n","-----Save best model-----\n","Epoch 188, Loss 160324042752.00, Val loss 685702381568.00\n","Epoch 189, Loss 160323895296.00, Val loss 685702643712.00\n","-----Save best model-----\n","Epoch 190, Loss 160323485696.00, Val loss 685701332992.00\n","-----Save best model-----\n","Epoch 191, Loss 160323469312.00, Val loss 685701005312.00\n","-----Save best model-----\n","Epoch 192, Loss 160323190784.00, Val loss 685700743168.00\n","-----Save best model-----\n","Epoch 193, Loss 160323043328.00, Val loss 685700153344.00\n","-----Save best model-----\n","Epoch 194, Loss 160323026944.00, Val loss 685699956736.00\n","-----Save best model-----\n","Epoch 195, Loss 160323174400.00, Val loss 685699497984.00\n","-----Save best model-----\n","Epoch 196, Loss 160322715648.00, Val loss 685699235840.00\n","-----Save best model-----\n","Epoch 197, Loss 160322584576.00, Val loss 685698580480.00\n","-----Save best model-----\n","Epoch 198, Loss 160322519040.00, Val loss 685698449408.00\n","-----Save best model-----\n","Epoch 199, Loss 160321945600.00, Val loss 685697662976.00\n"]}],"source":["#Train STAN_v4\n","\n","all_loss = []\n","file_name = './save/stan'\n","# min_loss = 1e10\n","min_loss = 1e20\n","\n","loc_name = 'Kentucky'\n","cur_loc = loc_list.index(loc_name)\n","\n","for epoch in range(200):\n","    model.train()\n","    optimizer.zero_grad()\n","    \n","    # cumulative_confirmed, cumulative_deaths, _ = model(train_x)\n","    cumulative_confirmed, _ = model(train_x)\n","    # loss = criterion(cumulative_confirmed.squeeze(), train_y_confirmed[cur_loc])+criterion(cumulative_deaths.squeeze(), train_y_deaths[cur_loc])\n","    loss = criterion(cumulative_confirmed.squeeze(), train_y_confirmed[cur_loc])\n","    \n","    loss.backward()\n","    optimizer.step()\n","    all_loss.append(loss.item())\n","    \n","    model.eval()\n","    # _, _, prev_h = model(train_x)\n","    _, prev_h = model(train_x)\n","    # val_cumulative_confirmed, val_cumulative_deaths, _ = model(val_x, prev_h)\n","    val_cumulative_confirmed, _ = model(val_x, prev_h)\n","    # val_loss = criterion(val_cumulative_confirmed.squeeze(), val_y_confirmed[cur_loc]) + criterion(val_cumulative_deaths, val_y_deaths[cur_loc])\n","    val_loss = criterion(val_cumulative_confirmed.squeeze(), val_y_confirmed[cur_loc])\n","    if val_loss < min_loss:    \n","        state = {\n","            'state': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","        }\n","        torch.save(state, file_name)\n","        min_loss = val_loss\n","        print('-----Save best model-----')\n","    \n","    print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, all_loss[-1], val_loss.item()))"]},{"cell_type":"code","execution_count":22,"metadata":{"scrolled":true,"id":"celF_EgLfODw","executionInfo":{"status":"ok","timestamp":1646336075637,"user_tz":360,"elapsed":847,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[],"source":["#Pred with STAN\n","file_name = './save/stan'\n","checkpoint = torch.load(file_name)\n","model.load_state_dict(checkpoint['state'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","model.eval()\n","\n","# Get hidden state from training/validation for use in testing\n","prev_x = torch.cat((train_x, val_x), dim=1)\n","# _, _, h = model(prev_x)\n","_, h = model(prev_x)\n","\n","\n","# Get predicted values for cumulative confirmed and cumulative deaths\n","# cumulative_confirmed, cumulative_deaths, _ = model(test_x, h)\n","cumulative_confirmed, _ = model(test_x, h)\n","\n","\n","# Real world reported/raw values for # cumulative confirmed and # cumulative deaths\n","# are test_y_confirmed and test_y_deaths\n","\n","# Prepare predicted and true values for plotting\n","predicted_confirmed = cumulative_confirmed.squeeze().detach().numpy()\n","# predicted_deaths = cumulative_deaths.squeeze().detach().numpy()\n","true_cumulative_confirmed = test_y_confirmed[cur_loc].squeeze()\n","# true_cumulative_deaths = test_y_deaths[cur_loc].squeeze()"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"iPRJiAwtfODy","outputId":"9fec772b-c1de-4d3e-9712-1f4fdb620998","colab":{"base_uri":"https://localhost:8080/","height":276},"executionInfo":{"status":"ok","timestamp":1646336075851,"user_tz":360,"elapsed":223,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}}},"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["plt.plot(true_cumulative_confirmed,c='r', label='Ground truth 1')\n","plt.plot(predicted_confirmed,c='b', label='Prediction 1')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","source":["print(predicted_confirmed)\n","# print(predicted_deaths)\n","print(true_cumulative_confirmed)\n","# print(true_cumulative_deaths)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvooqsL7-8oN","executionInfo":{"status":"ok","timestamp":1646336075854,"user_tz":360,"elapsed":22,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"ba18a363-57ea-4177-df94-0e00738c516f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[83.57101  83.78777  83.540726 84.96619  85.10915  84.012924 84.15616\n"," 83.66251  84.14442  84.08284  83.71713  83.10202  83.64532  85.15161\n"," 83.75926 ]\n","tensor([ 904916.,  915881.,  915881.,  915881.,  932552.,  941986.,  952956.,\n","         962007.,  975346.,  975346.,  975346.,  975346., 1014703., 1027069.,\n","        1040420.])\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"train_stan4.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}