{"cells":[{"cell_type":"code","source":["# Mount this colab file correctly in google drive so I can access relative/local files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","% cd /content/gdrive/My Drive/Github/\"CS 499 - SPRING 2022\"/\"1. STAN - Baseline Experiment\"\n","! pwd\n","\n","# Install python packages I will need in the local environment\n","! pip install epiweeks\n","! pip install haversine\n","! pip install dgl-cu111 -f https://data.dgl.ai/wheels/repo.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IyYzOWLb7efp","executionInfo":{"status":"ok","timestamp":1646756925852,"user_tz":360,"elapsed":7611,"user":{"displayName":"Andrew Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15265478926702059943"}},"outputId":"9adafbe8-5efe-4fe2-804a-31aa73caee6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/1. STAN - Baseline Experiment\n","/content/gdrive/My Drive/Github/CS 499 - SPRING 2022/1. STAN - Baseline Experiment\n","Requirement already satisfied: epiweeks in /usr/local/lib/python3.7/dist-packages (2.1.4)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.5.1)\n","Looking in links: https://data.dgl.ai/wheels/repo.html\n","Requirement already satisfied: dgl-cu111 in /usr/local/lib/python3.7/dist-packages (0.8.0.post1)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.6.3)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (4.63.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.23.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.21.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (1.24.3)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"]}]},{"cell_type":"code","source":["import time\n","time_start = time.time()"],"metadata":{"id":"OG6bvuUoJuxe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gU_SgLIE6eGQ"},"outputs":[],"source":["import csv\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","from epiweeks import Week\n","\n","from data_downloader import GenerateTrainingData\n","from utils import date_today, gravity_law_commute_dist\n","\n","os.environ['NUMEXPR_MAX_THREADS'] = '16'\n","os.environ['NUMEXPR_NUM_THREADS'] = '8'\n","\n","import pickle\n","import matplotlib.pyplot as plt\n","import dgl\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from model import STAN\n","\n","import sklearn\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"NAKjii446eGa"},"outputs":[],"source":["# GenerateTrainingData().download_jhu_data('2020-05-01', '2020-12-01')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0tFDWAn6eGe"},"outputs":[],"source":["#Merge population data with downloaded data\n","raw_data = pickle.load(open('./data/old_state_covid_data.pickle','rb'))\n","pop_data = pd.read_csv('./uszips.csv')\n","pop_data = pop_data.groupby('state_name').agg({'population':'sum', 'density':'mean', 'lat':'mean', 'lng':'mean'}).reset_index()\n","raw_data = pd.merge(raw_data, pop_data, how='inner', left_on='state', right_on='state_name')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"nw_TRvjf6eGg"},"outputs":[],"source":["# Generate location similarity\n","loc_list = list(raw_data['state'].unique())\n","loc_dist_map = {}\n","\n","for each_loc in loc_list:\n","    loc_dist_map[each_loc] = {}\n","    for each_loc2 in loc_list:\n","        lat1 = raw_data[raw_data['state']==each_loc]['latitude'].unique()[0]\n","        lng1 = raw_data[raw_data['state']==each_loc]['longitude'].unique()[0]\n","        pop1 = raw_data[raw_data['state']==each_loc]['population'].unique()[0]\n","        \n","        lat2 = raw_data[raw_data['state']==each_loc2]['latitude'].unique()[0]\n","        lng2 = raw_data[raw_data['state']==each_loc2]['longitude'].unique()[0]\n","        pop2 = raw_data[raw_data['state']==each_loc2]['population'].unique()[0]\n","        \n","        loc_dist_map[each_loc][each_loc2] = gravity_law_commute_dist(lat1, lng1, pop1, lat2, lng2, pop2, r=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qc12I2xT6eGh"},"outputs":[],"source":["#Generate Graph\n","dist_threshold = 18\n","\n","for each_loc in loc_dist_map:\n","    loc_dist_map[each_loc] = {k: v for k, v in sorted(loc_dist_map[each_loc].items(), key=lambda item: item[1], reverse=True)}\n","    \n","adj_map = {}\n","for each_loc in loc_dist_map:\n","    adj_map[each_loc] = []\n","    for i, each_loc2 in enumerate(loc_dist_map[each_loc]):\n","        if loc_dist_map[each_loc][each_loc2] > dist_threshold:\n","            if i <= 3:\n","                adj_map[each_loc].append(each_loc2)\n","            else:\n","                break\n","        else:\n","            if i <= 1:\n","                adj_map[each_loc].append(each_loc2)\n","            else:\n","                break\n","\n","rows = []\n","cols = []\n","for each_loc in adj_map:\n","    for each_loc2 in adj_map[each_loc]:\n","        rows.append(loc_list.index(each_loc))\n","        cols.append(loc_list.index(each_loc2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Py3-53LO6eGj"},"outputs":[],"source":["g = dgl.graph((rows, cols))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyWLeDaE6eGp"},"outputs":[],"source":["#Preprocess features\n","\n","active_cases = []\n","confirmed_cases = []\n","new_cases = []\n","death_cases = []\n","static_feat = []\n","\n","for i, each_loc in enumerate(loc_list):\n","    active_cases.append(raw_data[raw_data['state'] == each_loc]['active'])\n","    confirmed_cases.append(raw_data[raw_data['state'] == each_loc]['confirmed'])\n","    new_cases.append(raw_data[raw_data['state'] == each_loc]['new_cases'])\n","    death_cases.append(raw_data[raw_data['state'] == each_loc]['deaths'])\n","    static_feat.append(np.array(raw_data[raw_data['state'] == each_loc][['population','density','lng','lat']]))\n","    \n","active_cases = np.array(active_cases)\n","confirmed_cases = np.array(confirmed_cases)\n","death_cases = np.array(death_cases)\n","new_cases = np.array(new_cases)\n","static_feat = np.array(static_feat)[:, 0, :]\n","recovered_cases = confirmed_cases - active_cases - death_cases\n","susceptible_cases = np.expand_dims(static_feat[:, 0], -1) - active_cases - recovered_cases\n","\n","# Batch_feat: new_cases(dI), dR, dS\n","#dI = np.array(new_cases)\n","dI = np.concatenate((np.zeros((active_cases.shape[0],1), dtype=np.float32), np.diff(active_cases)), axis=-1)\n","dR = np.concatenate((np.zeros((recovered_cases.shape[0],1), dtype=np.float32), np.diff(recovered_cases)), axis=-1)\n","dS = np.concatenate((np.zeros((susceptible_cases.shape[0],1), dtype=np.float32), np.diff(susceptible_cases)), axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4eXAZ6Y6eGs"},"outputs":[],"source":["#Build normalizer\n","normalizer = {'S':{}, 'I':{}, 'R':{}, 'dS':{}, 'dI':{}, 'dR':{}}\n","\n","for i, each_loc in enumerate(loc_list):\n","    normalizer['S'][each_loc] = (np.mean(susceptible_cases[i]), np.std(susceptible_cases[i]))\n","    normalizer['I'][each_loc] = (np.mean(active_cases[i]), np.std(active_cases[i]))\n","    normalizer['R'][each_loc] = (np.mean(recovered_cases[i]), np.std(recovered_cases[i]))\n","    normalizer['dI'][each_loc] = (np.mean(dI[i]), np.std(dI[i]))\n","    normalizer['dR'][each_loc] = (np.mean(dR[i]), np.std(dR[i]))\n","    normalizer['dS'][each_loc] = (np.mean(dS[i]), np.std(dS[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyRACnIm6eGt"},"outputs":[],"source":["def prepare_data(data, sum_I, sum_R, history_window=5, pred_window=15, slide_step=5):\n","    # Data shape n_loc, timestep, n_feat\n","    # Reshape to n_loc, t, history_window*n_feat\n","    n_loc = data.shape[0]\n","    timestep = data.shape[1]\n","    n_feat = data.shape[2]\n","    \n","    x = []\n","    y_I = []\n","    y_R = []\n","    y_active_cases = []\n","    last_I = []\n","    last_R = []\n","    concat_I = []\n","    concat_R = []\n","    for i in range(0, timestep, slide_step):\n","        if i+history_window+pred_window-1 >= timestep or i+history_window >= timestep:\n","            break\n","        x.append(data[:, i:i+history_window, :].reshape((n_loc, history_window*n_feat)))\n","        \n","        concat_I.append(data[:, i+history_window-1, 0])\n","        concat_R.append(data[:, i+history_window-1, 1])\n","        last_I.append(sum_I[:, i+history_window-1])\n","        last_R.append(sum_R[:, i+history_window-1])\n","\n","        y_I.append(data[:, i+history_window:i+history_window+pred_window, 0])\n","        y_R.append(data[:, i+history_window:i+history_window+pred_window, 1])\n","        y_active_cases.append(sum_I[:, i+history_window:i+history_window+pred_window].reshape((n_loc, pred_window)))\n","\n","    x = np.array(x, dtype=np.float32).transpose((1, 0, 2))\n","    last_I = np.array(last_I, dtype=np.float32).transpose((1, 0))\n","    last_R = np.array(last_R, dtype=np.float32).transpose((1, 0))\n","    concat_I = np.array(concat_I, dtype=np.float32).transpose((1, 0))\n","    concat_R = np.array(concat_R, dtype=np.float32).transpose((1, 0))\n","    y_I = np.array(y_I, dtype=np.float32).transpose((1, 0, 2))\n","    y_R = np.array(y_R, dtype=np.float32).transpose((1, 0, 2))\n","    y_active_cases = np.array(y_active_cases, dtype=np.float32).transpose((1, 0, 2))\n","\n","    return x, last_I, last_R, concat_I, concat_R, y_I, y_R, y_active_cases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFnaQpcX6eGw"},"outputs":[],"source":["valid_window = 25\n","test_window = 25\n","\n","history_window=6\n","pred_window=15\n","slide_step=5\n","\n","dynamic_feat = np.concatenate((np.expand_dims(dI, axis=-1), np.expand_dims(dR, axis=-1), np.expand_dims(dS, axis=-1)), axis=-1)\n","    \n","#Normalize\n","for i, each_loc in enumerate(loc_list):\n","    dynamic_feat[i, :, 0] = (dynamic_feat[i, :, 0] - normalizer['dI'][each_loc][0]) / normalizer['dI'][each_loc][1]\n","    dynamic_feat[i, :, 1] = (dynamic_feat[i, :, 1] - normalizer['dR'][each_loc][0]) / normalizer['dR'][each_loc][1]\n","    dynamic_feat[i, :, 2] = (dynamic_feat[i, :, 2] - normalizer['dS'][each_loc][0]) / normalizer['dS'][each_loc][1]\n","\n","dI_mean = []\n","dI_std = []\n","dR_mean = []\n","dR_std = []\n","\n","for i, each_loc in enumerate(loc_list):\n","    dI_mean.append(normalizer['dI'][each_loc][0])\n","    dR_mean.append(normalizer['dR'][each_loc][0])\n","    dI_std.append(normalizer['dI'][each_loc][1])\n","    dR_std.append(normalizer['dR'][each_loc][1])\n","\n","dI_mean = np.array(dI_mean)\n","dI_std = np.array(dI_std)\n","dR_mean = np.array(dR_mean)\n","dR_std = np.array(dR_std)\n","\n","#Split train-test\n","train_feat = dynamic_feat[:, :-valid_window-test_window, :]\n","val_feat = dynamic_feat[:, -valid_window-test_window:-test_window, :]\n","test_feat = dynamic_feat[:, -test_window:, :]\n","\n","train_x, train_I, train_R, train_cI, train_cR, train_yI, train_yR, _ = prepare_data(train_feat, active_cases[:, :-valid_window-test_window], recovered_cases[:, :-valid_window-test_window], history_window, pred_window, slide_step)\n","val_x, val_I, val_R, val_cI, val_cR, val_yI, val_yR, _ = prepare_data(val_feat, active_cases[:, -valid_window-test_window:-test_window], recovered_cases[:, -valid_window-test_window:-test_window], history_window, pred_window, slide_step)\n","test_x, test_I, test_R, test_cI, test_cR, test_yI, test_yR, test_active_cases = prepare_data(test_feat, active_cases[:, -test_window:], recovered_cases[:, -test_window:], history_window, pred_window, slide_step)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFT4HJPP6eG2"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","train_x = torch.tensor(train_x).to(device)\n","train_I = torch.tensor(train_I).to(device)\n","train_R = torch.tensor(train_R).to(device)\n","train_cI = torch.tensor(train_cI).to(device)\n","train_cR = torch.tensor(train_cR).to(device)\n","train_yI = torch.tensor(train_yI).to(device)\n","train_yR = torch.tensor(train_yR).to(device)\n","\n","val_x = torch.tensor(val_x).to(device)\n","val_I = torch.tensor(val_I).to(device)\n","val_R = torch.tensor(val_R).to(device)\n","val_cI = torch.tensor(val_cI).to(device)\n","val_cR = torch.tensor(val_cR).to(device)\n","val_yI = torch.tensor(val_yI).to(device)\n","val_yR = torch.tensor(val_yR).to(device)\n","\n","test_x = torch.tensor(test_x).to(device)\n","test_I = torch.tensor(test_I).to(device)\n","test_R = torch.tensor(test_R).to(device)\n","test_cI = torch.tensor(test_cI).to(device)\n","test_cR = torch.tensor(test_cR).to(device)\n","test_yI = torch.tensor(test_yI).to(device)\n","test_yR = torch.tensor(test_yR).to(device)\n","test_active_cases = torch.tensor(test_active_cases).to(device)\n","\n","dI_mean = torch.tensor(dI_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n","dI_std = torch.tensor(dI_std, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n","dR_mean = torch.tensor(dR_mean, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n","dR_std = torch.tensor(dR_std, dtype=torch.float32).to(device).reshape((dI_mean.shape[0], 1, 1))\n","\n","N = torch.tensor(static_feat[:, 0], dtype=torch.float32).to(device).unsqueeze(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"PtNQVGk46eG4"},"outputs":[],"source":["#Train STAN\n","\n","# Hyperparameters\n","in_dim = 3*history_window\n","hidden_dim1 = 32\n","hidden_dim2 = 32\n","gru_dim = 32\n","num_heads = 1\n","\n","\n","# Build STAN model for each state\n","for loc_name in loc_list:\n","    print(\"TRAINING NEW LOCATION:\", loc_name)\n","    g = g.to(device)\n","    model = STAN(g, in_dim, hidden_dim1, hidden_dim2, gru_dim, num_heads, pred_window, device).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n","    criterion = nn.MSELoss()\n","\n","    all_loss = []\n","    file_name = './save/old_data/50_epochs_timed/' + loc_name\n","    min_loss = 1e10\n","\n","    cur_loc = loc_list.index(loc_name)\n","\n","    for epoch in range(50):\n","        model.train()\n","        optimizer.zero_grad()\n","        \n","        active_pred, recovered_pred, phy_active, phy_recover, _ = model(train_x, train_cI[cur_loc], train_cR[cur_loc], N[cur_loc], train_I[cur_loc], train_R[cur_loc])\n","        phy_active = (phy_active - dI_mean[cur_loc]) / dI_std[cur_loc]\n","        phy_recover = (phy_recover - dR_mean[cur_loc]) / dR_std[cur_loc]\n","        loss = criterion(active_pred.squeeze(), train_yI[cur_loc])+criterion(recovered_pred.squeeze(), train_yR[cur_loc])+0.1*criterion(phy_active.squeeze(), train_yI[cur_loc])+0.1*criterion(phy_recover.squeeze(), train_yR[cur_loc])\n","        \n","        loss.backward()\n","        optimizer.step()\n","        all_loss.append(loss.item())\n","        \n","        model.eval()\n","        _, _, _, _, prev_h = model(train_x, train_cI[cur_loc], train_cR[cur_loc], N[cur_loc], train_I[cur_loc], train_R[cur_loc])\n","        val_active_pred, val_recovered_pred, val_phy_active, val_phy_recover, _ = model(val_x, val_cI[cur_loc], val_cR[cur_loc], N[cur_loc], val_I[cur_loc], val_R[cur_loc], prev_h)\n","        \n","        val_phy_active = (val_phy_active - dI_mean[cur_loc]) / dI_std[cur_loc]\n","        val_loss = criterion(val_active_pred.squeeze(), val_yI[cur_loc]) + 0.1*criterion(val_phy_active.squeeze(), val_yI[cur_loc])\n","        if val_loss < min_loss:    \n","            state = {\n","                'state': model.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }\n","            torch.save(state, file_name)\n","            min_loss = val_loss\n","            print('Saved model')\n","        \n","        print('Epoch %d, Loss %.2f, Val loss %.2f'%(epoch, all_loss[-1], val_loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"x7ZVB8_a6eG8"},"outputs":[],"source":["#Pred with STAN\n","\n","model_predictions_number_infected_dictionary = {}\n","model_predictions_number_infected_list = []\n","\n","for loc_name in loc_list:\n","  file_name = './save/old_data/50_epochs_timed/' + loc_name\n","  checkpoint = torch.load(file_name)\n","  model = STAN(g, in_dim, hidden_dim1, hidden_dim2, gru_dim, num_heads, pred_window, device).to(device)\n","  model.load_state_dict(checkpoint['state'])\n","  model.eval()\n","\n","  prev_x = torch.cat((train_x, val_x), dim=1)\n","  prev_I = torch.cat((train_I, val_I), dim=1)\n","  prev_R = torch.cat((train_R, val_R), dim=1)\n","  prev_cI = torch.cat((train_cI, val_cI), dim=1)\n","  prev_cR = torch.cat((train_cR, val_cR), dim=1)\n","\n","  cur_loc = loc_list.index(loc_name)\n","\n","  prev_active_pred, _, prev_phyactive_pred, _, h = model(prev_x, prev_cI[cur_loc], prev_cR[cur_loc], N[cur_loc], prev_I[cur_loc], prev_R[cur_loc])\n","\n","  test_pred_active, test_pred_recovered, test_pred_phy_active, test_pred_phy_recover, _ = model(test_x, test_cI[cur_loc], test_cR[cur_loc], N[cur_loc], test_I[cur_loc], test_R[cur_loc], h)\n","\n","  # Cumulate predicted dI\n","  pred_I = []\n","\n","  for i in range(test_pred_active.size(1)):\n","      cur_pred = (test_pred_active[0, i, :].detach().cpu().numpy() * dI_std[cur_loc].reshape(1, 1).detach().cpu().numpy()) + dI_mean[cur_loc].reshape(1, 1).detach().cpu().numpy()\n","      #cur_pred = test_pred_phy_active[0, i, :].detach().cpu().numpy()\n","      cur_pred = (cur_pred + test_pred_phy_active[0, i, :].detach().cpu().numpy()) / 2\n","      cur_pred = np.cumsum(cur_pred)\n","      cur_pred = cur_pred + test_I[cur_loc, i].detach().cpu().item()\n","      pred_I.append(cur_pred)\n","\n","  model_predictions_number_infected_list.append(pred_I)  \n","  # pred_I = np.array(pred_I)\n","  # pred_I = pred_I\n","\n","  model_predictions_number_infected_dictionary[loc_name] = np.array(pred_I)[-1, :]\n","\n","model_predictions_number_infected_tensor = torch.tensor(model_predictions_number_infected_list)\n","model_predictions_number_infected = model_predictions_number_infected_tensor.reshape((len(loc_list), pred_window)) # (52, 1, 15) --> (52, 15)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"fkxZnASt6eHI"},"outputs":[],"source":["# Get the ground truth values for number of infected people \n","ground_truth_number_infected_dictionary = {}\n","ground_truth_number_infected_tensor = test_active_cases\n","\n","test_active_cases_tensor = test_active_cases.reshape((len(loc_list), pred_window)) # (52, 1, 15) --> (52, 15)\n","\n","for loc_name in loc_list:\n","  cur_loc = loc_list.index(loc_name)\n","  ground_truth_number_infected_dictionary[loc_name] = test_active_cases_tensor[cur_loc]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlL-G8F56eHM"},"outputs":[],"source":["archived_output = {\n","    'loc_list':loc_list, \n","    'model_predictions_number_infected_dictionary':model_predictions_number_infected_dictionary, \n","    'model_predictions_number_infected_tensor':model_predictions_number_infected_tensor, # (52, 15)\n","    'ground_truth_number_infected_dictionary':ground_truth_number_infected_dictionary, \n","    'ground_truth_number_infected_tensor':test_active_cases_tensor # (52, 15)\n","}\n","\n","import pickle\n","\n","# Save archived_output as pickle for use later\n","with open('./save/old_data/50_epochs_timed/archived_output_50_epochs.pickle', 'wb') as handle:\n","    pickle.dump(archived_output, handle)\n","\n","\n","# # Load in archived_output pickle\n","# archived_output = None\n","# with open('./save/old_data/50_epochs_timed/archived_output_50_epochs.pickle', 'rb') as handle:\n","#     archived_output = pickle.load(handle)\n","\n","\n","# print(model_predictions_number_infected['Alabama'])\n","# print(ground_truth_number_infected['Alabama'])"]},{"cell_type":"code","source":["time_end = time.time()\n","time_seconds = time_end - time_start\n","(t_min, t_sec) = divmod(time_seconds,60)\n","(t_hour,t_min) = divmod(t_min,60) \n","print('Time passed: {} hours:{} minutes:{} seconds'.format(t_hour,t_min,t_sec))\n","# 50 epochs: Time passed: 0.0 hours:27.0 minutes:46.89860773086548 seconds <-- on google colab with CPU <-- Monday 3/7\n","# 50 epochs: Time passed: 0.0 hours:25.0 minutes:17.643264055252075 seconds <-- Tuesday 3/8\n","# 1000 epochs: Time passed: 4.0 hours:8.0 minutes:40.086875438690186 seconds <-- on workstation with CPU"],"metadata":{"id":"E_9YYvLUJyLC"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"train_stan_old_data.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}